{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "new_model(hyper).ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f35677001b224e1a9846b598435ca091": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9160c5858f224565ae1a0e883d237218",
              "IPY_MODEL_19eaea0636f24162a78bfda44a9b6a9b"
            ],
            "layout": "IPY_MODEL_e4fc6dfce8324619b119d2616871533f"
          }
        },
        "9160c5858f224565ae1a0e883d237218": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4621c80f2af14e5496de8fd23499e2e7",
            "placeholder": "​",
            "style": "IPY_MODEL_9f3050f98b0c47debda3623736df6bf3",
            "value": "0.811 MB of 0.811 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "19eaea0636f24162a78bfda44a9b6a9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_975cc02ca4354e6989d7552de9e23057",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1322f66cd3804860a91332ac0e09dca8",
            "value": 1
          }
        },
        "e4fc6dfce8324619b119d2616871533f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4621c80f2af14e5496de8fd23499e2e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f3050f98b0c47debda3623736df6bf3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "975cc02ca4354e6989d7552de9e23057": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1322f66cd3804860a91332ac0e09dca8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKjl9Lhu_X13",
        "outputId": "6acca362-3e3e-4fa1-dbe1-3a60f65fc264"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.7/dist-packages (2.2.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.19.2)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.12.17)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.21.6)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.1.96)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.64.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.11.0+cu113)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.0.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.12.0+cu113)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.7.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->sentence-transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.5.12)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.27)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.7/dist-packages (from wandb) (1.2.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence-transformers transformers wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb\n",
        "# api key 입력\n",
        "!wandb login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BIvDcJODFnS",
        "outputId": "aacb7d75-63d6-4722-c670-3f116a38300b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.12.17)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.7/dist-packages (from wandb) (1.2.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.9)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.5.12)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.27)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.2.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkdb\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwmv7_NS_j-X",
        "outputId": "9f3c66c3-8d66-4cf8-962a-19bd355abfee"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from transformers import AutoTokenizer, AutoModel, RobertaModel, RobertaTokenizer, RobertaForSequenceClassification, ElectraTokenizer, ElectraForSequenceClassification\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler, random_split\n",
        "import numpy as np\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "import torch\n",
        "from functools import partial\n",
        "import wandb\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from sklearn.metrics import f1_score\n",
        "from scipy.stats import pearsonr"
      ],
      "metadata": {
        "id": "mEf1FkCJ_nD7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('/content/drive/MyDrive/NLP/df.csv')\n",
        "test = pd.read_csv('/content/drive/MyDrive/NLP/test.csv')"
      ],
      "metadata": {
        "id": "I-VcMr7vAS0T"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = train[['sentence1', 'sentence2', 'real-label', 'binary-label']]\n",
        "test = test[['sentence1', 'sentence2', 'real-label', 'binary-label']]"
      ],
      "metadata": {
        "id": "gLjfzIvPAjWX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test.rename(columns ={'real-label':'real_label', 'binary-label':'binary_label'}, inplace = True)"
      ],
      "metadata": {
        "id": "fEXwVoV6VHJs"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import html\n",
        "import regex as re\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def preprocess(sentence):\n",
        "    sen = BeautifulSoup(html.unescape(sentence), 'html.parser').text     # html parse\n",
        "    sen = sen.replace(\"\\n\", \" \")                                    # \\n\n",
        "    sen = re.sub('\"',' ', sen)                                      # 따옴표 \n",
        "    sen = re.sub(\"[^a-zA-Z0-9가-힣]\", \" \", sen)                  #영문, 한글, 숫자 만\n",
        "    sen = re.sub('[-=+,#/\\?:^$.@*\\\"※~&%ㆍ·!』\\\\‘〈〉|\\(\\)\\[\\]\\<\\>`\\'…》《]','', sen)    \n",
        "    return sen\n",
        "\n",
        "\n",
        "\n",
        "def preprocess_train(df):\n",
        "    sentence1 = df['sentence1'].tolist()\n",
        "    sentence2 = df['sentence2'].tolist()\n",
        "    real_label = df['real-label'].tolist()\n",
        "    label = df['binary-label'].tolist()\n",
        "\n",
        "    processed1 = []\n",
        "    processed2 = []\n",
        "\n",
        "    for sen1 in sentence1:\n",
        "        processed1.append(preprocess(sen1))\n",
        "    for sen2 in sentence2:\n",
        "        processed2.append(preprocess(sen2))\n",
        "    \n",
        "    processed_df = pd.DataFrame(list(zip(processed1, processed2, real_label, label)),\n",
        "                        columns = ['sentence1', 'sentence2', 'real_label', 'binary_label'])\n",
        "\n",
        "\n",
        "    return processed_df"
      ],
      "metadata": {
        "id": "jpKvpo7bAliQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = preprocess_train(train)"
      ],
      "metadata": {
        "id": "aWO6wNghAt3t"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train, val = train_test_split((train), test_size=0.1, random_state = 42)"
      ],
      "metadata": {
        "id": "pRxbqel9VVc8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 42\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)"
      ],
      "metadata": {
        "id": "QBo3btCeBBO-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = train.reset_index().drop(['index'], axis = 1)\n",
        "valid_data = val.reset_index().drop(['index'], axis = 1)\n",
        "test_data = test.reset_index().drop(['index'], axis = 1)"
      ],
      "metadata": {
        "id": "hYArxC8faqjW"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"# available GPUs : {torch.cuda.device_count()}\")\n",
        "    print(f\"GPU name : {torch.cuda.get_device_name()}\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DpmMezQBHxg",
        "outputId": "f9a0edba-d3ca-49f5-9aa1-08872a632b6a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# available GPUs : 1\n",
            "GPU name : Tesla T4\n",
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = RobertaForSequenceClassification.from_pretrained(\"klue/roberta-base\", num_labels = 1)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-base\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7CUi-OfBXX0",
        "outputId": "692e80c1-c493-4ffd-b0b6-655c7d5d42e0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at klue/roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data) -> None: \n",
        "        self.data = data            \n",
        "        self.input, self.label = list(zip(self.data['sentence1'], self.data['sentence2'])), self.data['real_label']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.label) # len(y)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.input[index], self.label[index]  "
      ],
      "metadata": {
        "id": "jVY640bNCkGR"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_collate_fn(batch, max_length):\n",
        "\n",
        "    global tokenizer\n",
        "  \n",
        "    input_list, target_list = zip(*batch) \n",
        "    tensorized_input = tokenizer.batch_encode_plus(\n",
        "\n",
        "        [(sentences[0], sentences[1]) for sentences in input_list],\n",
        "        max_length = max_length, # \n",
        "        padding= \"max_length\",\n",
        "        add_special_tokens=True,\n",
        "        truncation=True,\n",
        "        return_tensors='pt',\n",
        "    )\n",
        "    \n",
        "    tensorized_label = torch.tensor(target_list)\n",
        "  \n",
        "    return tensorized_input, tensorized_label"
      ],
      "metadata": {
        "id": "Ur3E35eOCraN"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = CustomDataset(train_data)\n",
        "valid_dataset = CustomDataset(valid_data)\n",
        "test_dataset = CustomDataset(test_data)"
      ],
      "metadata": {
        "id": "nqpuuOauCuYR"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(model, optimizer, scheduler, epoch, loss):\n",
        "\n",
        "    file_name = f'/content/drive/MyDrive/AI09/model_F.ckpt.{epoch}'\n",
        "        \n",
        "    torch.save(\n",
        "        {\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "            'loss' : loss\n",
        "        }, \n",
        "        file_name\n",
        "    )\n",
        "    \n",
        "    print(f\"Saving epoch {epoch} checkpoint at {file_name}\")"
      ],
      "metadata": {
        "id": "Ns2ZYeqVDZAo"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model, dataloader):    \n",
        "\n",
        "    model.eval()\n",
        "    \n",
        "    total_loss = 0\n",
        "    batch_count = 0\n",
        "    batch_loss = 0\n",
        "    pred_list = None \n",
        "\n",
        "    for step, batch in enumerate(dataloader):       \n",
        "        batch_count += 1\n",
        "        batch = tuple(item.to(device) for item in batch)\n",
        "\n",
        "        batch_input, batch_label = batch\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**batch_input, labels = batch_label.float()) \n",
        "\n",
        "        loss = outputs.loss \n",
        "        pred = outputs.logits.squeeze()\n",
        "        \n",
        "        if pred_list is None:\n",
        "           pred_list = pred.detach().cpu().numpy()\n",
        "           label_list = batch_label.detach().cpu().numpy()\n",
        "        else:\n",
        "            pred_list = np.append(pred_list, pred.detach().cpu().numpy(), axis=0)\n",
        "            label_list = np.append(label_list, batch_label.detach().cpu().numpy(), axis=0)        \n",
        "        \n",
        "        batch_loss += loss.item()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if (step % 10) == 0 and step != 0:  \n",
        "            print(f\"Step : {step}, valid Loss : {batch_loss / batch_count:.4f}\")\n",
        "            wandb.log({'valid_loss': batch_loss / batch_count})    \n",
        "            batch_loss = 0\n",
        "            batch_count = 0\n",
        "\n",
        "    fone_pred = np.where(pred_list >=3, 1, 0)\n",
        "    fone_label = np.where(label_list >=3, 1, 0)     \n",
        "    fone = f1_score(fone_pred, fone_label) * 100\n",
        "    p_score = pearsonr(pred_list, label_list)[0] * 100  \n",
        "       \n",
        "    total_valid_loss = total_loss / (step + 1)              \n",
        "           \n",
        "    wandb.log({'total_valid_loss': total_valid_loss, \"total_f1_score \": fone, \"total_pearsonr\" : p_score})     \n",
        "   \n",
        "    return total_valid_loss, fone, p_score"
      ],
      "metadata": {
        "id": "ube5tOBHDfCe"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, optimizer, scheduler, train_dataloader, valid_dataloader, epochs):   \n",
        "\n",
        "    wandb.watch(model, log=\"all\", log_freq = 10)\n",
        "      \n",
        "    for epoch in range(epochs):\n",
        "        print(f'****** Starting To Train Epoch #{epoch} ******')\n",
        "\n",
        "        total_loss = 0\n",
        "        batch_loss = 0\n",
        "        batch_count = 0      \n",
        "\n",
        "        model.to(device)\n",
        "        model.train()\n",
        "\n",
        "        \n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            batch_count += 1\n",
        "            batch = tuple(item.to(device) for item in batch)\n",
        "           \n",
        "            batch_input, batch_label = batch\n",
        "            model.zero_grad()\n",
        "\n",
        "            outputs = model(**batch_input, labels = batch_label.float())\n",
        "            loss = outputs.loss \n",
        "            \n",
        "            batch_loss += loss.item()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "             \n",
        "            clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            \n",
        "            if (step % 10) == 0 and step != 0:\n",
        "                wandb.log({'train_loss': batch_loss / batch_count, 'train_lr': optimizer.param_groups[0]['lr']})                    \n",
        "                print(f\"Epoch: {epoch}, Step : {step}, LR : {optimizer.param_groups[0]['lr']}, Avg Loss : {batch_loss / batch_count:.4f}\")\n",
        "                batch_loss, batch_count = 0,0\n",
        "                \n",
        "        wandb.log({'total_train_loss': total_loss / (step + 1), 'total_train_lr': optimizer.param_groups[0]['lr'], \"epoch\" : (epoch + 1)})\n",
        "        print(f\"Epoch {epoch} total_train_loss : {total_loss/(step+1):.4f}\")\n",
        "        print(f\"***** Finish To Train Epoch {epoch} *****\\n\") \n",
        "\n",
        "        print(f\"*****Epoch {epoch} Valid Start*****\")\n",
        "        total_valid_loss, fone, p_score = validate(model, valid_dataloader)\n",
        "        print('total_valid_loss : ', total_valid_loss, \"val_f1_score : \",  fone,  \"val_pearsonr :\",  p_score)  \n",
        "        print(f\"Epoch {epoch} total_Valid Loss : {total_valid_loss:.4f}\") \n",
        "        print(f\"*****Epoch {epoch} Valid Finish*****\\n\")\n",
        "        save_checkpoint(model, optimizer, scheduler,  epoch, total_valid_loss)\n",
        "\n",
        "\n",
        "\n",
        "    print(\"Train Finished\")"
      ],
      "metadata": {
        "id": "pIN75o4zDhnk"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\n",
        "    \n",
        "    \"name\" : \"AI09_F\",   \n",
        "    \"method\": \"bayes\",\n",
        "    \"metric\": {\n",
        "        \"name\" : \"total_valid_loss\", \n",
        "        \"goal\" : \"minimize\"\n",
        "                },\n",
        "    \n",
        "    \"parameters\": { \n",
        "        \"epochs\" : {\n",
        "            \"distribution\" : \"categorical\",\n",
        "            \"values\" : [5]},                     \n",
        "        \"learning_rate\" : {\n",
        "            \"distribution\" : \"categorical\",\n",
        "            \"values\" : [2e-5]},                     \n",
        "        \"eps\" : {\n",
        "            \"distribution\" : \"categorical\",\n",
        "            \"values\" : [1e-8]\n",
        "        },\n",
        "        \"train_batch_size\" : {\n",
        "            \"distribution\" : \"categorical\",\n",
        "            \"values\" : [8]\n",
        "        },\n",
        "        \"valid_batch_size\" : {\n",
        "            \"distribution\" : \"categorical\",\n",
        "            \"values\" : [32]\n",
        "        },\n",
        "        \"weight_decay\" : {\n",
        "            \"distribution\" : \"categorical\",\n",
        "            \"values\" : [0]\n",
        "        },\n",
        "        \"warm_up_ratio\" : {\n",
        "            \"distribution\" : \"categorical\",\n",
        "            \"values\" : [0.1]\n",
        "        },\n",
        "        \"max_length\" : {\n",
        "            \"distribution\" : \"categorical\",       \n",
        "            \"values\" : [128]\n",
        "        },\n",
        "        \"grad_norm\" : {\n",
        "            \"distribution\" : \"categorical\",\n",
        "            \"values\" : [1.0]\n",
        "        },\n",
        "    },         \n",
        "    \"early_terminate\" : {\n",
        "        \"type\": \"hyperband\", \n",
        "        \"min_iter\" : 2,\n",
        "        \"eta\" : 2\n",
        "        }\n",
        "}"
      ],
      "metadata": {
        "id": "o3Gr7CILDy4s"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initializer(config=None):\n",
        "    \"\"\"\n",
        "    설정에 맞춰서 wandb sweep 실행.\n",
        "    \"\"\"\n",
        "    wandb.init(config=config)\n",
        "\n",
        "    model = RobertaForSequenceClassification.from_pretrained(\"klue/roberta-base\", num_labels = 1)\n",
        "    \n",
        "    no_decay = ['bias', 'LayerNorm.weight']\n",
        "\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "         'weight_decay': 0},\n",
        "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
        "    \n",
        "\n",
        "    optimizer = AdamW(\n",
        "                      optimizer_grouped_parameters,\n",
        "                      lr = 2e-5,\n",
        "                      eps = 1e-8\n",
        "                      ) \n",
        "    num_training_steps = epochs * len(train_dataloader)\n",
        "\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "                                                optimizer=optimizer, \n",
        "                                                num_warmup_steps= (num_training_steps * 0.1),\n",
        "                                                num_training_steps = num_training_steps\n",
        "                                                )\n",
        "    \n",
        " \n",
        "    train(model, optimizer, scheduler, train_dataloader, valid_dataloader, epochs)   "
      ],
      "metadata": {
        "id": "qKDgtHdTDphq"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(\n",
        "                              train_dataset,\n",
        "                              batch_size = 8,\n",
        "                              sampler = RandomSampler(train_dataset),\n",
        "                              collate_fn = partial(custom_collate_fn, max_length=128)\n",
        "                              )\n",
        "valid_dataloader = DataLoader(\n",
        "                              valid_dataset,\n",
        "                              batch_size = 32,\n",
        "                              sampler = SequentialSampler(valid_dataset),\n",
        "                              collate_fn = partial(custom_collate_fn, max_length= 128)\n",
        "                              )\n",
        "test_dataloader = DataLoader(\n",
        "                            test_dataset, \n",
        "                            batch_size = 32,\n",
        "                            sampler = SequentialSampler(test_dataset),\n",
        "                            collate_fn = partial(custom_collate_fn, max_length= 128)\n",
        "                            )"
      ],
      "metadata": {
        "id": "AFlx6rhxDODA"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "sweep_id = wandb.sweep(sweep_config, project = \"AI09_f\")\n",
        "wandb.agent(sweep_id, initializer, count = 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f35677001b224e1a9846b598435ca091",
            "9160c5858f224565ae1a0e883d237218",
            "19eaea0636f24162a78bfda44a9b6a9b",
            "e4fc6dfce8324619b119d2616871533f",
            "4621c80f2af14e5496de8fd23499e2e7",
            "9f3050f98b0c47debda3623736df6bf3",
            "975cc02ca4354e6989d7552de9e23057",
            "1322f66cd3804860a91332ac0e09dca8"
          ]
        },
        "id": "aHE2oQEwExtZ",
        "outputId": "08ae82fc-c058-416f-b7ab-2c2d21230877"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: qm6ty8sp\n",
            "Sweep URL: https://wandb.ai/kdb/AI09_f/sweeps/qm6ty8sp\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: cib6ykvf with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \teps: 1e-08\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tgrad_norm: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 2e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_length: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_batch_size: 8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tvalid_batch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twarm_up_ratio: 0.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkdb\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.17"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220601_040135-cib6ykvf</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/kdb/AI09_f/runs/cib6ykvf\" target=\"_blank\">peachy-sweep-1</a></strong> to <a href=\"https://wandb.ai/kdb/AI09_f\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/kdb/AI09_f/sweeps/qm6ty8sp\" target=\"_blank\">https://wandb.ai/kdb/AI09_f/sweeps/qm6ty8sp</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at klue/roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "****** Starting To Train Epoch #0 ******\n",
            "Epoch: 0, Step : 10, LR : 2.2529441884280595e-07, Avg Loss : 9.1273\n",
            "Epoch: 0, Step : 20, LR : 4.301075268817205e-07, Avg Loss : 9.5161\n",
            "Epoch: 0, Step : 30, LR : 6.34920634920635e-07, Avg Loss : 11.8375\n",
            "Epoch: 0, Step : 40, LR : 8.397337429595495e-07, Avg Loss : 9.0661\n",
            "Epoch: 0, Step : 50, LR : 1.044546850998464e-06, Avg Loss : 8.5693\n",
            "Epoch: 0, Step : 60, LR : 1.2493599590373785e-06, Avg Loss : 7.5753\n",
            "Epoch: 0, Step : 70, LR : 1.454173067076293e-06, Avg Loss : 7.3283\n",
            "Epoch: 0, Step : 80, LR : 1.6589861751152075e-06, Avg Loss : 4.5207\n",
            "Epoch: 0, Step : 90, LR : 1.8637992831541222e-06, Avg Loss : 4.9453\n",
            "Epoch: 0, Step : 100, LR : 2.0686123911930364e-06, Avg Loss : 4.6219\n",
            "Epoch: 0, Step : 110, LR : 2.273425499231951e-06, Avg Loss : 3.5462\n",
            "Epoch: 0, Step : 120, LR : 2.4782386072708657e-06, Avg Loss : 3.0200\n",
            "Epoch: 0, Step : 130, LR : 2.6830517153097803e-06, Avg Loss : 2.8527\n",
            "Epoch: 0, Step : 140, LR : 2.8878648233486946e-06, Avg Loss : 2.6360\n",
            "Epoch: 0, Step : 150, LR : 3.092677931387609e-06, Avg Loss : 2.7990\n",
            "Epoch: 0, Step : 160, LR : 3.2974910394265234e-06, Avg Loss : 2.4495\n",
            "Epoch: 0, Step : 170, LR : 3.502304147465438e-06, Avg Loss : 2.6832\n",
            "Epoch: 0, Step : 180, LR : 3.7071172555043527e-06, Avg Loss : 3.2616\n",
            "Epoch: 0, Step : 190, LR : 3.911930363543267e-06, Avg Loss : 2.5816\n",
            "Epoch: 0, Step : 200, LR : 4.1167434715821816e-06, Avg Loss : 2.7889\n",
            "Epoch: 0, Step : 210, LR : 4.321556579621096e-06, Avg Loss : 2.3931\n",
            "Epoch: 0, Step : 220, LR : 4.526369687660011e-06, Avg Loss : 2.2816\n",
            "Epoch: 0, Step : 230, LR : 4.731182795698925e-06, Avg Loss : 1.9731\n",
            "Epoch: 0, Step : 240, LR : 4.935995903737839e-06, Avg Loss : 1.4179\n",
            "Epoch: 0, Step : 250, LR : 5.140809011776754e-06, Avg Loss : 0.9809\n",
            "Epoch: 0, Step : 260, LR : 5.345622119815669e-06, Avg Loss : 1.0030\n",
            "Epoch: 0, Step : 270, LR : 5.550435227854584e-06, Avg Loss : 0.7817\n",
            "Epoch: 0, Step : 280, LR : 5.755248335893497e-06, Avg Loss : 0.7342\n",
            "Epoch: 0, Step : 290, LR : 5.960061443932412e-06, Avg Loss : 0.5593\n",
            "Epoch: 0, Step : 300, LR : 6.164874551971327e-06, Avg Loss : 0.4806\n",
            "Epoch: 0, Step : 310, LR : 6.3696876600102405e-06, Avg Loss : 0.6735\n",
            "Epoch: 0, Step : 320, LR : 6.574500768049156e-06, Avg Loss : 0.4322\n",
            "Epoch: 0, Step : 330, LR : 6.779313876088071e-06, Avg Loss : 0.7628\n",
            "Epoch: 0, Step : 340, LR : 6.984126984126984e-06, Avg Loss : 0.6594\n",
            "Epoch: 0, Step : 350, LR : 7.188940092165899e-06, Avg Loss : 0.6443\n",
            "Epoch: 0, Step : 360, LR : 7.393753200204814e-06, Avg Loss : 1.0162\n",
            "Epoch: 0, Step : 370, LR : 7.5985663082437275e-06, Avg Loss : 0.6708\n",
            "Epoch: 0, Step : 380, LR : 7.803379416282643e-06, Avg Loss : 0.3623\n",
            "Epoch: 0, Step : 390, LR : 8.008192524321557e-06, Avg Loss : 0.7391\n",
            "Epoch: 0, Step : 400, LR : 8.213005632360471e-06, Avg Loss : 0.7247\n",
            "Epoch: 0, Step : 410, LR : 8.417818740399385e-06, Avg Loss : 0.5677\n",
            "Epoch: 0, Step : 420, LR : 8.622631848438301e-06, Avg Loss : 0.5765\n",
            "Epoch: 0, Step : 430, LR : 8.827444956477215e-06, Avg Loss : 0.5334\n",
            "Epoch: 0, Step : 440, LR : 9.03225806451613e-06, Avg Loss : 0.4872\n",
            "Epoch: 0, Step : 450, LR : 9.237071172555044e-06, Avg Loss : 0.3755\n",
            "Epoch: 0, Step : 460, LR : 9.44188428059396e-06, Avg Loss : 0.4270\n",
            "Epoch: 0, Step : 470, LR : 9.646697388632872e-06, Avg Loss : 0.5517\n",
            "Epoch: 0, Step : 480, LR : 9.851510496671788e-06, Avg Loss : 0.5844\n",
            "Epoch: 0, Step : 490, LR : 1.0056323604710702e-05, Avg Loss : 0.5148\n",
            "Epoch: 0, Step : 500, LR : 1.0261136712749618e-05, Avg Loss : 0.5840\n",
            "Epoch: 0, Step : 510, LR : 1.0465949820788533e-05, Avg Loss : 0.5170\n",
            "Epoch: 0, Step : 520, LR : 1.0670762928827445e-05, Avg Loss : 0.7525\n",
            "Epoch: 0, Step : 530, LR : 1.087557603686636e-05, Avg Loss : 0.5612\n",
            "Epoch: 0, Step : 540, LR : 1.1080389144905275e-05, Avg Loss : 0.7539\n",
            "Epoch: 0, Step : 550, LR : 1.128520225294419e-05, Avg Loss : 0.5342\n",
            "Epoch: 0, Step : 560, LR : 1.1490015360983104e-05, Avg Loss : 0.6496\n",
            "Epoch: 0, Step : 570, LR : 1.169482846902202e-05, Avg Loss : 0.6167\n",
            "Epoch: 0, Step : 580, LR : 1.1899641577060932e-05, Avg Loss : 0.7425\n",
            "Epoch: 0, Step : 590, LR : 1.2104454685099846e-05, Avg Loss : 0.8417\n",
            "Epoch: 0, Step : 600, LR : 1.2309267793138762e-05, Avg Loss : 0.5628\n",
            "Epoch: 0, Step : 610, LR : 1.2514080901177676e-05, Avg Loss : 0.4833\n",
            "Epoch: 0, Step : 620, LR : 1.271889400921659e-05, Avg Loss : 0.6766\n",
            "Epoch: 0, Step : 630, LR : 1.2923707117255507e-05, Avg Loss : 0.5007\n",
            "Epoch: 0, Step : 640, LR : 1.3128520225294419e-05, Avg Loss : 0.4518\n",
            "Epoch: 0, Step : 650, LR : 1.3333333333333333e-05, Avg Loss : 0.7155\n",
            "Epoch: 0, Step : 660, LR : 1.353814644137225e-05, Avg Loss : 0.5519\n",
            "Epoch: 0, Step : 670, LR : 1.3742959549411163e-05, Avg Loss : 0.4296\n",
            "Epoch: 0, Step : 680, LR : 1.3947772657450078e-05, Avg Loss : 0.5804\n",
            "Epoch: 0, Step : 690, LR : 1.4152585765488994e-05, Avg Loss : 0.4384\n",
            "Epoch: 0, Step : 700, LR : 1.4357398873527908e-05, Avg Loss : 0.5369\n",
            "Epoch: 0, Step : 710, LR : 1.456221198156682e-05, Avg Loss : 0.4355\n",
            "Epoch: 0, Step : 720, LR : 1.4767025089605736e-05, Avg Loss : 0.4120\n",
            "Epoch: 0, Step : 730, LR : 1.497183819764465e-05, Avg Loss : 0.3556\n",
            "Epoch: 0, Step : 740, LR : 1.5176651305683565e-05, Avg Loss : 0.3516\n",
            "Epoch: 0, Step : 750, LR : 1.538146441372248e-05, Avg Loss : 0.4297\n",
            "Epoch: 0, Step : 760, LR : 1.5586277521761396e-05, Avg Loss : 0.4197\n",
            "Epoch: 0, Step : 770, LR : 1.5791090629800307e-05, Avg Loss : 0.6885\n",
            "Epoch: 0, Step : 780, LR : 1.599590373783922e-05, Avg Loss : 0.5317\n",
            "Epoch: 0, Step : 790, LR : 1.6200716845878136e-05, Avg Loss : 0.4224\n",
            "Epoch: 0, Step : 800, LR : 1.6405529953917053e-05, Avg Loss : 0.4069\n",
            "Epoch: 0, Step : 810, LR : 1.6610343061955968e-05, Avg Loss : 0.4412\n",
            "Epoch: 0, Step : 820, LR : 1.6815156169994882e-05, Avg Loss : 0.4034\n",
            "Epoch: 0, Step : 830, LR : 1.7019969278033796e-05, Avg Loss : 0.5096\n",
            "Epoch: 0, Step : 840, LR : 1.722478238607271e-05, Avg Loss : 0.4541\n",
            "Epoch: 0, Step : 850, LR : 1.7429595494111624e-05, Avg Loss : 0.4083\n",
            "Epoch: 0, Step : 860, LR : 1.763440860215054e-05, Avg Loss : 0.4321\n",
            "Epoch: 0, Step : 870, LR : 1.7839221710189453e-05, Avg Loss : 0.4093\n",
            "Epoch: 0, Step : 880, LR : 1.8044034818228367e-05, Avg Loss : 0.7653\n",
            "Epoch: 0, Step : 890, LR : 1.8248847926267285e-05, Avg Loss : 0.5727\n",
            "Epoch: 0, Step : 900, LR : 1.8453661034306196e-05, Avg Loss : 0.4207\n",
            "Epoch: 0, Step : 910, LR : 1.865847414234511e-05, Avg Loss : 0.6225\n",
            "Epoch: 0, Step : 920, LR : 1.8863287250384027e-05, Avg Loss : 0.4578\n",
            "Epoch: 0, Step : 930, LR : 1.906810035842294e-05, Avg Loss : 0.2778\n",
            "Epoch: 0, Step : 940, LR : 1.9272913466461856e-05, Avg Loss : 0.5222\n",
            "Epoch: 0, Step : 950, LR : 1.947772657450077e-05, Avg Loss : 0.4708\n",
            "Epoch: 0, Step : 960, LR : 1.9682539682539684e-05, Avg Loss : 0.4585\n",
            "Epoch: 0, Step : 970, LR : 1.98873527905786e-05, Avg Loss : 0.4070\n",
            "Epoch: 0, Step : 980, LR : 1.9989759344598056e-05, Avg Loss : 0.3973\n",
            "Epoch: 0, Step : 990, LR : 1.9967002332593734e-05, Avg Loss : 0.4096\n",
            "Epoch: 0, Step : 1000, LR : 1.994424532058941e-05, Avg Loss : 0.6014\n",
            "Epoch: 0, Step : 1010, LR : 1.9921488308585083e-05, Avg Loss : 0.4572\n",
            "Epoch: 0, Step : 1020, LR : 1.989873129658076e-05, Avg Loss : 0.3810\n",
            "Epoch: 0, Step : 1030, LR : 1.9875974284576436e-05, Avg Loss : 0.3627\n",
            "Epoch: 0, Step : 1040, LR : 1.9853217272572114e-05, Avg Loss : 0.6506\n",
            "Epoch: 0, Step : 1050, LR : 1.983046026056779e-05, Avg Loss : 0.9594\n",
            "Epoch: 0, Step : 1060, LR : 1.9807703248563467e-05, Avg Loss : 0.3150\n",
            "Epoch: 0, Step : 1070, LR : 1.978494623655914e-05, Avg Loss : 0.3853\n",
            "Epoch: 0, Step : 1080, LR : 1.9762189224554816e-05, Avg Loss : 0.5810\n",
            "Epoch: 0, Step : 1090, LR : 1.9739432212550494e-05, Avg Loss : 0.5067\n",
            "Epoch: 0, Step : 1100, LR : 1.971667520054617e-05, Avg Loss : 0.4495\n",
            "Epoch: 0, Step : 1110, LR : 1.9693918188541844e-05, Avg Loss : 0.6806\n",
            "Epoch: 0, Step : 1120, LR : 1.967116117653752e-05, Avg Loss : 0.6485\n",
            "Epoch: 0, Step : 1130, LR : 1.96484041645332e-05, Avg Loss : 0.5707\n",
            "Epoch: 0, Step : 1140, LR : 1.9625647152528874e-05, Avg Loss : 0.8120\n",
            "Epoch: 0, Step : 1150, LR : 1.9602890140524552e-05, Avg Loss : 0.5176\n",
            "Epoch: 0, Step : 1160, LR : 1.9580133128520227e-05, Avg Loss : 0.2651\n",
            "Epoch: 0, Step : 1170, LR : 1.9557376116515902e-05, Avg Loss : 0.8024\n",
            "Epoch: 0, Step : 1180, LR : 1.953461910451158e-05, Avg Loss : 0.5895\n",
            "Epoch: 0, Step : 1190, LR : 1.9511862092507255e-05, Avg Loss : 0.5562\n",
            "Epoch: 0, Step : 1200, LR : 1.9489105080502933e-05, Avg Loss : 0.5080\n",
            "Epoch: 0, Step : 1210, LR : 1.9466348068498607e-05, Avg Loss : 0.4208\n",
            "Epoch: 0, Step : 1220, LR : 1.9443591056494285e-05, Avg Loss : 0.3397\n",
            "Epoch: 0, Step : 1230, LR : 1.942083404448996e-05, Avg Loss : 0.4343\n",
            "Epoch: 0, Step : 1240, LR : 1.9398077032485635e-05, Avg Loss : 0.4060\n",
            "Epoch: 0, Step : 1250, LR : 1.9375320020481313e-05, Avg Loss : 0.4874\n",
            "Epoch: 0, Step : 1260, LR : 1.9352563008476988e-05, Avg Loss : 0.3643\n",
            "Epoch: 0, Step : 1270, LR : 1.9329805996472662e-05, Avg Loss : 0.5328\n",
            "Epoch: 0, Step : 1280, LR : 1.930704898446834e-05, Avg Loss : 0.3791\n",
            "Epoch: 0, Step : 1290, LR : 1.928429197246402e-05, Avg Loss : 0.4394\n",
            "Epoch: 0, Step : 1300, LR : 1.9261534960459693e-05, Avg Loss : 0.4449\n",
            "Epoch: 0, Step : 1310, LR : 1.9238777948455368e-05, Avg Loss : 0.5207\n",
            "Epoch: 0, Step : 1320, LR : 1.9216020936451046e-05, Avg Loss : 0.5742\n",
            "Epoch: 0, Step : 1330, LR : 1.919326392444672e-05, Avg Loss : 0.6023\n",
            "Epoch: 0, Step : 1340, LR : 1.91705069124424e-05, Avg Loss : 0.4017\n",
            "Epoch: 0, Step : 1350, LR : 1.9147749900438077e-05, Avg Loss : 0.5354\n",
            "Epoch: 0, Step : 1360, LR : 1.912499288843375e-05, Avg Loss : 0.4699\n",
            "Epoch: 0, Step : 1370, LR : 1.9102235876429426e-05, Avg Loss : 0.5385\n",
            "Epoch: 0, Step : 1380, LR : 1.9079478864425104e-05, Avg Loss : 0.8013\n",
            "Epoch: 0, Step : 1390, LR : 1.905672185242078e-05, Avg Loss : 0.6345\n",
            "Epoch: 0, Step : 1400, LR : 1.9033964840416454e-05, Avg Loss : 0.2340\n",
            "Epoch: 0, Step : 1410, LR : 1.9011207828412132e-05, Avg Loss : 0.3102\n",
            "Epoch: 0, Step : 1420, LR : 1.8988450816407806e-05, Avg Loss : 0.5092\n",
            "Epoch: 0, Step : 1430, LR : 1.8965693804403484e-05, Avg Loss : 0.6289\n",
            "Epoch: 0, Step : 1440, LR : 1.894293679239916e-05, Avg Loss : 0.2855\n",
            "Epoch: 0, Step : 1450, LR : 1.8920179780394837e-05, Avg Loss : 0.4959\n",
            "Epoch: 0, Step : 1460, LR : 1.8897422768390512e-05, Avg Loss : 0.4359\n",
            "Epoch: 0, Step : 1470, LR : 1.8874665756386187e-05, Avg Loss : 0.5401\n",
            "Epoch: 0, Step : 1480, LR : 1.8851908744381865e-05, Avg Loss : 0.5562\n",
            "Epoch: 0, Step : 1490, LR : 1.882915173237754e-05, Avg Loss : 0.4095\n",
            "Epoch: 0, Step : 1500, LR : 1.8806394720373214e-05, Avg Loss : 0.3043\n",
            "Epoch: 0, Step : 1510, LR : 1.8783637708368892e-05, Avg Loss : 0.4711\n",
            "Epoch: 0, Step : 1520, LR : 1.876088069636457e-05, Avg Loss : 0.3818\n",
            "Epoch: 0, Step : 1530, LR : 1.8738123684360245e-05, Avg Loss : 0.6519\n",
            "Epoch: 0, Step : 1540, LR : 1.871536667235592e-05, Avg Loss : 0.7321\n",
            "Epoch: 0, Step : 1550, LR : 1.8692609660351598e-05, Avg Loss : 0.5467\n",
            "Epoch: 0, Step : 1560, LR : 1.8669852648347272e-05, Avg Loss : 0.3378\n",
            "Epoch: 0, Step : 1570, LR : 1.864709563634295e-05, Avg Loss : 0.3746\n",
            "Epoch: 0, Step : 1580, LR : 1.8624338624338625e-05, Avg Loss : 0.3561\n",
            "Epoch: 0, Step : 1590, LR : 1.8601581612334303e-05, Avg Loss : 0.6169\n",
            "Epoch: 0, Step : 1600, LR : 1.8578824600329978e-05, Avg Loss : 0.4957\n",
            "Epoch: 0, Step : 1610, LR : 1.8556067588325656e-05, Avg Loss : 0.5844\n",
            "Epoch: 0, Step : 1620, LR : 1.853331057632133e-05, Avg Loss : 0.4755\n",
            "Epoch: 0, Step : 1630, LR : 1.8510553564317005e-05, Avg Loss : 0.6327\n",
            "Epoch: 0, Step : 1640, LR : 1.8487796552312684e-05, Avg Loss : 0.3978\n",
            "Epoch: 0, Step : 1650, LR : 1.8465039540308358e-05, Avg Loss : 0.3020\n",
            "Epoch: 0, Step : 1660, LR : 1.8442282528304033e-05, Avg Loss : 0.4060\n",
            "Epoch: 0, Step : 1670, LR : 1.841952551629971e-05, Avg Loss : 0.2985\n",
            "Epoch: 0, Step : 1680, LR : 1.839676850429539e-05, Avg Loss : 0.4569\n",
            "Epoch: 0, Step : 1690, LR : 1.8374011492291064e-05, Avg Loss : 0.5817\n",
            "Epoch: 0, Step : 1700, LR : 1.835125448028674e-05, Avg Loss : 0.5044\n",
            "Epoch: 0, Step : 1710, LR : 1.8328497468282417e-05, Avg Loss : 0.4516\n",
            "Epoch: 0, Step : 1720, LR : 1.830574045627809e-05, Avg Loss : 0.3449\n",
            "Epoch: 0, Step : 1730, LR : 1.8282983444273766e-05, Avg Loss : 0.4035\n",
            "Epoch: 0, Step : 1740, LR : 1.8260226432269444e-05, Avg Loss : 0.4507\n",
            "Epoch: 0, Step : 1750, LR : 1.8237469420265122e-05, Avg Loss : 0.3946\n",
            "Epoch: 0, Step : 1760, LR : 1.8214712408260797e-05, Avg Loss : 0.3732\n",
            "Epoch: 0, Step : 1770, LR : 1.8191955396256475e-05, Avg Loss : 0.4948\n",
            "Epoch: 0, Step : 1780, LR : 1.816919838425215e-05, Avg Loss : 0.4819\n",
            "Epoch: 0, Step : 1790, LR : 1.8146441372247824e-05, Avg Loss : 0.2605\n",
            "Epoch: 0, Step : 1800, LR : 1.8123684360243502e-05, Avg Loss : 0.4351\n",
            "Epoch: 0, Step : 1810, LR : 1.8100927348239177e-05, Avg Loss : 0.3612\n",
            "Epoch: 0, Step : 1820, LR : 1.8078170336234855e-05, Avg Loss : 0.3881\n",
            "Epoch: 0, Step : 1830, LR : 1.805541332423053e-05, Avg Loss : 0.3217\n",
            "Epoch: 0, Step : 1840, LR : 1.8032656312226208e-05, Avg Loss : 0.3845\n",
            "Epoch: 0, Step : 1850, LR : 1.8009899300221883e-05, Avg Loss : 0.4622\n",
            "Epoch: 0, Step : 1860, LR : 1.7987142288217557e-05, Avg Loss : 0.5086\n",
            "Epoch: 0, Step : 1870, LR : 1.7964385276213235e-05, Avg Loss : 0.5167\n",
            "Epoch: 0, Step : 1880, LR : 1.794162826420891e-05, Avg Loss : 0.4710\n",
            "Epoch: 0, Step : 1890, LR : 1.7918871252204585e-05, Avg Loss : 0.3728\n",
            "Epoch: 0, Step : 1900, LR : 1.7896114240200263e-05, Avg Loss : 0.6318\n",
            "Epoch: 0, Step : 1910, LR : 1.787335722819594e-05, Avg Loss : 0.3241\n",
            "Epoch: 0, Step : 1920, LR : 1.7850600216191616e-05, Avg Loss : 0.3917\n",
            "Epoch: 0, Step : 1930, LR : 1.782784320418729e-05, Avg Loss : 0.2559\n",
            "Epoch: 0, Step : 1940, LR : 1.780508619218297e-05, Avg Loss : 0.2848\n",
            "Epoch: 0, Step : 1950, LR : 1.7782329180178643e-05, Avg Loss : 0.2479\n",
            "Epoch 0 total_train_loss : 1.0326\n",
            "***** Finish To Train Epoch 0 *****\n",
            "\n",
            "*****Epoch 0 Valid Start*****\n",
            "Step : 11, valid Loss : 0.4329\n",
            "Step : 21, valid Loss : 0.4046\n",
            "Step : 31, valid Loss : 0.5500\n",
            "Step : 41, valid Loss : 0.4767\n",
            "Step : 51, valid Loss : 0.4193\n",
            "total_valid_loss :  0.4619849264621735 val_f1_score :  90.51359516616314 val_pearsonr : 92.43574307112603\n",
            "Epoch 0 total_Valid Loss : 0.4620\n",
            "*****Epoch 0 Valid Finish*****\n",
            "\n",
            "Saving epoch 0 checkpoint at /content/drive/MyDrive/AI09/model_F.ckpt.0\n",
            "****** Starting To Train Epoch #1 ******\n",
            "Epoch: 1, Step : 10, LR : 1.775274506457302e-05, Avg Loss : 0.3482\n",
            "Epoch: 1, Step : 20, LR : 1.77299880525687e-05, Avg Loss : 0.3576\n",
            "Epoch: 1, Step : 30, LR : 1.7707231040564374e-05, Avg Loss : 0.1802\n",
            "Epoch: 1, Step : 40, LR : 1.7684474028560052e-05, Avg Loss : 0.2915\n",
            "Epoch: 1, Step : 50, LR : 1.7661717016555727e-05, Avg Loss : 0.2657\n",
            "Epoch: 1, Step : 60, LR : 1.7638960004551405e-05, Avg Loss : 0.3881\n",
            "Epoch: 1, Step : 70, LR : 1.761620299254708e-05, Avg Loss : 0.3281\n",
            "Epoch: 1, Step : 80, LR : 1.7593445980542754e-05, Avg Loss : 0.2852\n",
            "Epoch: 1, Step : 90, LR : 1.7570688968538433e-05, Avg Loss : 0.3270\n",
            "Epoch: 1, Step : 100, LR : 1.7547931956534107e-05, Avg Loss : 0.2949\n",
            "Epoch: 1, Step : 110, LR : 1.7525174944529782e-05, Avg Loss : 0.3590\n",
            "Epoch: 1, Step : 120, LR : 1.7502417932525463e-05, Avg Loss : 0.2704\n",
            "Epoch: 1, Step : 130, LR : 1.7479660920521138e-05, Avg Loss : 0.2751\n",
            "Epoch: 1, Step : 140, LR : 1.7456903908516813e-05, Avg Loss : 0.4453\n",
            "Epoch: 1, Step : 150, LR : 1.743414689651249e-05, Avg Loss : 0.1924\n",
            "Epoch: 1, Step : 160, LR : 1.7411389884508166e-05, Avg Loss : 0.2475\n",
            "Epoch: 1, Step : 170, LR : 1.738863287250384e-05, Avg Loss : 0.3275\n",
            "Epoch: 1, Step : 180, LR : 1.736587586049952e-05, Avg Loss : 0.2018\n",
            "Epoch: 1, Step : 190, LR : 1.7343118848495193e-05, Avg Loss : 0.3001\n",
            "Epoch: 1, Step : 200, LR : 1.732036183649087e-05, Avg Loss : 0.2571\n",
            "Epoch: 1, Step : 210, LR : 1.7297604824486546e-05, Avg Loss : 0.2916\n",
            "Epoch: 1, Step : 220, LR : 1.7274847812482224e-05, Avg Loss : 0.2846\n",
            "Epoch: 1, Step : 230, LR : 1.72520908004779e-05, Avg Loss : 0.4097\n",
            "Epoch: 1, Step : 240, LR : 1.7229333788473573e-05, Avg Loss : 0.4711\n",
            "Epoch: 1, Step : 250, LR : 1.720657677646925e-05, Avg Loss : 0.4316\n",
            "Epoch: 1, Step : 260, LR : 1.7183819764464926e-05, Avg Loss : 0.3472\n",
            "Epoch: 1, Step : 270, LR : 1.7161062752460604e-05, Avg Loss : 0.2195\n",
            "Epoch: 1, Step : 280, LR : 1.713830574045628e-05, Avg Loss : 0.2142\n",
            "Epoch: 1, Step : 290, LR : 1.7115548728451957e-05, Avg Loss : 0.2963\n",
            "Epoch: 1, Step : 300, LR : 1.709279171644763e-05, Avg Loss : 0.2937\n",
            "Epoch: 1, Step : 310, LR : 1.7070034704443306e-05, Avg Loss : 0.3091\n",
            "Epoch: 1, Step : 320, LR : 1.7047277692438984e-05, Avg Loss : 0.2924\n",
            "Epoch: 1, Step : 330, LR : 1.702452068043466e-05, Avg Loss : 0.2704\n",
            "Epoch: 1, Step : 340, LR : 1.7001763668430337e-05, Avg Loss : 0.2193\n",
            "Epoch: 1, Step : 350, LR : 1.6979006656426015e-05, Avg Loss : 0.2905\n",
            "Epoch: 1, Step : 360, LR : 1.695624964442169e-05, Avg Loss : 0.4514\n",
            "Epoch: 1, Step : 370, LR : 1.6933492632417365e-05, Avg Loss : 0.2478\n",
            "Epoch: 1, Step : 380, LR : 1.6910735620413043e-05, Avg Loss : 0.3953\n",
            "Epoch: 1, Step : 390, LR : 1.6887978608408717e-05, Avg Loss : 0.3140\n",
            "Epoch: 1, Step : 400, LR : 1.6865221596404392e-05, Avg Loss : 0.2862\n",
            "Epoch: 1, Step : 410, LR : 1.684246458440007e-05, Avg Loss : 0.3005\n",
            "Epoch: 1, Step : 420, LR : 1.6819707572395745e-05, Avg Loss : 0.3065\n",
            "Epoch: 1, Step : 430, LR : 1.6796950560391423e-05, Avg Loss : 0.2946\n",
            "Epoch: 1, Step : 440, LR : 1.6774193548387098e-05, Avg Loss : 0.3982\n",
            "Epoch: 1, Step : 450, LR : 1.6751436536382776e-05, Avg Loss : 0.4153\n",
            "Epoch: 1, Step : 460, LR : 1.672867952437845e-05, Avg Loss : 0.2348\n",
            "Epoch: 1, Step : 470, LR : 1.6705922512374125e-05, Avg Loss : 0.6559\n",
            "Epoch: 1, Step : 480, LR : 1.6683165500369803e-05, Avg Loss : 0.2952\n",
            "Epoch: 1, Step : 490, LR : 1.6660408488365478e-05, Avg Loss : 0.3070\n",
            "Epoch: 1, Step : 500, LR : 1.6637651476361153e-05, Avg Loss : 0.2990\n",
            "Epoch: 1, Step : 510, LR : 1.661489446435683e-05, Avg Loss : 0.2490\n",
            "Epoch: 1, Step : 520, LR : 1.659213745235251e-05, Avg Loss : 0.2444\n",
            "Epoch: 1, Step : 530, LR : 1.6569380440348183e-05, Avg Loss : 0.2150\n",
            "Epoch: 1, Step : 540, LR : 1.654662342834386e-05, Avg Loss : 0.4096\n",
            "Epoch: 1, Step : 550, LR : 1.6523866416339536e-05, Avg Loss : 0.3291\n",
            "Epoch: 1, Step : 560, LR : 1.650110940433521e-05, Avg Loss : 0.5187\n",
            "Epoch: 1, Step : 570, LR : 1.647835239233089e-05, Avg Loss : 0.3010\n",
            "Epoch: 1, Step : 580, LR : 1.6455595380326567e-05, Avg Loss : 0.3560\n",
            "Epoch: 1, Step : 590, LR : 1.6432838368322242e-05, Avg Loss : 0.3037\n",
            "Epoch: 1, Step : 600, LR : 1.6410081356317916e-05, Avg Loss : 0.2987\n",
            "Epoch: 1, Step : 610, LR : 1.6387324344313594e-05, Avg Loss : 0.2373\n",
            "Epoch: 1, Step : 620, LR : 1.636456733230927e-05, Avg Loss : 0.3435\n",
            "Epoch: 1, Step : 630, LR : 1.6341810320304944e-05, Avg Loss : 0.2792\n",
            "Epoch: 1, Step : 640, LR : 1.6319053308300622e-05, Avg Loss : 0.3015\n",
            "Epoch: 1, Step : 650, LR : 1.6296296296296297e-05, Avg Loss : 0.2391\n",
            "Epoch: 1, Step : 660, LR : 1.6273539284291975e-05, Avg Loss : 0.2409\n",
            "Epoch: 1, Step : 670, LR : 1.625078227228765e-05, Avg Loss : 0.3494\n",
            "Epoch: 1, Step : 680, LR : 1.6228025260283327e-05, Avg Loss : 0.3124\n",
            "Epoch: 1, Step : 690, LR : 1.6205268248279002e-05, Avg Loss : 0.2997\n",
            "Epoch: 1, Step : 700, LR : 1.6182511236274677e-05, Avg Loss : 0.3183\n",
            "Epoch: 1, Step : 710, LR : 1.6159754224270355e-05, Avg Loss : 0.2065\n",
            "Epoch: 1, Step : 720, LR : 1.613699721226603e-05, Avg Loss : 0.4581\n",
            "Epoch: 1, Step : 730, LR : 1.6114240200261704e-05, Avg Loss : 0.3818\n",
            "Epoch: 1, Step : 740, LR : 1.6091483188257382e-05, Avg Loss : 0.2125\n",
            "Epoch: 1, Step : 750, LR : 1.606872617625306e-05, Avg Loss : 0.4229\n",
            "Epoch: 1, Step : 760, LR : 1.6045969164248735e-05, Avg Loss : 0.2081\n",
            "Epoch: 1, Step : 770, LR : 1.6023212152244413e-05, Avg Loss : 0.2701\n",
            "Epoch: 1, Step : 780, LR : 1.6000455140240088e-05, Avg Loss : 0.2802\n",
            "Epoch: 1, Step : 790, LR : 1.5977698128235763e-05, Avg Loss : 0.2604\n",
            "Epoch: 1, Step : 800, LR : 1.595494111623144e-05, Avg Loss : 0.4286\n",
            "Epoch: 1, Step : 810, LR : 1.5932184104227115e-05, Avg Loss : 0.3559\n",
            "Epoch: 1, Step : 820, LR : 1.5909427092222793e-05, Avg Loss : 0.2390\n",
            "Epoch: 1, Step : 830, LR : 1.5886670080218468e-05, Avg Loss : 0.3141\n",
            "Epoch: 1, Step : 840, LR : 1.5863913068214146e-05, Avg Loss : 0.3873\n",
            "Epoch: 1, Step : 850, LR : 1.584115605620982e-05, Avg Loss : 0.4414\n",
            "Epoch: 1, Step : 860, LR : 1.5818399044205496e-05, Avg Loss : 0.2457\n",
            "Epoch: 1, Step : 870, LR : 1.5795642032201174e-05, Avg Loss : 0.3540\n",
            "Epoch: 1, Step : 880, LR : 1.577288502019685e-05, Avg Loss : 0.2240\n",
            "Epoch: 1, Step : 890, LR : 1.5750128008192523e-05, Avg Loss : 0.3519\n",
            "Epoch: 1, Step : 900, LR : 1.57273709961882e-05, Avg Loss : 0.1739\n",
            "Epoch: 1, Step : 910, LR : 1.570461398418388e-05, Avg Loss : 0.3792\n",
            "Epoch: 1, Step : 920, LR : 1.5681856972179554e-05, Avg Loss : 0.3268\n",
            "Epoch: 1, Step : 930, LR : 1.565909996017523e-05, Avg Loss : 0.3790\n",
            "Epoch: 1, Step : 940, LR : 1.5636342948170907e-05, Avg Loss : 0.2390\n",
            "Epoch: 1, Step : 950, LR : 1.561358593616658e-05, Avg Loss : 0.2486\n",
            "Epoch: 1, Step : 960, LR : 1.559082892416226e-05, Avg Loss : 0.3686\n",
            "Epoch: 1, Step : 970, LR : 1.5568071912157938e-05, Avg Loss : 0.3634\n",
            "Epoch: 1, Step : 980, LR : 1.5545314900153612e-05, Avg Loss : 0.3740\n",
            "Epoch: 1, Step : 990, LR : 1.5522557888149287e-05, Avg Loss : 0.3200\n",
            "Epoch: 1, Step : 1000, LR : 1.5499800876144965e-05, Avg Loss : 0.2082\n",
            "Epoch: 1, Step : 1010, LR : 1.547704386414064e-05, Avg Loss : 0.3359\n",
            "Epoch: 1, Step : 1020, LR : 1.5454286852136314e-05, Avg Loss : 0.3029\n",
            "Epoch: 1, Step : 1030, LR : 1.5431529840131993e-05, Avg Loss : 0.2547\n",
            "Epoch: 1, Step : 1040, LR : 1.5408772828127667e-05, Avg Loss : 0.3015\n",
            "Epoch: 1, Step : 1050, LR : 1.5386015816123345e-05, Avg Loss : 0.3681\n",
            "Epoch: 1, Step : 1060, LR : 1.536325880411902e-05, Avg Loss : 0.2763\n",
            "Epoch: 1, Step : 1070, LR : 1.5340501792114698e-05, Avg Loss : 0.2667\n",
            "Epoch: 1, Step : 1080, LR : 1.5317744780110373e-05, Avg Loss : 0.2551\n",
            "Epoch: 1, Step : 1090, LR : 1.5294987768106047e-05, Avg Loss : 0.2575\n",
            "Epoch: 1, Step : 1100, LR : 1.5272230756101726e-05, Avg Loss : 0.3880\n",
            "Epoch: 1, Step : 1110, LR : 1.52494737440974e-05, Avg Loss : 0.2287\n",
            "Epoch: 1, Step : 1120, LR : 1.5226716732093077e-05, Avg Loss : 0.1748\n",
            "Epoch: 1, Step : 1130, LR : 1.5203959720088755e-05, Avg Loss : 0.3173\n",
            "Epoch: 1, Step : 1140, LR : 1.5181202708084431e-05, Avg Loss : 0.4094\n",
            "Epoch: 1, Step : 1150, LR : 1.5158445696080106e-05, Avg Loss : 0.3526\n",
            "Epoch: 1, Step : 1160, LR : 1.5135688684075782e-05, Avg Loss : 0.2416\n",
            "Epoch: 1, Step : 1170, LR : 1.5112931672071459e-05, Avg Loss : 0.3711\n",
            "Epoch: 1, Step : 1180, LR : 1.5090174660067133e-05, Avg Loss : 0.2338\n",
            "Epoch: 1, Step : 1190, LR : 1.506741764806281e-05, Avg Loss : 0.3383\n",
            "Epoch: 1, Step : 1200, LR : 1.5044660636058486e-05, Avg Loss : 0.3492\n",
            "Epoch: 1, Step : 1210, LR : 1.5021903624054164e-05, Avg Loss : 0.3083\n",
            "Epoch: 1, Step : 1220, LR : 1.499914661204984e-05, Avg Loss : 0.2311\n",
            "Epoch: 1, Step : 1230, LR : 1.4976389600045515e-05, Avg Loss : 0.2749\n",
            "Epoch: 1, Step : 1240, LR : 1.4953632588041192e-05, Avg Loss : 0.2834\n",
            "Epoch: 1, Step : 1250, LR : 1.4930875576036868e-05, Avg Loss : 0.2403\n",
            "Epoch: 1, Step : 1260, LR : 1.4908118564032543e-05, Avg Loss : 0.3440\n",
            "Epoch: 1, Step : 1270, LR : 1.4885361552028219e-05, Avg Loss : 0.3526\n",
            "Epoch: 1, Step : 1280, LR : 1.4862604540023897e-05, Avg Loss : 0.2199\n",
            "Epoch: 1, Step : 1290, LR : 1.4839847528019573e-05, Avg Loss : 0.1961\n",
            "Epoch: 1, Step : 1300, LR : 1.4817090516015248e-05, Avg Loss : 0.2192\n",
            "Epoch: 1, Step : 1310, LR : 1.4794333504010925e-05, Avg Loss : 0.4240\n",
            "Epoch: 1, Step : 1320, LR : 1.4771576492006601e-05, Avg Loss : 0.2720\n",
            "Epoch: 1, Step : 1330, LR : 1.4748819480002277e-05, Avg Loss : 0.2777\n",
            "Epoch: 1, Step : 1340, LR : 1.4726062467997952e-05, Avg Loss : 0.2099\n",
            "Epoch: 1, Step : 1350, LR : 1.4703305455993628e-05, Avg Loss : 0.3271\n",
            "Epoch: 1, Step : 1360, LR : 1.4680548443989306e-05, Avg Loss : 0.3275\n",
            "Epoch: 1, Step : 1370, LR : 1.4657791431984983e-05, Avg Loss : 0.3070\n",
            "Epoch: 1, Step : 1380, LR : 1.4635034419980658e-05, Avg Loss : 0.2000\n",
            "Epoch: 1, Step : 1390, LR : 1.4612277407976334e-05, Avg Loss : 0.2773\n",
            "Epoch: 1, Step : 1400, LR : 1.458952039597201e-05, Avg Loss : 0.1816\n",
            "Epoch: 1, Step : 1410, LR : 1.4566763383967685e-05, Avg Loss : 0.1751\n",
            "Epoch: 1, Step : 1420, LR : 1.4544006371963361e-05, Avg Loss : 0.1695\n",
            "Epoch: 1, Step : 1430, LR : 1.4521249359959038e-05, Avg Loss : 0.2183\n",
            "Epoch: 1, Step : 1440, LR : 1.4498492347954716e-05, Avg Loss : 0.2122\n",
            "Epoch: 1, Step : 1450, LR : 1.4475735335950392e-05, Avg Loss : 0.3044\n",
            "Epoch: 1, Step : 1460, LR : 1.4452978323946067e-05, Avg Loss : 0.2300\n",
            "Epoch: 1, Step : 1470, LR : 1.4430221311941743e-05, Avg Loss : 0.2092\n",
            "Epoch: 1, Step : 1480, LR : 1.440746429993742e-05, Avg Loss : 0.3532\n",
            "Epoch: 1, Step : 1490, LR : 1.4384707287933094e-05, Avg Loss : 0.2652\n",
            "Epoch: 1, Step : 1500, LR : 1.436195027592877e-05, Avg Loss : 0.2488\n",
            "Epoch: 1, Step : 1510, LR : 1.4339193263924447e-05, Avg Loss : 0.3334\n",
            "Epoch: 1, Step : 1520, LR : 1.4316436251920125e-05, Avg Loss : 0.2326\n",
            "Epoch: 1, Step : 1530, LR : 1.4293679239915802e-05, Avg Loss : 0.3579\n",
            "Epoch: 1, Step : 1540, LR : 1.4270922227911476e-05, Avg Loss : 0.1977\n",
            "Epoch: 1, Step : 1550, LR : 1.4248165215907153e-05, Avg Loss : 0.2556\n",
            "Epoch: 1, Step : 1560, LR : 1.4225408203902829e-05, Avg Loss : 0.3044\n",
            "Epoch: 1, Step : 1570, LR : 1.4202651191898504e-05, Avg Loss : 0.2319\n",
            "Epoch: 1, Step : 1580, LR : 1.417989417989418e-05, Avg Loss : 0.2349\n",
            "Epoch: 1, Step : 1590, LR : 1.4157137167889857e-05, Avg Loss : 0.1636\n",
            "Epoch: 1, Step : 1600, LR : 1.4134380155885535e-05, Avg Loss : 0.2748\n",
            "Epoch: 1, Step : 1610, LR : 1.411162314388121e-05, Avg Loss : 0.2281\n",
            "Epoch: 1, Step : 1620, LR : 1.4088866131876886e-05, Avg Loss : 0.3552\n",
            "Epoch: 1, Step : 1630, LR : 1.4066109119872562e-05, Avg Loss : 0.2049\n",
            "Epoch: 1, Step : 1640, LR : 1.4043352107868239e-05, Avg Loss : 0.2550\n",
            "Epoch: 1, Step : 1650, LR : 1.4020595095863913e-05, Avg Loss : 0.2094\n",
            "Epoch: 1, Step : 1660, LR : 1.399783808385959e-05, Avg Loss : 0.3283\n",
            "Epoch: 1, Step : 1670, LR : 1.3975081071855268e-05, Avg Loss : 0.1553\n",
            "Epoch: 1, Step : 1680, LR : 1.3952324059850944e-05, Avg Loss : 0.2893\n",
            "Epoch: 1, Step : 1690, LR : 1.3929567047846619e-05, Avg Loss : 0.2994\n",
            "Epoch: 1, Step : 1700, LR : 1.3906810035842295e-05, Avg Loss : 0.2418\n",
            "Epoch: 1, Step : 1710, LR : 1.3884053023837972e-05, Avg Loss : 0.2586\n",
            "Epoch: 1, Step : 1720, LR : 1.3861296011833646e-05, Avg Loss : 0.2212\n",
            "Epoch: 1, Step : 1730, LR : 1.3838538999829323e-05, Avg Loss : 0.2734\n",
            "Epoch: 1, Step : 1740, LR : 1.3815781987824999e-05, Avg Loss : 0.2371\n",
            "Epoch: 1, Step : 1750, LR : 1.3793024975820677e-05, Avg Loss : 0.2091\n",
            "Epoch: 1, Step : 1760, LR : 1.3770267963816353e-05, Avg Loss : 0.2646\n",
            "Epoch: 1, Step : 1770, LR : 1.3747510951812028e-05, Avg Loss : 0.2997\n",
            "Epoch: 1, Step : 1780, LR : 1.3724753939807705e-05, Avg Loss : 0.2544\n",
            "Epoch: 1, Step : 1790, LR : 1.3701996927803381e-05, Avg Loss : 0.2117\n",
            "Epoch: 1, Step : 1800, LR : 1.3679239915799056e-05, Avg Loss : 0.3028\n",
            "Epoch: 1, Step : 1810, LR : 1.3656482903794732e-05, Avg Loss : 0.3261\n",
            "Epoch: 1, Step : 1820, LR : 1.3633725891790408e-05, Avg Loss : 0.3773\n",
            "Epoch: 1, Step : 1830, LR : 1.3610968879786086e-05, Avg Loss : 0.2015\n",
            "Epoch: 1, Step : 1840, LR : 1.3588211867781763e-05, Avg Loss : 0.2642\n",
            "Epoch: 1, Step : 1850, LR : 1.3565454855777438e-05, Avg Loss : 0.2987\n",
            "Epoch: 1, Step : 1860, LR : 1.3542697843773114e-05, Avg Loss : 0.2662\n",
            "Epoch: 1, Step : 1870, LR : 1.351994083176879e-05, Avg Loss : 0.3851\n",
            "Epoch: 1, Step : 1880, LR : 1.3497183819764465e-05, Avg Loss : 0.2932\n",
            "Epoch: 1, Step : 1890, LR : 1.3474426807760141e-05, Avg Loss : 0.2620\n",
            "Epoch: 1, Step : 1900, LR : 1.3451669795755818e-05, Avg Loss : 0.2352\n",
            "Epoch: 1, Step : 1910, LR : 1.3428912783751496e-05, Avg Loss : 0.2244\n",
            "Epoch: 1, Step : 1920, LR : 1.340615577174717e-05, Avg Loss : 0.2620\n",
            "Epoch: 1, Step : 1930, LR : 1.3383398759742847e-05, Avg Loss : 0.3007\n",
            "Epoch: 1, Step : 1940, LR : 1.3360641747738523e-05, Avg Loss : 0.2755\n",
            "Epoch: 1, Step : 1950, LR : 1.33378847357342e-05, Avg Loss : 0.2833\n",
            "Epoch 1 total_train_loss : 0.2931\n",
            "***** Finish To Train Epoch 1 *****\n",
            "\n",
            "*****Epoch 1 Valid Start*****\n",
            "Step : 11, valid Loss : 0.3659\n",
            "Step : 21, valid Loss : 0.3335\n",
            "Step : 31, valid Loss : 0.4332\n",
            "Step : 41, valid Loss : 0.3693\n",
            "Step : 51, valid Loss : 0.4041\n",
            "total_valid_loss :  0.38733004927635195 val_f1_score :  91.67630057803467 val_pearsonr : 93.91100117141613\n",
            "Epoch 1 total_Valid Loss : 0.3873\n",
            "*****Epoch 1 Valid Finish*****\n",
            "\n",
            "Saving epoch 1 checkpoint at /content/drive/MyDrive/AI09/model_F.ckpt.1\n",
            "****** Starting To Train Epoch #2 ******\n",
            "Epoch: 2, Step : 10, LR : 1.3308300620128578e-05, Avg Loss : 0.1393\n",
            "Epoch: 2, Step : 20, LR : 1.3285543608124255e-05, Avg Loss : 0.1999\n",
            "Epoch: 2, Step : 30, LR : 1.326278659611993e-05, Avg Loss : 0.1847\n",
            "Epoch: 2, Step : 40, LR : 1.3240029584115606e-05, Avg Loss : 0.3222\n",
            "Epoch: 2, Step : 50, LR : 1.3217272572111284e-05, Avg Loss : 0.1210\n",
            "Epoch: 2, Step : 60, LR : 1.319451556010696e-05, Avg Loss : 0.2856\n",
            "Epoch: 2, Step : 70, LR : 1.3171758548102635e-05, Avg Loss : 0.1424\n",
            "Epoch: 2, Step : 80, LR : 1.3149001536098311e-05, Avg Loss : 0.1367\n",
            "Epoch: 2, Step : 90, LR : 1.3126244524093988e-05, Avg Loss : 0.1799\n",
            "Epoch: 2, Step : 100, LR : 1.3103487512089664e-05, Avg Loss : 0.2173\n",
            "Epoch: 2, Step : 110, LR : 1.3080730500085339e-05, Avg Loss : 0.1706\n",
            "Epoch: 2, Step : 120, LR : 1.3057973488081015e-05, Avg Loss : 0.1605\n",
            "Epoch: 2, Step : 130, LR : 1.3035216476076693e-05, Avg Loss : 0.1859\n",
            "Epoch: 2, Step : 140, LR : 1.301245946407237e-05, Avg Loss : 0.2053\n",
            "Epoch: 2, Step : 150, LR : 1.2989702452068044e-05, Avg Loss : 0.1790\n",
            "Epoch: 2, Step : 160, LR : 1.296694544006372e-05, Avg Loss : 0.1767\n",
            "Epoch: 2, Step : 170, LR : 1.2944188428059397e-05, Avg Loss : 0.1646\n",
            "Epoch: 2, Step : 180, LR : 1.2921431416055072e-05, Avg Loss : 0.1437\n",
            "Epoch: 2, Step : 190, LR : 1.2898674404050748e-05, Avg Loss : 0.2377\n",
            "Epoch: 2, Step : 200, LR : 1.2875917392046426e-05, Avg Loss : 0.1048\n",
            "Epoch: 2, Step : 210, LR : 1.2853160380042102e-05, Avg Loss : 0.1550\n",
            "Epoch: 2, Step : 220, LR : 1.2830403368037779e-05, Avg Loss : 0.2467\n",
            "Epoch: 2, Step : 230, LR : 1.2807646356033454e-05, Avg Loss : 0.2454\n",
            "Epoch: 2, Step : 240, LR : 1.278488934402913e-05, Avg Loss : 0.3752\n",
            "Epoch: 2, Step : 250, LR : 1.2762132332024806e-05, Avg Loss : 0.1843\n",
            "Epoch: 2, Step : 260, LR : 1.2739375320020481e-05, Avg Loss : 0.2262\n",
            "Epoch: 2, Step : 270, LR : 1.2716618308016157e-05, Avg Loss : 0.1452\n",
            "Epoch: 2, Step : 280, LR : 1.2693861296011836e-05, Avg Loss : 0.2323\n",
            "Epoch: 2, Step : 290, LR : 1.2671104284007512e-05, Avg Loss : 0.1453\n",
            "Epoch: 2, Step : 300, LR : 1.2648347272003188e-05, Avg Loss : 0.2504\n",
            "Epoch: 2, Step : 310, LR : 1.2625590259998863e-05, Avg Loss : 0.1546\n",
            "Epoch: 2, Step : 320, LR : 1.260283324799454e-05, Avg Loss : 0.2288\n",
            "Epoch: 2, Step : 330, LR : 1.2580076235990216e-05, Avg Loss : 0.2485\n",
            "Epoch: 2, Step : 340, LR : 1.255731922398589e-05, Avg Loss : 0.1358\n",
            "Epoch: 2, Step : 350, LR : 1.2534562211981567e-05, Avg Loss : 0.2571\n",
            "Epoch: 2, Step : 360, LR : 1.2511805199977245e-05, Avg Loss : 0.1368\n",
            "Epoch: 2, Step : 370, LR : 1.2489048187972921e-05, Avg Loss : 0.1466\n",
            "Epoch: 2, Step : 380, LR : 1.2466291175968596e-05, Avg Loss : 0.2013\n",
            "Epoch: 2, Step : 390, LR : 1.2443534163964272e-05, Avg Loss : 0.1876\n",
            "Epoch: 2, Step : 400, LR : 1.2420777151959949e-05, Avg Loss : 0.1647\n",
            "Epoch: 2, Step : 410, LR : 1.2398020139955625e-05, Avg Loss : 0.1315\n",
            "Epoch: 2, Step : 420, LR : 1.23752631279513e-05, Avg Loss : 0.1361\n",
            "Epoch: 2, Step : 430, LR : 1.2352506115946976e-05, Avg Loss : 0.1590\n",
            "Epoch: 2, Step : 440, LR : 1.2329749103942654e-05, Avg Loss : 0.1440\n",
            "Epoch: 2, Step : 450, LR : 1.230699209193833e-05, Avg Loss : 0.2069\n",
            "Epoch: 2, Step : 460, LR : 1.2284235079934005e-05, Avg Loss : 0.1795\n",
            "Epoch: 2, Step : 470, LR : 1.2261478067929682e-05, Avg Loss : 0.1771\n",
            "Epoch: 2, Step : 480, LR : 1.2238721055925358e-05, Avg Loss : 0.1917\n",
            "Epoch: 2, Step : 490, LR : 1.2215964043921033e-05, Avg Loss : 0.2227\n",
            "Epoch: 2, Step : 500, LR : 1.219320703191671e-05, Avg Loss : 0.3224\n",
            "Epoch: 2, Step : 510, LR : 1.2170450019912386e-05, Avg Loss : 0.2161\n",
            "Epoch: 2, Step : 520, LR : 1.2147693007908064e-05, Avg Loss : 0.1071\n",
            "Epoch: 2, Step : 530, LR : 1.212493599590374e-05, Avg Loss : 0.1259\n",
            "Epoch: 2, Step : 540, LR : 1.2102178983899415e-05, Avg Loss : 0.1154\n",
            "Epoch: 2, Step : 550, LR : 1.2079421971895091e-05, Avg Loss : 0.2303\n",
            "Epoch: 2, Step : 560, LR : 1.2056664959890768e-05, Avg Loss : 0.2449\n",
            "Epoch: 2, Step : 570, LR : 1.2033907947886442e-05, Avg Loss : 0.1700\n",
            "Epoch: 2, Step : 580, LR : 1.2011150935882119e-05, Avg Loss : 0.1920\n",
            "Epoch: 2, Step : 590, LR : 1.1988393923877797e-05, Avg Loss : 0.1662\n",
            "Epoch: 2, Step : 600, LR : 1.1965636911873473e-05, Avg Loss : 0.2277\n",
            "Epoch: 2, Step : 610, LR : 1.194287989986915e-05, Avg Loss : 0.2004\n",
            "Epoch: 2, Step : 620, LR : 1.1920122887864824e-05, Avg Loss : 0.2086\n",
            "Epoch: 2, Step : 630, LR : 1.18973658758605e-05, Avg Loss : 0.1750\n",
            "Epoch: 2, Step : 640, LR : 1.1874608863856177e-05, Avg Loss : 0.2100\n",
            "Epoch: 2, Step : 650, LR : 1.1851851851851852e-05, Avg Loss : 0.1756\n",
            "Epoch: 2, Step : 660, LR : 1.1829094839847528e-05, Avg Loss : 0.1977\n",
            "Epoch: 2, Step : 670, LR : 1.1806337827843206e-05, Avg Loss : 0.1627\n",
            "Epoch: 2, Step : 680, LR : 1.1783580815838882e-05, Avg Loss : 0.2110\n",
            "Epoch: 2, Step : 690, LR : 1.1760823803834557e-05, Avg Loss : 0.1986\n",
            "Epoch: 2, Step : 700, LR : 1.1738066791830234e-05, Avg Loss : 0.1535\n",
            "Epoch: 2, Step : 710, LR : 1.171530977982591e-05, Avg Loss : 0.2528\n",
            "Epoch: 2, Step : 720, LR : 1.1692552767821586e-05, Avg Loss : 0.1313\n",
            "Epoch: 2, Step : 730, LR : 1.1669795755817261e-05, Avg Loss : 0.1985\n",
            "Epoch: 2, Step : 740, LR : 1.1647038743812937e-05, Avg Loss : 0.1880\n",
            "Epoch: 2, Step : 750, LR : 1.1624281731808615e-05, Avg Loss : 0.1815\n",
            "Epoch: 2, Step : 760, LR : 1.1601524719804292e-05, Avg Loss : 0.1864\n",
            "Epoch: 2, Step : 770, LR : 1.1578767707799967e-05, Avg Loss : 0.2326\n",
            "Epoch: 2, Step : 780, LR : 1.1556010695795643e-05, Avg Loss : 0.1441\n",
            "Epoch: 2, Step : 790, LR : 1.153325368379132e-05, Avg Loss : 0.1240\n",
            "Epoch: 2, Step : 800, LR : 1.1510496671786994e-05, Avg Loss : 0.1511\n",
            "Epoch: 2, Step : 810, LR : 1.148773965978267e-05, Avg Loss : 0.1669\n",
            "Epoch: 2, Step : 820, LR : 1.1464982647778347e-05, Avg Loss : 0.1228\n",
            "Epoch: 2, Step : 830, LR : 1.1442225635774025e-05, Avg Loss : 0.1341\n",
            "Epoch: 2, Step : 840, LR : 1.1419468623769701e-05, Avg Loss : 0.1130\n",
            "Epoch: 2, Step : 850, LR : 1.1396711611765376e-05, Avg Loss : 0.2021\n",
            "Epoch: 2, Step : 860, LR : 1.1373954599761052e-05, Avg Loss : 0.1635\n",
            "Epoch: 2, Step : 870, LR : 1.1351197587756729e-05, Avg Loss : 0.2364\n",
            "Epoch: 2, Step : 880, LR : 1.1328440575752403e-05, Avg Loss : 0.1698\n",
            "Epoch: 2, Step : 890, LR : 1.130568356374808e-05, Avg Loss : 0.1867\n",
            "Epoch: 2, Step : 900, LR : 1.1282926551743758e-05, Avg Loss : 0.2189\n",
            "Epoch: 2, Step : 910, LR : 1.1260169539739434e-05, Avg Loss : 0.1888\n",
            "Epoch: 2, Step : 920, LR : 1.123741252773511e-05, Avg Loss : 0.1497\n",
            "Epoch: 2, Step : 930, LR : 1.1214655515730785e-05, Avg Loss : 0.1772\n",
            "Epoch: 2, Step : 940, LR : 1.1191898503726462e-05, Avg Loss : 0.1376\n",
            "Epoch: 2, Step : 950, LR : 1.1169141491722138e-05, Avg Loss : 0.2085\n",
            "Epoch: 2, Step : 960, LR : 1.1146384479717813e-05, Avg Loss : 0.1957\n",
            "Epoch: 2, Step : 970, LR : 1.112362746771349e-05, Avg Loss : 0.2582\n",
            "Epoch: 2, Step : 980, LR : 1.1100870455709167e-05, Avg Loss : 0.1378\n",
            "Epoch: 2, Step : 990, LR : 1.1078113443704844e-05, Avg Loss : 0.1749\n",
            "Epoch: 2, Step : 1000, LR : 1.1055356431700518e-05, Avg Loss : 0.0955\n",
            "Epoch: 2, Step : 1010, LR : 1.1032599419696195e-05, Avg Loss : 0.2371\n",
            "Epoch: 2, Step : 1020, LR : 1.1009842407691871e-05, Avg Loss : 0.1908\n",
            "Epoch: 2, Step : 1030, LR : 1.0987085395687548e-05, Avg Loss : 0.1716\n",
            "Epoch: 2, Step : 1040, LR : 1.0964328383683222e-05, Avg Loss : 0.1914\n",
            "Epoch: 2, Step : 1050, LR : 1.0941571371678899e-05, Avg Loss : 0.2328\n",
            "Epoch: 2, Step : 1060, LR : 1.0918814359674577e-05, Avg Loss : 0.1153\n",
            "Epoch: 2, Step : 1070, LR : 1.0896057347670253e-05, Avg Loss : 0.1891\n",
            "Epoch: 2, Step : 1080, LR : 1.0873300335665928e-05, Avg Loss : 0.2042\n",
            "Epoch: 2, Step : 1090, LR : 1.0850543323661604e-05, Avg Loss : 0.1609\n",
            "Epoch: 2, Step : 1100, LR : 1.082778631165728e-05, Avg Loss : 0.2464\n",
            "Epoch: 2, Step : 1110, LR : 1.0805029299652955e-05, Avg Loss : 0.1735\n",
            "Epoch: 2, Step : 1120, LR : 1.0782272287648632e-05, Avg Loss : 0.1977\n",
            "Epoch: 2, Step : 1130, LR : 1.0759515275644308e-05, Avg Loss : 0.1933\n",
            "Epoch: 2, Step : 1140, LR : 1.0736758263639986e-05, Avg Loss : 0.1451\n",
            "Epoch: 2, Step : 1150, LR : 1.0714001251635662e-05, Avg Loss : 0.1201\n",
            "Epoch: 2, Step : 1160, LR : 1.0691244239631337e-05, Avg Loss : 0.2122\n",
            "Epoch: 2, Step : 1170, LR : 1.0668487227627014e-05, Avg Loss : 0.1967\n",
            "Epoch: 2, Step : 1180, LR : 1.064573021562269e-05, Avg Loss : 0.1198\n",
            "Epoch: 2, Step : 1190, LR : 1.0622973203618365e-05, Avg Loss : 0.2138\n",
            "Epoch: 2, Step : 1200, LR : 1.0600216191614041e-05, Avg Loss : 0.2055\n",
            "Epoch: 2, Step : 1210, LR : 1.0577459179609717e-05, Avg Loss : 0.1455\n",
            "Epoch: 2, Step : 1220, LR : 1.0554702167605395e-05, Avg Loss : 0.1704\n",
            "Epoch: 2, Step : 1230, LR : 1.053194515560107e-05, Avg Loss : 0.2121\n",
            "Epoch: 2, Step : 1240, LR : 1.0509188143596747e-05, Avg Loss : 0.1548\n",
            "Epoch: 2, Step : 1250, LR : 1.0486431131592423e-05, Avg Loss : 0.1317\n",
            "Epoch: 2, Step : 1260, LR : 1.04636741195881e-05, Avg Loss : 0.2109\n",
            "Epoch: 2, Step : 1270, LR : 1.0440917107583774e-05, Avg Loss : 0.2026\n",
            "Epoch: 2, Step : 1280, LR : 1.041816009557945e-05, Avg Loss : 0.2156\n",
            "Epoch: 2, Step : 1290, LR : 1.0395403083575128e-05, Avg Loss : 0.1491\n",
            "Epoch: 2, Step : 1300, LR : 1.0372646071570805e-05, Avg Loss : 0.1661\n",
            "Epoch: 2, Step : 1310, LR : 1.034988905956648e-05, Avg Loss : 0.1556\n",
            "Epoch: 2, Step : 1320, LR : 1.0327132047562156e-05, Avg Loss : 0.2492\n",
            "Epoch: 2, Step : 1330, LR : 1.0304375035557832e-05, Avg Loss : 0.1866\n",
            "Epoch: 2, Step : 1340, LR : 1.0281618023553509e-05, Avg Loss : 0.2352\n",
            "Epoch: 2, Step : 1350, LR : 1.0258861011549183e-05, Avg Loss : 0.1915\n",
            "Epoch: 2, Step : 1360, LR : 1.023610399954486e-05, Avg Loss : 0.1606\n",
            "Epoch: 2, Step : 1370, LR : 1.0213346987540538e-05, Avg Loss : 0.1786\n",
            "Epoch: 2, Step : 1380, LR : 1.0190589975536214e-05, Avg Loss : 0.1665\n",
            "Epoch: 2, Step : 1390, LR : 1.0167832963531889e-05, Avg Loss : 0.1676\n",
            "Epoch: 2, Step : 1400, LR : 1.0145075951527565e-05, Avg Loss : 0.1424\n",
            "Epoch: 2, Step : 1410, LR : 1.0122318939523242e-05, Avg Loss : 0.2908\n",
            "Epoch: 2, Step : 1420, LR : 1.0099561927518916e-05, Avg Loss : 0.1381\n",
            "Epoch: 2, Step : 1430, LR : 1.0076804915514593e-05, Avg Loss : 0.1045\n",
            "Epoch: 2, Step : 1440, LR : 1.005404790351027e-05, Avg Loss : 0.1986\n",
            "Epoch: 2, Step : 1450, LR : 1.0031290891505947e-05, Avg Loss : 0.1691\n",
            "Epoch: 2, Step : 1460, LR : 1.0008533879501624e-05, Avg Loss : 0.1073\n",
            "Epoch: 2, Step : 1470, LR : 9.985776867497298e-06, Avg Loss : 0.1510\n",
            "Epoch: 2, Step : 1480, LR : 9.963019855492975e-06, Avg Loss : 0.1343\n",
            "Epoch: 2, Step : 1490, LR : 9.940262843488651e-06, Avg Loss : 0.2143\n",
            "Epoch: 2, Step : 1500, LR : 9.917505831484326e-06, Avg Loss : 0.1882\n",
            "Epoch: 2, Step : 1510, LR : 9.894748819480004e-06, Avg Loss : 0.2381\n",
            "Epoch: 2, Step : 1520, LR : 9.871991807475679e-06, Avg Loss : 0.0892\n",
            "Epoch: 2, Step : 1530, LR : 9.849234795471355e-06, Avg Loss : 0.1437\n",
            "Epoch: 2, Step : 1540, LR : 9.826477783467031e-06, Avg Loss : 0.2337\n",
            "Epoch: 2, Step : 1550, LR : 9.803720771462708e-06, Avg Loss : 0.2175\n",
            "Epoch: 2, Step : 1560, LR : 9.780963759458384e-06, Avg Loss : 0.0795\n",
            "Epoch: 2, Step : 1570, LR : 9.75820674745406e-06, Avg Loss : 0.0916\n",
            "Epoch: 2, Step : 1580, LR : 9.735449735449735e-06, Avg Loss : 0.1381\n",
            "Epoch: 2, Step : 1590, LR : 9.712692723445413e-06, Avg Loss : 0.1630\n",
            "Epoch: 2, Step : 1600, LR : 9.689935711441088e-06, Avg Loss : 0.1346\n",
            "Epoch: 2, Step : 1610, LR : 9.667178699436764e-06, Avg Loss : 0.2228\n",
            "Epoch: 2, Step : 1620, LR : 9.64442168743244e-06, Avg Loss : 0.2635\n",
            "Epoch: 2, Step : 1630, LR : 9.621664675428117e-06, Avg Loss : 0.3115\n",
            "Epoch: 2, Step : 1640, LR : 9.598907663423794e-06, Avg Loss : 0.1045\n",
            "Epoch: 2, Step : 1650, LR : 9.57615065141947e-06, Avg Loss : 0.1244\n",
            "Epoch: 2, Step : 1660, LR : 9.553393639415146e-06, Avg Loss : 0.1324\n",
            "Epoch: 2, Step : 1670, LR : 9.530636627410823e-06, Avg Loss : 0.0973\n",
            "Epoch: 2, Step : 1680, LR : 9.507879615406497e-06, Avg Loss : 0.1490\n",
            "Epoch: 2, Step : 1690, LR : 9.485122603402174e-06, Avg Loss : 0.1853\n",
            "Epoch: 2, Step : 1700, LR : 9.46236559139785e-06, Avg Loss : 0.1328\n",
            "Epoch: 2, Step : 1710, LR : 9.439608579393527e-06, Avg Loss : 0.1269\n",
            "Epoch: 2, Step : 1720, LR : 9.416851567389203e-06, Avg Loss : 0.2799\n",
            "Epoch: 2, Step : 1730, LR : 9.394094555384878e-06, Avg Loss : 0.1607\n",
            "Epoch: 2, Step : 1740, LR : 9.371337543380556e-06, Avg Loss : 0.1220\n",
            "Epoch: 2, Step : 1750, LR : 9.34858053137623e-06, Avg Loss : 0.2038\n",
            "Epoch: 2, Step : 1760, LR : 9.325823519371907e-06, Avg Loss : 0.1263\n",
            "Epoch: 2, Step : 1770, LR : 9.303066507367583e-06, Avg Loss : 0.1519\n",
            "Epoch: 2, Step : 1780, LR : 9.28030949536326e-06, Avg Loss : 0.1439\n",
            "Epoch: 2, Step : 1790, LR : 9.257552483358936e-06, Avg Loss : 0.1257\n",
            "Epoch: 2, Step : 1800, LR : 9.234795471354612e-06, Avg Loss : 0.1297\n",
            "Epoch: 2, Step : 1810, LR : 9.212038459350287e-06, Avg Loss : 0.2315\n",
            "Epoch: 2, Step : 1820, LR : 9.189281447345965e-06, Avg Loss : 0.1939\n",
            "Epoch: 2, Step : 1830, LR : 9.16652443534164e-06, Avg Loss : 0.1728\n",
            "Epoch: 2, Step : 1840, LR : 9.143767423337316e-06, Avg Loss : 0.1299\n",
            "Epoch: 2, Step : 1850, LR : 9.121010411332993e-06, Avg Loss : 0.1621\n",
            "Epoch: 2, Step : 1860, LR : 9.098253399328669e-06, Avg Loss : 0.2129\n",
            "Epoch: 2, Step : 1870, LR : 9.075496387324345e-06, Avg Loss : 0.1638\n",
            "Epoch: 2, Step : 1880, LR : 9.052739375320022e-06, Avg Loss : 0.1367\n",
            "Epoch: 2, Step : 1890, LR : 9.029982363315696e-06, Avg Loss : 0.1862\n",
            "Epoch: 2, Step : 1900, LR : 9.007225351311374e-06, Avg Loss : 0.2240\n",
            "Epoch: 2, Step : 1910, LR : 8.98446833930705e-06, Avg Loss : 0.1777\n",
            "Epoch: 2, Step : 1920, LR : 8.961711327302726e-06, Avg Loss : 0.2020\n",
            "Epoch: 2, Step : 1930, LR : 8.938954315298402e-06, Avg Loss : 0.1487\n",
            "Epoch: 2, Step : 1940, LR : 8.916197303294078e-06, Avg Loss : 0.1483\n",
            "Epoch: 2, Step : 1950, LR : 8.893440291289755e-06, Avg Loss : 0.2159\n",
            "Epoch 2 total_train_loss : 0.1796\n",
            "***** Finish To Train Epoch 2 *****\n",
            "\n",
            "*****Epoch 2 Valid Start*****\n",
            "Step : 11, valid Loss : 0.3187\n",
            "Step : 21, valid Loss : 0.3075\n",
            "Step : 31, valid Loss : 0.3946\n",
            "Step : 41, valid Loss : 0.3365\n",
            "Step : 51, valid Loss : 0.3614\n",
            "total_valid_loss :  0.34988811910152434 val_f1_score :  91.73457508731083 val_pearsonr : 94.43672821023333\n",
            "Epoch 2 total_Valid Loss : 0.3499\n",
            "*****Epoch 2 Valid Finish*****\n",
            "\n",
            "Saving epoch 2 checkpoint at /content/drive/MyDrive/AI09/model_F.ckpt.2\n",
            "****** Starting To Train Epoch #3 ******\n",
            "Epoch: 3, Step : 10, LR : 8.863856175684133e-06, Avg Loss : 0.1326\n",
            "Epoch: 3, Step : 20, LR : 8.84109916367981e-06, Avg Loss : 0.1161\n",
            "Epoch: 3, Step : 30, LR : 8.818342151675486e-06, Avg Loss : 0.0718\n",
            "Epoch: 3, Step : 40, LR : 8.795585139671162e-06, Avg Loss : 0.1276\n",
            "Epoch: 3, Step : 50, LR : 8.772828127666839e-06, Avg Loss : 0.1135\n",
            "Epoch: 3, Step : 60, LR : 8.750071115662513e-06, Avg Loss : 0.1189\n",
            "Epoch: 3, Step : 70, LR : 8.72731410365819e-06, Avg Loss : 0.1258\n",
            "Epoch: 3, Step : 80, LR : 8.704557091653866e-06, Avg Loss : 0.1095\n",
            "Epoch: 3, Step : 90, LR : 8.681800079649543e-06, Avg Loss : 0.1206\n",
            "Epoch: 3, Step : 100, LR : 8.659043067645219e-06, Avg Loss : 0.1418\n",
            "Epoch: 3, Step : 110, LR : 8.636286055640895e-06, Avg Loss : 0.1014\n",
            "Epoch: 3, Step : 120, LR : 8.613529043636572e-06, Avg Loss : 0.1246\n",
            "Epoch: 3, Step : 130, LR : 8.590772031632248e-06, Avg Loss : 0.1316\n",
            "Epoch: 3, Step : 140, LR : 8.568015019627923e-06, Avg Loss : 0.1145\n",
            "Epoch: 3, Step : 150, LR : 8.545258007623601e-06, Avg Loss : 0.1157\n",
            "Epoch: 3, Step : 160, LR : 8.522500995619276e-06, Avg Loss : 0.1228\n",
            "Epoch: 3, Step : 170, LR : 8.499743983614952e-06, Avg Loss : 0.1546\n",
            "Epoch: 3, Step : 180, LR : 8.476986971610628e-06, Avg Loss : 0.1223\n",
            "Epoch: 3, Step : 190, LR : 8.454229959606305e-06, Avg Loss : 0.0882\n",
            "Epoch: 3, Step : 200, LR : 8.431472947601981e-06, Avg Loss : 0.1220\n",
            "Epoch: 3, Step : 210, LR : 8.408715935597656e-06, Avg Loss : 0.2116\n",
            "Epoch: 3, Step : 220, LR : 8.385958923593332e-06, Avg Loss : 0.0973\n",
            "Epoch: 3, Step : 230, LR : 8.36320191158901e-06, Avg Loss : 0.1140\n",
            "Epoch: 3, Step : 240, LR : 8.340444899584685e-06, Avg Loss : 0.1436\n",
            "Epoch: 3, Step : 250, LR : 8.317687887580361e-06, Avg Loss : 0.1269\n",
            "Epoch: 3, Step : 260, LR : 8.294930875576038e-06, Avg Loss : 0.1160\n",
            "Epoch: 3, Step : 270, LR : 8.272173863571714e-06, Avg Loss : 0.1474\n",
            "Epoch: 3, Step : 280, LR : 8.24941685156739e-06, Avg Loss : 0.1081\n",
            "Epoch: 3, Step : 290, LR : 8.226659839563065e-06, Avg Loss : 0.0812\n",
            "Epoch: 3, Step : 300, LR : 8.203902827558742e-06, Avg Loss : 0.1189\n",
            "Epoch: 3, Step : 310, LR : 8.181145815554418e-06, Avg Loss : 0.0874\n",
            "Epoch: 3, Step : 320, LR : 8.158388803550094e-06, Avg Loss : 0.0845\n",
            "Epoch: 3, Step : 330, LR : 8.13563179154577e-06, Avg Loss : 0.1127\n",
            "Epoch: 3, Step : 340, LR : 8.112874779541447e-06, Avg Loss : 0.1405\n",
            "Epoch: 3, Step : 350, LR : 8.090117767537124e-06, Avg Loss : 0.1040\n",
            "Epoch: 3, Step : 360, LR : 8.0673607555328e-06, Avg Loss : 0.1741\n",
            "Epoch: 3, Step : 370, LR : 8.044603743528475e-06, Avg Loss : 0.0984\n",
            "Epoch: 3, Step : 380, LR : 8.021846731524151e-06, Avg Loss : 0.0861\n",
            "Epoch: 3, Step : 390, LR : 7.999089719519827e-06, Avg Loss : 0.1551\n",
            "Epoch: 3, Step : 400, LR : 7.976332707515504e-06, Avg Loss : 0.0943\n",
            "Epoch: 3, Step : 410, LR : 7.95357569551118e-06, Avg Loss : 0.1133\n",
            "Epoch: 3, Step : 420, LR : 7.930818683506857e-06, Avg Loss : 0.0896\n",
            "Epoch: 3, Step : 430, LR : 7.908061671502533e-06, Avg Loss : 0.0740\n",
            "Epoch: 3, Step : 440, LR : 7.88530465949821e-06, Avg Loss : 0.1297\n",
            "Epoch: 3, Step : 450, LR : 7.862547647493884e-06, Avg Loss : 0.1023\n",
            "Epoch: 3, Step : 460, LR : 7.83979063548956e-06, Avg Loss : 0.1111\n",
            "Epoch: 3, Step : 470, LR : 7.817033623485237e-06, Avg Loss : 0.0917\n",
            "Epoch: 3, Step : 480, LR : 7.794276611480913e-06, Avg Loss : 0.1143\n",
            "Epoch: 3, Step : 490, LR : 7.77151959947659e-06, Avg Loss : 0.1381\n",
            "Epoch: 3, Step : 500, LR : 7.748762587472266e-06, Avg Loss : 0.1716\n",
            "Epoch: 3, Step : 510, LR : 7.726005575467942e-06, Avg Loss : 0.1143\n",
            "Epoch: 3, Step : 520, LR : 7.703248563463617e-06, Avg Loss : 0.1033\n",
            "Epoch: 3, Step : 530, LR : 7.680491551459293e-06, Avg Loss : 0.1366\n",
            "Epoch: 3, Step : 540, LR : 7.657734539454971e-06, Avg Loss : 0.1204\n",
            "Epoch: 3, Step : 550, LR : 7.634977527450646e-06, Avg Loss : 0.1423\n",
            "Epoch: 3, Step : 560, LR : 7.6122205154463225e-06, Avg Loss : 0.1033\n",
            "Epoch: 3, Step : 570, LR : 7.589463503441998e-06, Avg Loss : 0.1062\n",
            "Epoch: 3, Step : 580, LR : 7.566706491437675e-06, Avg Loss : 0.0799\n",
            "Epoch: 3, Step : 590, LR : 7.543949479433351e-06, Avg Loss : 0.1065\n",
            "Epoch: 3, Step : 600, LR : 7.521192467429027e-06, Avg Loss : 0.0885\n",
            "Epoch: 3, Step : 610, LR : 7.498435455424703e-06, Avg Loss : 0.0833\n",
            "Epoch: 3, Step : 620, LR : 7.47567844342038e-06, Avg Loss : 0.1011\n",
            "Epoch: 3, Step : 630, LR : 7.4529214314160556e-06, Avg Loss : 0.1008\n",
            "Epoch: 3, Step : 640, LR : 7.430164419411732e-06, Avg Loss : 0.0890\n",
            "Epoch: 3, Step : 650, LR : 7.4074074074074075e-06, Avg Loss : 0.1930\n",
            "Epoch: 3, Step : 660, LR : 7.384650395403085e-06, Avg Loss : 0.1099\n",
            "Epoch: 3, Step : 670, LR : 7.36189338339876e-06, Avg Loss : 0.1005\n",
            "Epoch: 3, Step : 680, LR : 7.339136371394437e-06, Avg Loss : 0.1213\n",
            "Epoch: 3, Step : 690, LR : 7.316379359390112e-06, Avg Loss : 0.1061\n",
            "Epoch: 3, Step : 700, LR : 7.293622347385789e-06, Avg Loss : 0.0883\n",
            "Epoch: 3, Step : 710, LR : 7.270865335381465e-06, Avg Loss : 0.0924\n",
            "Epoch: 3, Step : 720, LR : 7.248108323377141e-06, Avg Loss : 0.1108\n",
            "Epoch: 3, Step : 730, LR : 7.225351311372817e-06, Avg Loss : 0.0837\n",
            "Epoch: 3, Step : 740, LR : 7.202594299368494e-06, Avg Loss : 0.0977\n",
            "Epoch: 3, Step : 750, LR : 7.17983728736417e-06, Avg Loss : 0.0980\n",
            "Epoch: 3, Step : 760, LR : 7.157080275359846e-06, Avg Loss : 0.1467\n",
            "Epoch: 3, Step : 770, LR : 7.1343232633555216e-06, Avg Loss : 0.1047\n",
            "Epoch: 3, Step : 780, LR : 7.111566251351199e-06, Avg Loss : 0.0831\n",
            "Epoch: 3, Step : 790, LR : 7.088809239346874e-06, Avg Loss : 0.1207\n",
            "Epoch: 3, Step : 800, LR : 7.06605222734255e-06, Avg Loss : 0.0977\n",
            "Epoch: 3, Step : 810, LR : 7.043295215338226e-06, Avg Loss : 0.1214\n",
            "Epoch: 3, Step : 820, LR : 7.0205382033339035e-06, Avg Loss : 0.0956\n",
            "Epoch: 3, Step : 830, LR : 6.997781191329579e-06, Avg Loss : 0.1316\n",
            "Epoch: 3, Step : 840, LR : 6.975024179325255e-06, Avg Loss : 0.1066\n",
            "Epoch: 3, Step : 850, LR : 6.952267167320932e-06, Avg Loss : 0.1208\n",
            "Epoch: 3, Step : 860, LR : 6.929510155316607e-06, Avg Loss : 0.1350\n",
            "Epoch: 3, Step : 870, LR : 6.906753143312284e-06, Avg Loss : 0.0903\n",
            "Epoch: 3, Step : 880, LR : 6.883996131307959e-06, Avg Loss : 0.0846\n",
            "Epoch: 3, Step : 890, LR : 6.8612391193036365e-06, Avg Loss : 0.1203\n",
            "Epoch: 3, Step : 900, LR : 6.838482107299312e-06, Avg Loss : 0.0939\n",
            "Epoch: 3, Step : 910, LR : 6.8157250952949884e-06, Avg Loss : 0.1382\n",
            "Epoch: 3, Step : 920, LR : 6.792968083290664e-06, Avg Loss : 0.1070\n",
            "Epoch: 3, Step : 930, LR : 6.770211071286341e-06, Avg Loss : 0.1052\n",
            "Epoch: 3, Step : 940, LR : 6.747454059282017e-06, Avg Loss : 0.1538\n",
            "Epoch: 3, Step : 950, LR : 6.724697047277693e-06, Avg Loss : 0.0901\n",
            "Epoch: 3, Step : 960, LR : 6.701940035273369e-06, Avg Loss : 0.0880\n",
            "Epoch: 3, Step : 970, LR : 6.679183023269046e-06, Avg Loss : 0.0908\n",
            "Epoch: 3, Step : 980, LR : 6.6564260112647214e-06, Avg Loss : 0.1194\n",
            "Epoch: 3, Step : 990, LR : 6.633668999260398e-06, Avg Loss : 0.0995\n",
            "Epoch: 3, Step : 1000, LR : 6.610911987256073e-06, Avg Loss : 0.1081\n",
            "Epoch: 3, Step : 1010, LR : 6.588154975251751e-06, Avg Loss : 0.0851\n",
            "Epoch: 3, Step : 1020, LR : 6.565397963247426e-06, Avg Loss : 0.0831\n",
            "Epoch: 3, Step : 1030, LR : 6.5426409512431025e-06, Avg Loss : 0.1354\n",
            "Epoch: 3, Step : 1040, LR : 6.519883939238778e-06, Avg Loss : 0.1420\n",
            "Epoch: 3, Step : 1050, LR : 6.497126927234455e-06, Avg Loss : 0.1113\n",
            "Epoch: 3, Step : 1060, LR : 6.474369915230131e-06, Avg Loss : 0.0772\n",
            "Epoch: 3, Step : 1070, LR : 6.451612903225806e-06, Avg Loss : 0.1358\n",
            "Epoch: 3, Step : 1080, LR : 6.428855891221483e-06, Avg Loss : 0.1187\n",
            "Epoch: 3, Step : 1090, LR : 6.40609887921716e-06, Avg Loss : 0.0794\n",
            "Epoch: 3, Step : 1100, LR : 6.3833418672128355e-06, Avg Loss : 0.1259\n",
            "Epoch: 3, Step : 1110, LR : 6.360584855208511e-06, Avg Loss : 0.1336\n",
            "Epoch: 3, Step : 1120, LR : 6.3378278432041875e-06, Avg Loss : 0.1118\n",
            "Epoch: 3, Step : 1130, LR : 6.315070831199865e-06, Avg Loss : 0.1219\n",
            "Epoch: 3, Step : 1140, LR : 6.29231381919554e-06, Avg Loss : 0.1089\n",
            "Epoch: 3, Step : 1150, LR : 6.269556807191216e-06, Avg Loss : 0.1189\n",
            "Epoch: 3, Step : 1160, LR : 6.246799795186892e-06, Avg Loss : 0.1005\n",
            "Epoch: 3, Step : 1170, LR : 6.2240427831825685e-06, Avg Loss : 0.0912\n",
            "Epoch: 3, Step : 1180, LR : 6.201285771178245e-06, Avg Loss : 0.0675\n",
            "Epoch: 3, Step : 1190, LR : 6.1785287591739205e-06, Avg Loss : 0.1273\n",
            "Epoch: 3, Step : 1200, LR : 6.155771747169598e-06, Avg Loss : 0.0768\n",
            "Epoch: 3, Step : 1210, LR : 6.133014735165273e-06, Avg Loss : 0.1248\n",
            "Epoch: 3, Step : 1220, LR : 6.11025772316095e-06, Avg Loss : 0.2016\n",
            "Epoch: 3, Step : 1230, LR : 6.087500711156625e-06, Avg Loss : 0.0765\n",
            "Epoch: 3, Step : 1240, LR : 6.064743699152302e-06, Avg Loss : 0.1138\n",
            "Epoch: 3, Step : 1250, LR : 6.041986687147978e-06, Avg Loss : 0.1584\n",
            "Epoch: 3, Step : 1260, LR : 6.019229675143654e-06, Avg Loss : 0.0988\n",
            "Epoch: 3, Step : 1270, LR : 5.99647266313933e-06, Avg Loss : 0.0967\n",
            "Epoch: 3, Step : 1280, LR : 5.973715651135007e-06, Avg Loss : 0.1256\n",
            "Epoch: 3, Step : 1290, LR : 5.950958639130683e-06, Avg Loss : 0.1132\n",
            "Epoch: 3, Step : 1300, LR : 5.928201627126359e-06, Avg Loss : 0.1967\n",
            "Epoch: 3, Step : 1310, LR : 5.9054446151220346e-06, Avg Loss : 0.1698\n",
            "Epoch: 3, Step : 1320, LR : 5.882687603117712e-06, Avg Loss : 0.0749\n",
            "Epoch: 3, Step : 1330, LR : 5.859930591113387e-06, Avg Loss : 0.1019\n",
            "Epoch: 3, Step : 1340, LR : 5.837173579109064e-06, Avg Loss : 0.0669\n",
            "Epoch: 3, Step : 1350, LR : 5.814416567104739e-06, Avg Loss : 0.0915\n",
            "Epoch: 3, Step : 1360, LR : 5.7916595551004165e-06, Avg Loss : 0.0915\n",
            "Epoch: 3, Step : 1370, LR : 5.768902543096092e-06, Avg Loss : 0.0924\n",
            "Epoch: 3, Step : 1380, LR : 5.7461455310917676e-06, Avg Loss : 0.1210\n",
            "Epoch: 3, Step : 1390, LR : 5.723388519087444e-06, Avg Loss : 0.1143\n",
            "Epoch: 3, Step : 1400, LR : 5.700631507083121e-06, Avg Loss : 0.0990\n",
            "Epoch: 3, Step : 1410, LR : 5.677874495078797e-06, Avg Loss : 0.0965\n",
            "Epoch: 3, Step : 1420, LR : 5.655117483074472e-06, Avg Loss : 0.0779\n",
            "Epoch: 3, Step : 1430, LR : 5.632360471070149e-06, Avg Loss : 0.1153\n",
            "Epoch: 3, Step : 1440, LR : 5.609603459065826e-06, Avg Loss : 0.0782\n",
            "Epoch: 3, Step : 1450, LR : 5.5868464470615014e-06, Avg Loss : 0.0650\n",
            "Epoch: 3, Step : 1460, LR : 5.564089435057177e-06, Avg Loss : 0.1484\n",
            "Epoch: 3, Step : 1470, LR : 5.541332423052853e-06, Avg Loss : 0.1426\n",
            "Epoch: 3, Step : 1480, LR : 5.51857541104853e-06, Avg Loss : 0.0564\n",
            "Epoch: 3, Step : 1490, LR : 5.495818399044206e-06, Avg Loss : 0.0772\n",
            "Epoch: 3, Step : 1500, LR : 5.473061387039882e-06, Avg Loss : 0.0955\n",
            "Epoch: 3, Step : 1510, LR : 5.450304375035558e-06, Avg Loss : 0.0909\n",
            "Epoch: 3, Step : 1520, LR : 5.4275473630312344e-06, Avg Loss : 0.0792\n",
            "Epoch: 3, Step : 1530, LR : 5.404790351026911e-06, Avg Loss : 0.1274\n",
            "Epoch: 3, Step : 1540, LR : 5.382033339022586e-06, Avg Loss : 0.1097\n",
            "Epoch: 3, Step : 1550, LR : 5.359276327018263e-06, Avg Loss : 0.1473\n",
            "Epoch: 3, Step : 1560, LR : 5.336519315013939e-06, Avg Loss : 0.1105\n",
            "Epoch: 3, Step : 1570, LR : 5.3137623030096155e-06, Avg Loss : 0.1677\n",
            "Epoch: 3, Step : 1580, LR : 5.291005291005291e-06, Avg Loss : 0.1221\n",
            "Epoch: 3, Step : 1590, LR : 5.268248279000968e-06, Avg Loss : 0.1492\n",
            "Epoch: 3, Step : 1600, LR : 5.245491266996644e-06, Avg Loss : 0.1390\n",
            "Epoch: 3, Step : 1610, LR : 5.22273425499232e-06, Avg Loss : 0.1057\n",
            "Epoch: 3, Step : 1620, LR : 5.199977242987996e-06, Avg Loss : 0.0851\n",
            "Epoch: 3, Step : 1630, LR : 5.177220230983673e-06, Avg Loss : 0.1349\n",
            "Epoch: 3, Step : 1640, LR : 5.1544632189793485e-06, Avg Loss : 0.1102\n",
            "Epoch: 3, Step : 1650, LR : 5.131706206975025e-06, Avg Loss : 0.0749\n",
            "Epoch: 3, Step : 1660, LR : 5.1089491949707005e-06, Avg Loss : 0.0693\n",
            "Epoch: 3, Step : 1670, LR : 5.086192182966378e-06, Avg Loss : 0.1123\n",
            "Epoch: 3, Step : 1680, LR : 5.063435170962053e-06, Avg Loss : 0.1312\n",
            "Epoch: 3, Step : 1690, LR : 5.040678158957729e-06, Avg Loss : 0.1683\n",
            "Epoch: 3, Step : 1700, LR : 5.017921146953405e-06, Avg Loss : 0.0916\n",
            "Epoch: 3, Step : 1710, LR : 4.9951641349490815e-06, Avg Loss : 0.1388\n",
            "Epoch: 3, Step : 1720, LR : 4.972407122944758e-06, Avg Loss : 0.1216\n",
            "Epoch: 3, Step : 1730, LR : 4.9496501109404335e-06, Avg Loss : 0.1602\n",
            "Epoch: 3, Step : 1740, LR : 4.92689309893611e-06, Avg Loss : 0.0757\n",
            "Epoch: 3, Step : 1750, LR : 4.904136086931786e-06, Avg Loss : 0.1130\n",
            "Epoch: 3, Step : 1760, LR : 4.881379074927463e-06, Avg Loss : 0.1278\n",
            "Epoch: 3, Step : 1770, LR : 4.858622062923138e-06, Avg Loss : 0.1506\n",
            "Epoch: 3, Step : 1780, LR : 4.8358650509188145e-06, Avg Loss : 0.0910\n",
            "Epoch: 3, Step : 1790, LR : 4.813108038914491e-06, Avg Loss : 0.1386\n",
            "Epoch: 3, Step : 1800, LR : 4.790351026910167e-06, Avg Loss : 0.1211\n",
            "Epoch: 3, Step : 1810, LR : 4.767594014905843e-06, Avg Loss : 0.0990\n",
            "Epoch: 3, Step : 1820, LR : 4.744837002901519e-06, Avg Loss : 0.1058\n",
            "Epoch: 3, Step : 1830, LR : 4.722079990897196e-06, Avg Loss : 0.0861\n",
            "Epoch: 3, Step : 1840, LR : 4.699322978892872e-06, Avg Loss : 0.2066\n",
            "Epoch: 3, Step : 1850, LR : 4.6765659668885476e-06, Avg Loss : 0.1276\n",
            "Epoch: 3, Step : 1860, LR : 4.653808954884224e-06, Avg Loss : 0.1355\n",
            "Epoch: 3, Step : 1870, LR : 4.6310519428799e-06, Avg Loss : 0.1495\n",
            "Epoch: 3, Step : 1880, LR : 4.608294930875577e-06, Avg Loss : 0.1076\n",
            "Epoch: 3, Step : 1890, LR : 4.585537918871252e-06, Avg Loss : 0.1175\n",
            "Epoch: 3, Step : 1900, LR : 4.562780906866929e-06, Avg Loss : 0.1190\n",
            "Epoch: 3, Step : 1910, LR : 4.540023894862605e-06, Avg Loss : 0.1185\n",
            "Epoch: 3, Step : 1920, LR : 4.517266882858281e-06, Avg Loss : 0.1044\n",
            "Epoch: 3, Step : 1930, LR : 4.494509870853958e-06, Avg Loss : 0.0644\n",
            "Epoch: 3, Step : 1940, LR : 4.471752858849633e-06, Avg Loss : 0.1115\n",
            "Epoch: 3, Step : 1950, LR : 4.44899584684531e-06, Avg Loss : 0.0787\n",
            "Epoch 3 total_train_loss : 0.1132\n",
            "***** Finish To Train Epoch 3 *****\n",
            "\n",
            "*****Epoch 3 Valid Start*****\n",
            "Step : 11, valid Loss : 0.2766\n",
            "Step : 21, valid Loss : 0.2745\n",
            "Step : 31, valid Loss : 0.3295\n",
            "Step : 41, valid Loss : 0.3075\n",
            "Step : 51, valid Loss : 0.3140\n",
            "total_valid_loss :  0.3038756241852587 val_f1_score :  92.39835002946376 val_pearsonr : 94.7060802426409\n",
            "Epoch 3 total_Valid Loss : 0.3039\n",
            "*****Epoch 3 Valid Finish*****\n",
            "\n",
            "Saving epoch 3 checkpoint at /content/drive/MyDrive/AI09/model_F.ckpt.3\n",
            "****** Starting To Train Epoch #4 ******\n",
            "Epoch: 4, Step : 10, LR : 4.419411731239688e-06, Avg Loss : 0.0754\n",
            "Epoch: 4, Step : 20, LR : 4.3966547192353646e-06, Avg Loss : 0.1410\n",
            "Epoch: 4, Step : 30, LR : 4.373897707231041e-06, Avg Loss : 0.0895\n",
            "Epoch: 4, Step : 40, LR : 4.351140695226717e-06, Avg Loss : 0.0795\n",
            "Epoch: 4, Step : 50, LR : 4.328383683222393e-06, Avg Loss : 0.0505\n",
            "Epoch: 4, Step : 60, LR : 4.305626671218069e-06, Avg Loss : 0.0664\n",
            "Epoch: 4, Step : 70, LR : 4.282869659213746e-06, Avg Loss : 0.0552\n",
            "Epoch: 4, Step : 80, LR : 4.260112647209422e-06, Avg Loss : 0.1074\n",
            "Epoch: 4, Step : 90, LR : 4.237355635205098e-06, Avg Loss : 0.0906\n",
            "Epoch: 4, Step : 100, LR : 4.214598623200774e-06, Avg Loss : 0.0644\n",
            "Epoch: 4, Step : 110, LR : 4.19184161119645e-06, Avg Loss : 0.0867\n",
            "Epoch: 4, Step : 120, LR : 4.169084599192127e-06, Avg Loss : 0.1119\n",
            "Epoch: 4, Step : 130, LR : 4.146327587187803e-06, Avg Loss : 0.1135\n",
            "Epoch: 4, Step : 140, LR : 4.123570575183479e-06, Avg Loss : 0.0803\n",
            "Epoch: 4, Step : 150, LR : 4.100813563179155e-06, Avg Loss : 0.0650\n",
            "Epoch: 4, Step : 160, LR : 4.0780565511748314e-06, Avg Loss : 0.0598\n",
            "Epoch: 4, Step : 170, LR : 4.055299539170508e-06, Avg Loss : 0.0858\n",
            "Epoch: 4, Step : 180, LR : 4.032542527166183e-06, Avg Loss : 0.0775\n",
            "Epoch: 4, Step : 190, LR : 4.00978551516186e-06, Avg Loss : 0.0657\n",
            "Epoch: 4, Step : 200, LR : 3.987028503157535e-06, Avg Loss : 0.0646\n",
            "Epoch: 4, Step : 210, LR : 3.964271491153212e-06, Avg Loss : 0.0618\n",
            "Epoch: 4, Step : 220, LR : 3.941514479148888e-06, Avg Loss : 0.0556\n",
            "Epoch: 4, Step : 230, LR : 3.9187574671445644e-06, Avg Loss : 0.1124\n",
            "Epoch: 4, Step : 240, LR : 3.89600045514024e-06, Avg Loss : 0.0963\n",
            "Epoch: 4, Step : 250, LR : 3.873243443135916e-06, Avg Loss : 0.0690\n",
            "Epoch: 4, Step : 260, LR : 3.850486431131593e-06, Avg Loss : 0.0650\n",
            "Epoch: 4, Step : 270, LR : 3.827729419127269e-06, Avg Loss : 0.0525\n",
            "Epoch: 4, Step : 280, LR : 3.804972407122945e-06, Avg Loss : 0.1253\n",
            "Epoch: 4, Step : 290, LR : 3.7822153951186215e-06, Avg Loss : 0.0508\n",
            "Epoch: 4, Step : 300, LR : 3.759458383114297e-06, Avg Loss : 0.0867\n",
            "Epoch: 4, Step : 310, LR : 3.736701371109974e-06, Avg Loss : 0.0821\n",
            "Epoch: 4, Step : 320, LR : 3.7139443591056494e-06, Avg Loss : 0.0839\n",
            "Epoch: 4, Step : 330, LR : 3.691187347101326e-06, Avg Loss : 0.0562\n",
            "Epoch: 4, Step : 340, LR : 3.6684303350970017e-06, Avg Loss : 0.1022\n",
            "Epoch: 4, Step : 350, LR : 3.645673323092678e-06, Avg Loss : 0.0656\n",
            "Epoch: 4, Step : 360, LR : 3.622916311088354e-06, Avg Loss : 0.0729\n",
            "Epoch: 4, Step : 370, LR : 3.6001592990840304e-06, Avg Loss : 0.1264\n",
            "Epoch: 4, Step : 380, LR : 3.5774022870797064e-06, Avg Loss : 0.0513\n",
            "Epoch: 4, Step : 390, LR : 3.554645275075383e-06, Avg Loss : 0.0587\n",
            "Epoch: 4, Step : 400, LR : 3.5318882630710588e-06, Avg Loss : 0.0717\n",
            "Epoch: 4, Step : 410, LR : 3.509131251066735e-06, Avg Loss : 0.0688\n",
            "Epoch: 4, Step : 420, LR : 3.486374239062411e-06, Avg Loss : 0.0754\n",
            "Epoch: 4, Step : 430, LR : 3.4636172270580875e-06, Avg Loss : 0.0613\n",
            "Epoch: 4, Step : 440, LR : 3.440860215053764e-06, Avg Loss : 0.1035\n",
            "Epoch: 4, Step : 450, LR : 3.41810320304944e-06, Avg Loss : 0.0696\n",
            "Epoch: 4, Step : 460, LR : 3.3953461910451162e-06, Avg Loss : 0.1133\n",
            "Epoch: 4, Step : 470, LR : 3.372589179040792e-06, Avg Loss : 0.0771\n",
            "Epoch: 4, Step : 480, LR : 3.3498321670364686e-06, Avg Loss : 0.0792\n",
            "Epoch: 4, Step : 490, LR : 3.3270751550321445e-06, Avg Loss : 0.0739\n",
            "Epoch: 4, Step : 500, LR : 3.304318143027821e-06, Avg Loss : 0.1076\n",
            "Epoch: 4, Step : 510, LR : 3.281561131023497e-06, Avg Loss : 0.0738\n",
            "Epoch: 4, Step : 520, LR : 3.2588041190191733e-06, Avg Loss : 0.0703\n",
            "Epoch: 4, Step : 530, LR : 3.2360471070148492e-06, Avg Loss : 0.0968\n",
            "Epoch: 4, Step : 540, LR : 3.2132900950105256e-06, Avg Loss : 0.0657\n",
            "Epoch: 4, Step : 550, LR : 3.1905330830062016e-06, Avg Loss : 0.0587\n",
            "Epoch: 4, Step : 560, LR : 3.167776071001878e-06, Avg Loss : 0.0616\n",
            "Epoch: 4, Step : 570, LR : 3.145019058997554e-06, Avg Loss : 0.0850\n",
            "Epoch: 4, Step : 580, LR : 3.1222620469932303e-06, Avg Loss : 0.0691\n",
            "Epoch: 4, Step : 590, LR : 3.0995050349889063e-06, Avg Loss : 0.0500\n",
            "Epoch: 4, Step : 600, LR : 3.0767480229845827e-06, Avg Loss : 0.0566\n",
            "Epoch: 4, Step : 610, LR : 3.053991010980258e-06, Avg Loss : 0.0624\n",
            "Epoch: 4, Step : 620, LR : 3.031233998975935e-06, Avg Loss : 0.0596\n",
            "Epoch: 4, Step : 630, LR : 3.0084769869716106e-06, Avg Loss : 0.0828\n",
            "Epoch: 4, Step : 640, LR : 2.9857199749672874e-06, Avg Loss : 0.0721\n",
            "Epoch: 4, Step : 650, LR : 2.962962962962963e-06, Avg Loss : 0.0436\n",
            "Epoch: 4, Step : 660, LR : 2.9402059509586393e-06, Avg Loss : 0.1140\n",
            "Epoch: 4, Step : 670, LR : 2.9174489389543153e-06, Avg Loss : 0.0467\n",
            "Epoch: 4, Step : 680, LR : 2.8946919269499916e-06, Avg Loss : 0.0788\n",
            "Epoch: 4, Step : 690, LR : 2.8719349149456676e-06, Avg Loss : 0.0842\n",
            "Epoch: 4, Step : 700, LR : 2.849177902941344e-06, Avg Loss : 0.1081\n",
            "Epoch: 4, Step : 710, LR : 2.82642089093702e-06, Avg Loss : 0.0643\n",
            "Epoch: 4, Step : 720, LR : 2.8036638789326963e-06, Avg Loss : 0.0656\n",
            "Epoch: 4, Step : 730, LR : 2.7809068669283723e-06, Avg Loss : 0.0742\n",
            "Epoch: 4, Step : 740, LR : 2.7581498549240487e-06, Avg Loss : 0.2088\n",
            "Epoch: 4, Step : 750, LR : 2.7353928429197247e-06, Avg Loss : 0.0761\n",
            "Epoch: 4, Step : 760, LR : 2.712635830915401e-06, Avg Loss : 0.0637\n",
            "Epoch: 4, Step : 770, LR : 2.689878818911077e-06, Avg Loss : 0.0824\n",
            "Epoch: 4, Step : 780, LR : 2.6671218069067534e-06, Avg Loss : 0.0820\n",
            "Epoch: 4, Step : 790, LR : 2.6443647949024293e-06, Avg Loss : 0.1297\n",
            "Epoch: 4, Step : 800, LR : 2.6216077828981057e-06, Avg Loss : 0.0657\n",
            "Epoch: 4, Step : 810, LR : 2.598850770893782e-06, Avg Loss : 0.0655\n",
            "Epoch: 4, Step : 820, LR : 2.576093758889458e-06, Avg Loss : 0.0638\n",
            "Epoch: 4, Step : 830, LR : 2.5533367468851345e-06, Avg Loss : 0.0688\n",
            "Epoch: 4, Step : 840, LR : 2.5305797348808104e-06, Avg Loss : 0.0641\n",
            "Epoch: 4, Step : 850, LR : 2.507822722876487e-06, Avg Loss : 0.0742\n",
            "Epoch: 4, Step : 860, LR : 2.4850657108721628e-06, Avg Loss : 0.0717\n",
            "Epoch: 4, Step : 870, LR : 2.4623086988678387e-06, Avg Loss : 0.0841\n",
            "Epoch: 4, Step : 880, LR : 2.439551686863515e-06, Avg Loss : 0.0728\n",
            "Epoch: 4, Step : 890, LR : 2.416794674859191e-06, Avg Loss : 0.0929\n",
            "Epoch: 4, Step : 900, LR : 2.3940376628548675e-06, Avg Loss : 0.0770\n",
            "Epoch: 4, Step : 910, LR : 2.3712806508505434e-06, Avg Loss : 0.0569\n",
            "Epoch: 4, Step : 920, LR : 2.3485236388462194e-06, Avg Loss : 0.0889\n",
            "Epoch: 4, Step : 930, LR : 2.325766626841896e-06, Avg Loss : 0.0844\n",
            "Epoch: 4, Step : 940, LR : 2.3030096148375718e-06, Avg Loss : 0.0862\n",
            "Epoch: 4, Step : 950, LR : 2.280252602833248e-06, Avg Loss : 0.0540\n",
            "Epoch: 4, Step : 960, LR : 2.257495590828924e-06, Avg Loss : 0.1164\n",
            "Epoch: 4, Step : 970, LR : 2.2347385788246005e-06, Avg Loss : 0.0556\n",
            "Epoch: 4, Step : 980, LR : 2.211981566820277e-06, Avg Loss : 0.0864\n",
            "Epoch: 4, Step : 990, LR : 2.189224554815953e-06, Avg Loss : 0.0467\n",
            "Epoch: 4, Step : 1000, LR : 2.1664675428116292e-06, Avg Loss : 0.1062\n",
            "Epoch: 4, Step : 1010, LR : 2.143710530807305e-06, Avg Loss : 0.0733\n",
            "Epoch: 4, Step : 1020, LR : 2.1209535188029816e-06, Avg Loss : 0.0735\n",
            "Epoch: 4, Step : 1030, LR : 2.0981965067986575e-06, Avg Loss : 0.0743\n",
            "Epoch: 4, Step : 1040, LR : 2.075439494794334e-06, Avg Loss : 0.0640\n",
            "Epoch: 4, Step : 1050, LR : 2.05268248279001e-06, Avg Loss : 0.0968\n",
            "Epoch: 4, Step : 1060, LR : 2.0299254707856863e-06, Avg Loss : 0.0726\n",
            "Epoch: 4, Step : 1070, LR : 2.0071684587813622e-06, Avg Loss : 0.0720\n",
            "Epoch: 4, Step : 1080, LR : 1.984411446777038e-06, Avg Loss : 0.0793\n",
            "Epoch: 4, Step : 1090, LR : 1.9616544347727146e-06, Avg Loss : 0.0482\n",
            "Epoch: 4, Step : 1100, LR : 1.9388974227683905e-06, Avg Loss : 0.0549\n",
            "Epoch: 4, Step : 1110, LR : 1.916140410764067e-06, Avg Loss : 0.0728\n",
            "Epoch: 4, Step : 1120, LR : 1.893383398759743e-06, Avg Loss : 0.0642\n",
            "Epoch: 4, Step : 1130, LR : 1.8706263867554193e-06, Avg Loss : 0.0711\n",
            "Epoch: 4, Step : 1140, LR : 1.8478693747510954e-06, Avg Loss : 0.0576\n",
            "Epoch: 4, Step : 1150, LR : 1.8251123627467714e-06, Avg Loss : 0.0606\n",
            "Epoch: 4, Step : 1160, LR : 1.8023553507424476e-06, Avg Loss : 0.0665\n",
            "Epoch: 4, Step : 1170, LR : 1.7795983387381238e-06, Avg Loss : 0.1205\n",
            "Epoch: 4, Step : 1180, LR : 1.7568413267338e-06, Avg Loss : 0.0735\n",
            "Epoch: 4, Step : 1190, LR : 1.7340843147294761e-06, Avg Loss : 0.0596\n",
            "Epoch: 4, Step : 1200, LR : 1.7113273027251523e-06, Avg Loss : 0.0565\n",
            "Epoch: 4, Step : 1210, LR : 1.6885702907208285e-06, Avg Loss : 0.1100\n",
            "Epoch: 4, Step : 1220, LR : 1.6658132787165046e-06, Avg Loss : 0.0682\n",
            "Epoch: 4, Step : 1230, LR : 1.6430562667121808e-06, Avg Loss : 0.0636\n",
            "Epoch: 4, Step : 1240, LR : 1.620299254707857e-06, Avg Loss : 0.0748\n",
            "Epoch: 4, Step : 1250, LR : 1.5975422427035332e-06, Avg Loss : 0.0787\n",
            "Epoch: 4, Step : 1260, LR : 1.5747852306992093e-06, Avg Loss : 0.0736\n",
            "Epoch: 4, Step : 1270, LR : 1.5520282186948855e-06, Avg Loss : 0.0603\n",
            "Epoch: 4, Step : 1280, LR : 1.5292712066905615e-06, Avg Loss : 0.0803\n",
            "Epoch: 4, Step : 1290, LR : 1.5065141946862376e-06, Avg Loss : 0.0729\n",
            "Epoch: 4, Step : 1300, LR : 1.4837571826819138e-06, Avg Loss : 0.0688\n",
            "Epoch: 4, Step : 1310, LR : 1.46100017067759e-06, Avg Loss : 0.0638\n",
            "Epoch: 4, Step : 1320, LR : 1.4382431586732662e-06, Avg Loss : 0.0529\n",
            "Epoch: 4, Step : 1330, LR : 1.4154861466689423e-06, Avg Loss : 0.0593\n",
            "Epoch: 4, Step : 1340, LR : 1.3927291346646187e-06, Avg Loss : 0.0468\n",
            "Epoch: 4, Step : 1350, LR : 1.369972122660295e-06, Avg Loss : 0.0704\n",
            "Epoch: 4, Step : 1360, LR : 1.347215110655971e-06, Avg Loss : 0.0701\n",
            "Epoch: 4, Step : 1370, LR : 1.3244580986516472e-06, Avg Loss : 0.0680\n",
            "Epoch: 4, Step : 1380, LR : 1.3017010866473234e-06, Avg Loss : 0.0918\n",
            "Epoch: 4, Step : 1390, LR : 1.2789440746429996e-06, Avg Loss : 0.0747\n",
            "Epoch: 4, Step : 1400, LR : 1.2561870626386758e-06, Avg Loss : 0.0820\n",
            "Epoch: 4, Step : 1410, LR : 1.2334300506343517e-06, Avg Loss : 0.0584\n",
            "Epoch: 4, Step : 1420, LR : 1.210673038630028e-06, Avg Loss : 0.0844\n",
            "Epoch: 4, Step : 1430, LR : 1.1879160266257043e-06, Avg Loss : 0.0662\n",
            "Epoch: 4, Step : 1440, LR : 1.1651590146213805e-06, Avg Loss : 0.0828\n",
            "Epoch: 4, Step : 1450, LR : 1.1424020026170564e-06, Avg Loss : 0.0545\n",
            "Epoch: 4, Step : 1460, LR : 1.1196449906127326e-06, Avg Loss : 0.0747\n",
            "Epoch: 4, Step : 1470, LR : 1.0968879786084088e-06, Avg Loss : 0.0508\n",
            "Epoch: 4, Step : 1480, LR : 1.074130966604085e-06, Avg Loss : 0.0853\n",
            "Epoch: 4, Step : 1490, LR : 1.0513739545997611e-06, Avg Loss : 0.0578\n",
            "Epoch: 4, Step : 1500, LR : 1.0286169425954373e-06, Avg Loss : 0.0826\n",
            "Epoch: 4, Step : 1510, LR : 1.0058599305911135e-06, Avg Loss : 0.0692\n",
            "Epoch: 4, Step : 1520, LR : 9.831029185867897e-07, Avg Loss : 0.1070\n",
            "Epoch: 4, Step : 1530, LR : 9.603459065824658e-07, Avg Loss : 0.1040\n",
            "Epoch: 4, Step : 1540, LR : 9.37588894578142e-07, Avg Loss : 0.0546\n",
            "Epoch: 4, Step : 1550, LR : 9.148318825738181e-07, Avg Loss : 0.0418\n",
            "Epoch: 4, Step : 1560, LR : 8.920748705694942e-07, Avg Loss : 0.0843\n",
            "Epoch: 4, Step : 1570, LR : 8.693178585651704e-07, Avg Loss : 0.0928\n",
            "Epoch: 4, Step : 1580, LR : 8.465608465608466e-07, Avg Loss : 0.0747\n",
            "Epoch: 4, Step : 1590, LR : 8.238038345565228e-07, Avg Loss : 0.1154\n",
            "Epoch: 4, Step : 1600, LR : 8.010468225521989e-07, Avg Loss : 0.1517\n",
            "Epoch: 4, Step : 1610, LR : 7.782898105478752e-07, Avg Loss : 0.0883\n",
            "Epoch: 4, Step : 1620, LR : 7.555327985435514e-07, Avg Loss : 0.0513\n",
            "Epoch: 4, Step : 1630, LR : 7.327757865392275e-07, Avg Loss : 0.0537\n",
            "Epoch: 4, Step : 1640, LR : 7.100187745349036e-07, Avg Loss : 0.0437\n",
            "Epoch: 4, Step : 1650, LR : 6.872617625305798e-07, Avg Loss : 0.0750\n",
            "Epoch: 4, Step : 1660, LR : 6.64504750526256e-07, Avg Loss : 0.0950\n",
            "Epoch: 4, Step : 1670, LR : 6.417477385219322e-07, Avg Loss : 0.0671\n",
            "Epoch: 4, Step : 1680, LR : 6.189907265176083e-07, Avg Loss : 0.0732\n",
            "Epoch: 4, Step : 1690, LR : 5.962337145132845e-07, Avg Loss : 0.0665\n",
            "Epoch: 4, Step : 1700, LR : 5.734767025089606e-07, Avg Loss : 0.0716\n",
            "Epoch: 4, Step : 1710, LR : 5.507196905046368e-07, Avg Loss : 0.0677\n",
            "Epoch: 4, Step : 1720, LR : 5.279626785003129e-07, Avg Loss : 0.0880\n",
            "Epoch: 4, Step : 1730, LR : 5.052056664959891e-07, Avg Loss : 0.0854\n",
            "Epoch: 4, Step : 1740, LR : 4.824486544916653e-07, Avg Loss : 0.0667\n",
            "Epoch: 4, Step : 1750, LR : 4.596916424873415e-07, Avg Loss : 0.0470\n",
            "Epoch: 4, Step : 1760, LR : 4.369346304830176e-07, Avg Loss : 0.0661\n",
            "Epoch: 4, Step : 1770, LR : 4.141776184786938e-07, Avg Loss : 0.0484\n",
            "Epoch: 4, Step : 1780, LR : 3.9142060647436997e-07, Avg Loss : 0.0555\n",
            "Epoch: 4, Step : 1790, LR : 3.686635944700461e-07, Avg Loss : 0.0702\n",
            "Epoch: 4, Step : 1800, LR : 3.4590658246572227e-07, Avg Loss : 0.0537\n",
            "Epoch: 4, Step : 1810, LR : 3.2314957046139844e-07, Avg Loss : 0.0688\n",
            "Epoch: 4, Step : 1820, LR : 3.003925584570746e-07, Avg Loss : 0.1302\n",
            "Epoch: 4, Step : 1830, LR : 2.776355464527508e-07, Avg Loss : 0.1078\n",
            "Epoch: 4, Step : 1840, LR : 2.5487853444842696e-07, Avg Loss : 0.0728\n",
            "Epoch: 4, Step : 1850, LR : 2.3212152244410309e-07, Avg Loss : 0.0555\n",
            "Epoch: 4, Step : 1860, LR : 2.0936451043977929e-07, Avg Loss : 0.0744\n",
            "Epoch: 4, Step : 1870, LR : 1.8660749843545546e-07, Avg Loss : 0.0730\n",
            "Epoch: 4, Step : 1880, LR : 1.638504864311316e-07, Avg Loss : 0.0668\n",
            "Epoch: 4, Step : 1890, LR : 1.4109347442680776e-07, Avg Loss : 0.0602\n",
            "Epoch: 4, Step : 1900, LR : 1.1833646242248394e-07, Avg Loss : 0.0505\n",
            "Epoch: 4, Step : 1910, LR : 9.557945041816009e-08, Avg Loss : 0.0727\n",
            "Epoch: 4, Step : 1920, LR : 7.282243841383628e-08, Avg Loss : 0.0499\n",
            "Epoch: 4, Step : 1930, LR : 5.0065426409512434e-08, Avg Loss : 0.0835\n",
            "Epoch: 4, Step : 1940, LR : 2.73084144051886e-08, Avg Loss : 0.0776\n",
            "Epoch: 4, Step : 1950, LR : 4.5514024008647675e-09, Avg Loss : 0.0595\n",
            "Epoch 4 total_train_loss : 0.0757\n",
            "***** Finish To Train Epoch 4 *****\n",
            "\n",
            "*****Epoch 4 Valid Start*****\n",
            "Step : 11, valid Loss : 0.2725\n",
            "Step : 21, valid Loss : 0.2954\n",
            "Step : 31, valid Loss : 0.3391\n",
            "Step : 41, valid Loss : 0.3275\n",
            "Step : 51, valid Loss : 0.3223\n",
            "total_valid_loss :  0.31413054357875475 val_f1_score :  92.3525977816696 val_pearsonr : 94.87123510074798\n",
            "Epoch 4 total_Valid Loss : 0.3141\n",
            "*****Epoch 4 Valid Finish*****\n",
            "\n",
            "Saving epoch 4 checkpoint at /content/drive/MyDrive/AI09/model_F.ckpt.4\n",
            "Train Finished\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f35677001b224e1a9846b598435ca091"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>total_f1_score </td><td>▁▅▆██</td></tr><tr><td>total_pearsonr</td><td>▁▅▇██</td></tr><tr><td>total_train_loss</td><td>█▃▂▁▁</td></tr><tr><td>total_train_lr</td><td>█▆▅▃▁</td></tr><tr><td>total_valid_loss</td><td>█▅▃▁▁</td></tr><tr><td>train_loss</td><td>█▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_lr</td><td>▂▃▅▇███▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>valid_loss</td><td>▅▄█▆▅▃▃▅▃▄▂▂▄▃▃▁▁▂▂▂▁▂▃▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>5</td></tr><tr><td>total_f1_score </td><td>92.3526</td></tr><tr><td>total_pearsonr</td><td>94.87124</td></tr><tr><td>total_train_loss</td><td>0.07568</td></tr><tr><td>total_train_lr</td><td>0.0</td></tr><tr><td>total_valid_loss</td><td>0.31413</td></tr><tr><td>train_loss</td><td>0.05954</td></tr><tr><td>train_lr</td><td>0.0</td></tr><tr><td>valid_loss</td><td>0.32229</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">peachy-sweep-1</strong>: <a href=\"https://wandb.ai/kdb/AI09_f/runs/cib6ykvf\" target=\"_blank\">https://wandb.ai/kdb/AI09_f/runs/cib6ykvf</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20220601_040135-cib6ykvf/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: hxnsbx2v with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \teps: 1e-08\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tgrad_norm: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 2e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_length: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_batch_size: 8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tvalid_batch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twarm_up_ratio: 0.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.17"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220601_050651-hxnsbx2v</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/kdb/AI09_f/runs/hxnsbx2v\" target=\"_blank\">misunderstood-sweep-2</a></strong> to <a href=\"https://wandb.ai/kdb/AI09_f\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/kdb/AI09_f/sweeps/qm6ty8sp\" target=\"_blank\">https://wandb.ai/kdb/AI09_f/sweeps/qm6ty8sp</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at klue/roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "****** Starting To Train Epoch #0 ******\n",
            "Epoch: 0, Step : 10, LR : 2.2529441884280595e-07, Avg Loss : 8.3281\n",
            "Epoch: 0, Step : 20, LR : 4.301075268817205e-07, Avg Loss : 7.8835\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Step : 30, LR : 6.34920634920635e-07, Avg Loss : 7.9659\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, dataloader):    \n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    \n",
        "    total_loss = 0\n",
        "    batch_count = 0\n",
        "    batch_loss = 0\n",
        "    \n",
        "    pred_np = None\n",
        "\n",
        "    for step, batch in enumerate(dataloader):       \n",
        "        batch_count += 1\n",
        "        batch = tuple(item.to(device) for item in batch)\n",
        "\n",
        "        batch_input, batch_label = batch\n",
        "\n",
        "        with torch.no_grad():\n",
        "             outputs = model(**batch_input, labels = batch_label)\n",
        "    \n",
        "        loss = outputs.loss\n",
        "        pred = outputs.logits.squeeze()\n",
        "\n",
        "        if pred_np is None:\n",
        "            pred_np = pred.detach().cpu().numpy()\n",
        "            label_np = batch_label.detach().cpu().numpy()\n",
        "        else:\n",
        "            pred_np = np.append(pred_np, pred.detach().cpu().numpy(), axis=0)\n",
        "            label_np = np.append(label_np, batch_label.detach().cpu().numpy(), axis=0)\n",
        "        \n",
        "        batch_loss += loss.item()\n",
        "        total_loss += loss.item()\n",
        "                        \n",
        "        if (step % 10) == 0 and step != 0:\n",
        "            print('test_loss : ' ,batch_loss / batch_count)                           \n",
        "            batch_loss, batch_count = 0, 0\n",
        "\n",
        "    total_valid_loss = total_loss / (step + 1)\n",
        "\n",
        "    fone_pred = np.where(pred_np >=3, 1, 0)\n",
        "    fone_label = np.where(label_np >=3, 1, 0)\n",
        "       \n",
        "    fone= f1_score(fone_pred , fone_label) * 100\n",
        "    p_score = pearsonr(pred_np, label_np)[0] * 100           \n",
        "    print('total_test_loss : ' , total_valid_loss, \"total_f1_score : \" , fone, \"total_pearsonr:\" , p_score)"
      ],
      "metadata": {
        "id": "weow2O4Vjw59"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ckpt1 = '/content/drive/MyDrive/AI09/model_F.ckpt.0'\n",
        "ckpt2 = '/content/drive/MyDrive/AI09/model_F.ckpt.1'\n",
        "ckpt3 = '/content/drive/MyDrive/AI09/model_F.ckpt.2'\n",
        "ckpt4 = '/content/drive/MyDrive/AI09/model_F.ckpt.3'\n",
        "ckpt5 = '/content/drive/MyDrive/AI09/model_F.ckpt.4'"
      ],
      "metadata": {
        "id": "f97vDx90jQ7Z"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_checkpoints = [ckpt1, ckpt2, ckpt3, ckpt4, ckpt5]\n",
        "\n",
        "for checkpoint in all_checkpoints:\n",
        "    loaded_ckpt = torch.load(checkpoint)\n",
        "    loaded_ckpt['epoch'], loaded_ckpt['loss']\n",
        "    model.load_state_dict(loaded_ckpt[\"model_state_dict\"])\n",
        "    test(model, test_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSuvt24hjdYr",
        "outputId": "5c132a79-27c6-4c81-ed11-3b90aba34523"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_loss :  0.4823445234100174\n",
            "total_test_loss :  0.4942567559131 total_f1_score :  81.34831460674155 total_pearsonr: 89.38817275926439\n",
            "test_loss :  0.46479722392225253\n",
            "total_test_loss :  0.49530772425431957 total_f1_score :  84.2315369261477 total_pearsonr: 90.40347654920467\n",
            "test_loss :  0.3967039429870596\n",
            "total_test_loss :  0.42338890310288474 total_f1_score :  86.53061224489797 total_pearsonr: 92.00251292662712\n",
            "test_loss :  0.32531930094422834\n",
            "total_test_loss :  0.33462238605657596 total_f1_score :  85.53459119496856 total_pearsonr: 92.24971720129449\n",
            "test_loss :  0.35813261458811746\n",
            "total_test_loss :  0.37279451583115897 total_f1_score :  85.06224066390043 total_pearsonr: 92.32472216235828\n"
          ]
        }
      ]
    }
  ]
}