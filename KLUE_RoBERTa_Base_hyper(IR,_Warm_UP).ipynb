{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KLUE-RoBERTa-Base(IR, Warm_UP)",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "54f1944f3aed4d75a2c089c1c9f877f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a579ea4d869044adb7cd0e809c206938",
              "IPY_MODEL_323860e327684c23b8beab7b4d27e438"
            ],
            "layout": "IPY_MODEL_afba433550c14db7b1a2af359ce181f6"
          }
        },
        "a579ea4d869044adb7cd0e809c206938": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2758574720f34ccd9335d949bd9e532b",
            "placeholder": "​",
            "style": "IPY_MODEL_ff344e262f6d4d2b9332d8101a9b83da",
            "value": "2.980 MB of 2.980 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "323860e327684c23b8beab7b4d27e438": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2359337145ff4905a40e91a93decdbf5",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5555aa4c56e2481c9c635f38929b705c",
            "value": 1
          }
        },
        "afba433550c14db7b1a2af359ce181f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2758574720f34ccd9335d949bd9e532b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff344e262f6d4d2b9332d8101a9b83da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2359337145ff4905a40e91a93decdbf5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5555aa4c56e2481c9c635f38929b705c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4128eb1524724093a7add0d9c0dd975f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_787403590e4c40ac93736464e25d7452",
              "IPY_MODEL_11db8f1dd0e4479fbad04f25999ca774"
            ],
            "layout": "IPY_MODEL_2dab60499a6c480b90080fc2fbc9da10"
          }
        },
        "787403590e4c40ac93736464e25d7452": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_324c6e3625014d838eac05e3f5f21afe",
            "placeholder": "​",
            "style": "IPY_MODEL_0aa71a4963fe4d92ad48e046afef8349",
            "value": "2.980 MB of 2.980 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "11db8f1dd0e4479fbad04f25999ca774": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d3c407dbdf5a431da837ef5244d7cb6f",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1837c4ffff994ee095251beaf4d99ea3",
            "value": 1
          }
        },
        "2dab60499a6c480b90080fc2fbc9da10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "324c6e3625014d838eac05e3f5f21afe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0aa71a4963fe4d92ad48e046afef8349": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d3c407dbdf5a431da837ef5244d7cb6f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1837c4ffff994ee095251beaf4d99ea3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fcf6c4ebd1114ee9916cb552eeb0f304": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_add65d3950174a4494ef485af765b88f",
              "IPY_MODEL_5b9e85a793ee479682abe295b2856994"
            ],
            "layout": "IPY_MODEL_dbbc609dcdc041eab6bf407daa68edf1"
          }
        },
        "add65d3950174a4494ef485af765b88f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_98c9e943d8714afa8a749cc7d8bdad12",
            "placeholder": "​",
            "style": "IPY_MODEL_9f87702cfa214ec9abcefd3d3c349a25",
            "value": "2.977 MB of 2.977 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "5b9e85a793ee479682abe295b2856994": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0bd17ee6cc814091be818a71f7994029",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eab8ce553e9e4e708edddaa9164c29dc",
            "value": 1
          }
        },
        "dbbc609dcdc041eab6bf407daa68edf1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98c9e943d8714afa8a749cc7d8bdad12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f87702cfa214ec9abcefd3d3c349a25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0bd17ee6cc814091be818a71f7994029": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eab8ce553e9e4e708edddaa9164c29dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9dc39149f29e4e549a3115d7f3590cae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b28b42cbe7694f5e98f399fee480ebe9",
              "IPY_MODEL_5742004d2abb4fc193f50b41590ae132"
            ],
            "layout": "IPY_MODEL_9b811af0b0474774811fe89367996f66"
          }
        },
        "b28b42cbe7694f5e98f399fee480ebe9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eafb673318cd403c80146af8ca6eefad",
            "placeholder": "​",
            "style": "IPY_MODEL_40fae4b24ba7438798336e34e56ad277",
            "value": "2.980 MB of 2.980 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "5742004d2abb4fc193f50b41590ae132": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1444e5c3b0fd48c7b612cd27584d8dbc",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cb02b96f05a749c5848cb9c68a1d864b",
            "value": 1
          }
        },
        "9b811af0b0474774811fe89367996f66": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eafb673318cd403c80146af8ca6eefad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40fae4b24ba7438798336e34e56ad277": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1444e5c3b0fd48c7b612cd27584d8dbc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb02b96f05a749c5848cb9c68a1d864b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6aab5e212f2f45cbb713fcc319b97c55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_024d1808ab9346b2b8da10a4b7c7aa81",
              "IPY_MODEL_5f0e74dc0914480bb8d9a698c783750f"
            ],
            "layout": "IPY_MODEL_895c35ff80ef472eb642af97c84be4ac"
          }
        },
        "024d1808ab9346b2b8da10a4b7c7aa81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9b1ce3276a44b5cbad53f4e71c5a07d",
            "placeholder": "​",
            "style": "IPY_MODEL_b773350b293e487aa17891ffce9fe535",
            "value": "2.982 MB of 2.982 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "5f0e74dc0914480bb8d9a698c783750f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9fcbe0aaf76843078fceb1b8046da27f",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_45eb0073dc6d480a8ce19895c37c405f",
            "value": 1
          }
        },
        "895c35ff80ef472eb642af97c84be4ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9b1ce3276a44b5cbad53f4e71c5a07d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b773350b293e487aa17891ffce9fe535": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9fcbe0aaf76843078fceb1b8046da27f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45eb0073dc6d480a8ce19895c37c405f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3baf2937c8eb4c85bf18aeb15b4442e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d8f2c79c3d5749faa9c25c5ce1a8f903",
              "IPY_MODEL_2776671f8f6b435098dc823dd5a14490"
            ],
            "layout": "IPY_MODEL_dd32f42a3f074420aa1a828d011e874c"
          }
        },
        "d8f2c79c3d5749faa9c25c5ce1a8f903": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2a8f82cf1bd42ac810adb9a0788fcd9",
            "placeholder": "​",
            "style": "IPY_MODEL_0f8b3dd50ce344608b36a45d643a87ac",
            "value": "0.011 MB of 0.011 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "2776671f8f6b435098dc823dd5a14490": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_618d899b54794b698b6267119a697056",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6dded0365b8f47458d791a1400a0fd97",
            "value": 1
          }
        },
        "dd32f42a3f074420aa1a828d011e874c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2a8f82cf1bd42ac810adb9a0788fcd9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f8b3dd50ce344608b36a45d643a87ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "618d899b54794b698b6267119a697056": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6dded0365b8f47458d791a1400a0fd97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a678beed18534bac8c325cbfe8cd8cac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b5d0384596494f9482ae7e76f64f9b35",
              "IPY_MODEL_8398f6c1800e41f287190b2aaf3f2497"
            ],
            "layout": "IPY_MODEL_9518ad898ac842519d9b6f38d9104208"
          }
        },
        "b5d0384596494f9482ae7e76f64f9b35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d2894ad3a1f4a23b73a42c37a10a66d",
            "placeholder": "​",
            "style": "IPY_MODEL_2501e9d7023144c0b1c50da5b1ceefb6",
            "value": "0.011 MB of 0.011 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "8398f6c1800e41f287190b2aaf3f2497": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_816ea30dca6841cdb2404786deae348a",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_529e0ca10a234727aabd3b5b2fdf403f",
            "value": 1
          }
        },
        "9518ad898ac842519d9b6f38d9104208": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d2894ad3a1f4a23b73a42c37a10a66d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2501e9d7023144c0b1c50da5b1ceefb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "816ea30dca6841cdb2404786deae348a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "529e0ca10a234727aabd3b5b2fdf403f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "98f6367f4e9140fd84235e300f26b465": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_64aff2f1cc48486ca38af292b479598f",
              "IPY_MODEL_9170cbdf2902471d9454b69ee12e80ae"
            ],
            "layout": "IPY_MODEL_f719057c9d1f4b2d9286cb3585434b11"
          }
        },
        "64aff2f1cc48486ca38af292b479598f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff87f0733405423693fe14feb989b3b5",
            "placeholder": "​",
            "style": "IPY_MODEL_70fb4e7cf24947e9a1d3868e8d8b230c",
            "value": "0.011 MB of 0.011 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "9170cbdf2902471d9454b69ee12e80ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d28c956d5f9b4fbcb4738ca53ae83142",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8790902061ee4339a2a6e77d2bce063d",
            "value": 1
          }
        },
        "f719057c9d1f4b2d9286cb3585434b11": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff87f0733405423693fe14feb989b3b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70fb4e7cf24947e9a1d3868e8d8b230c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d28c956d5f9b4fbcb4738ca53ae83142": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8790902061ee4339a2a6e77d2bce063d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8d4aea82b7a14f18bb45d615c4538b6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cdfef1eaf3754e2594dacd6e8f437b3d",
              "IPY_MODEL_b53285a5a6a54c14857a4a71c8becb3a"
            ],
            "layout": "IPY_MODEL_60b7eaf4a16c41fc94bcabded61ab8c7"
          }
        },
        "cdfef1eaf3754e2594dacd6e8f437b3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1adb9b7052684924a2973aaefe938be8",
            "placeholder": "​",
            "style": "IPY_MODEL_94ccc1e1ad4b497887de3cdfd0169f4e",
            "value": "0.011 MB of 0.011 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "b53285a5a6a54c14857a4a71c8becb3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c5a5c49409e4741a70f0a229f3a6369",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a60a0f17836a4523927d95831d14f769",
            "value": 1
          }
        },
        "60b7eaf4a16c41fc94bcabded61ab8c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1adb9b7052684924a2973aaefe938be8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94ccc1e1ad4b497887de3cdfd0169f4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0c5a5c49409e4741a70f0a229f3a6369": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a60a0f17836a4523927d95831d14f769": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers transformers wandb"
      ],
      "metadata": {
        "id": "NeOJ6af_SRnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "tv05kftVt7i3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np \n",
        "import torch\n",
        "import random\n",
        "import tarfile\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler, random_split\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import AdamW\n",
        "from torch.nn import CosineSimilarity, MSELoss\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from sklearn.metrics import f1_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEf06O7lt-tR",
        "outputId": "ffe3b704-74e9-4709-e2bb-f764a07da820"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"# available GPUs : {torch.cuda.device_count()}\")\n",
        "    print(f\"GPU name : {torch.cuda.get_device_name()}\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FA7zujvXYJwv",
        "outputId": "56e612c5-6a58-4248-edd9-1dd7d6be025a"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# available GPUs : 1\n",
            "GPU name : Tesla P100-PCIE-16GB\n",
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/NLP"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJ8F_XO2uFL4",
        "outputId": "2a15213d-2686-42d1-db0e-d6a4cc436a3d"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/NLP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/NLP/df.csv')\n",
        "test = pd.read_csv('/content/drive/MyDrive/NLP/test.csv')"
      ],
      "metadata": {
        "id": "zsKDESZjuIAR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[['sentence1', 'sentence2', 'real-label', 'binary-label']]\n",
        "test = test[['sentence1', 'sentence2', 'real-label', 'binary-label']]"
      ],
      "metadata": {
        "id": "cAIIIQ5g1pSm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train, val = train_test_split(df, test_size=0.1, shuffle=True)"
      ],
      "metadata": {
        "id": "wyHdi4vk2B97"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_lFLu-y5p8l",
        "outputId": "5b5fb3b8-4540-472c-d120-f03134b2792f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.19.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.7/dist-packages (2.2.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.0.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.19.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.7.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.21.6)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.11.0+cu113)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.12.0+cu113)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.64.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.1.96)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (4.2.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.12.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.7.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (4.11.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.8.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (1.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel, RobertaModel, RobertaTokenizer\n",
        "from transformers import ElectraModel, ElectraTokenizer\n",
        "from transformers import get_linear_schedule_with_warmup, get_constant_schedule"
      ],
      "metadata": {
        "id": "anv44oQg5uiH"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# seed\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)"
      ],
      "metadata": {
        "id": "e8ivO6sd53oV"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, sentence1, sentence2, real_label):\n",
        "        self.X1 = sentence1 #list str\n",
        "        self.X2 = sentence2 #list str\n",
        "        self.Y = real_label #list float\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X1)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        return self.X1[index], self.X2[index], self.Y[index]"
      ],
      "metadata": {
        "id": "y3z37Lxv54RS"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_dataset(df, flag):\n",
        "    sen_one = df['sentence1'].tolist()\n",
        "    sen_two = df['sentence2'].tolist()\n",
        "    lab = df['binary-label'].tolist()\n",
        "    real_lab = df['real-label'].tolist()\n",
        "    \n",
        "    if flag:\n",
        "        return CustomDataset(sen_one, sen_two, real_lab)\n",
        "    else:\n",
        "        return CustomDataset(sen_one, sen_two, lab)"
      ],
      "metadata": {
        "id": "L9-46u5556TM"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = generate_dataset(train, True)\n",
        "val_dataset = generate_dataset(val, True)\n",
        "test_dataset = generate_dataset(test, True)"
      ],
      "metadata": {
        "id": "nuSuFCPJ574W"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-base\")"
      ],
      "metadata": {
        "id": "Xgpybyaz59tf"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def CustomCollateFn(batch):\n",
        "    sen_one_list = []\n",
        "    sen_two_list = []\n",
        "    label_list = []\n",
        "\n",
        "\n",
        "    for sen_one, sen_two, label in batch:\n",
        "        sen_one_list.append(sen_one)\n",
        "        sen_two_list.append(sen_two)\n",
        "        label_list.append(label/5.0)\n",
        "    \n",
        "    tokenized_sen_one = tokenizer(sen_one_list, add_special_tokens=True, padding='max_length',\n",
        "                                truncation=True, max_length=128, return_tensors='pt')\n",
        "    tokenized_sen_two = tokenizer(sen_two_list, add_special_tokens=True, padding='max_length',\n",
        "                                truncation=True, max_length=128, return_tensors='pt')\n",
        "\n",
        "    label_list = torch.Tensor(label_list)\n",
        "\n",
        "\n",
        "    return (tokenized_sen_one, tokenized_sen_two, label_list)\n",
        "\n",
        "def CustomCollateFn_dev(batch):\n",
        "    sen_one_list = []\n",
        "    sen_two_list = []\n",
        "    label_list = []\n",
        "\n",
        "\n",
        "    for sen_one, sen_two, label in batch:\n",
        "        sen_one_list.append(sen_one)\n",
        "        sen_two_list.append(sen_two)\n",
        "        label_list.append(label)\n",
        "\n",
        "    tokenized_sen_one = tokenizer(sen_one_list, add_special_tokens=True, padding='max_length',\n",
        "                                truncation=True, max_length=128, return_tensors='pt')\n",
        "    tokenized_sen_two = tokenizer(sen_two_list, add_special_tokens=True, padding='max_length',\n",
        "                                truncation=True, max_length=128, return_tensors='pt')\n",
        "\n",
        "    label_list = torch.Tensor(label_list)\n",
        "\n",
        "    return (tokenized_sen_one, tokenized_sen_two, label_list)    "
      ],
      "metadata": {
        "id": "VdFZ6ia95_q2"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_pooling_fn(output, attention_mask):\n",
        "    embedding = output.last_hidden_state # (batch len, longest sentence length, 1024)\n",
        "    att_msk = attention_mask # (batch_len, 1024)\n",
        "    mask = att_msk.unsqueeze(-1).expand(output.last_hidden_state.size()).float() # (batch len, longest sentence length, 1024)\n",
        "    masked_embedding = output.last_hidden_state * mask # (batch_len, longest sen len, 1024)\n",
        "    me_sum = torch.sum(masked_embedding, 1) # (batch_len, 1024)\n",
        "    ms_sum = torch.clamp(mask.sum(1), min=1e-9) # (batch_len, 1024)\n",
        "    mean_pool = me_sum/ms_sum # batch_len, 1024\n",
        "    return mean_pool"
      ],
      "metadata": {
        "id": "Nihox4Xf6E3q"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_pooling_fn(model_output, attention_mask):\n",
        "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)"
      ],
      "metadata": {
        "id": "aHHd1zKFY29G"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomPooling(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomPooling, self).__init__()\n",
        "\n",
        "        self.robert = AutoModel.from_pretrained(\"klue/roberta-base\")\n",
        "\n",
        "\n",
        "        self.cos_score = nn.Sequential(\n",
        "            nn.Identity()\n",
        "        )\n",
        "    \n",
        "    def forward(self, senone, sentwo):\n",
        "        output_one = self.robert(input_ids=senone['input_ids'], attention_mask=senone['attention_mask'],\n",
        "                             token_type_ids=senone['token_type_ids'])\n",
        "        output_two = self.robert(input_ids=sentwo['input_ids'], attention_mask=sentwo['attention_mask'],\n",
        "                             token_type_ids=sentwo['token_type_ids'])\n",
        "\n",
        "        pooled_one = mean_pooling_fn(output_one, senone['attention_mask'])\n",
        "        pooled_two = mean_pooling_fn(output_two, sentwo['attention_mask'])\n",
        "\n",
        "\n",
        "        cos_sim = torch.cosine_similarity(pooled_one, pooled_two)\n",
        "        logit = self.cos_score(cos_sim)\n",
        "\n",
        "        return logit"
      ],
      "metadata": {
        "id": "RfyaStDL6LS7"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initializer(input_dataloader, epochs):\n",
        "\n",
        "    model = CustomPooling()\n",
        "    optimizer = AdamW(model.parameters(), lr=1e-5, eps=1e-8)\n",
        "    print(f'total step: {len(input_dataloader) * epochs}')\n",
        "\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps = round(len(input_dataloader)*0.1),\n",
        "        num_training_steps = len(input_dataloader) * epochs,\n",
        "\n",
        "    )\n",
        "\n",
        "    return model, optimizer, scheduler"
      ],
      "metadata": {
        "id": "TBuhhWK56OLI"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initializer(input_dataloader, epochs):\n",
        "    \"\"\"\n",
        "    설정에 맞춰서 wandb sweep 실행.\n",
        "    \"\"\"\n",
        "    wandb.init(config=sweep_config)\n",
        "    model = CustomPooling()   \n",
        "    w_config = wandb.config   \n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr = 1e-5, eps = 1e-8) \n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps = round(len(input_dataloader)*0.1),\n",
        "        num_training_steps = len(input_dataloader) * epochs,\n",
        "\n",
        "    )\n",
        "    print(f'total step: {len(input_dataloader) * epochs}') \n",
        "    text_table = wandb.Table(columns=[\"epoch\", \"step\", \"text\", 'true_label', 'pred_label'])\n",
        "    wandb.log({f\"error-text-{wandb.run.name}\" : text_table})\n",
        "    return model, optimizer, scheduler    "
      ],
      "metadata": {
        "id": "Rzmzowcn6QjU"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(path, model, optimizer, scheduler, epoch, loss):\n",
        "    file_name = f'/content/drive/MyDrive/data/checkpoints/sts_hyper.ckpt.{epoch}'\n",
        "    torch.save({\n",
        "        'epoch':epoch,\n",
        "        'model_state_dict':model.state_dict(),\n",
        "        'optimizer_state_dict':optimizer.state_dict(),\n",
        "        'scheduler_state_dict':scheduler.state_dict(),\n",
        "        'loss':loss\n",
        "    }, file_name)\n",
        "\n",
        "    print(f'SAVING EPOCH {epoch} ...')"
      ],
      "metadata": {
        "id": "yAvB2AnyIxsU"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, loss_fct, scheduler, optimizer, train_dataloader, valid_dataloader, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        print(f'****** STARTING TO TRAIN EPOCH #{epoch} ******')\n",
        "\n",
        "        wandb.watch(model, log=\"all\", log_freq = 10)\n",
        "        total_loss = 0\n",
        "        batch_loss = 0\n",
        "        batch_count = 0\n",
        "\n",
        "        model.train()\n",
        "        model.to(device)\n",
        "\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            batch_count += 1\n",
        "            batch = tuple(items.to(device) for items in batch)\n",
        "\n",
        "            (x_batch_one, x_batch_two, y_batch) = batch\n",
        "\n",
        "            model.zero_grad()\n",
        "\n",
        "            logit = model(x_batch_one, x_batch_two)\n",
        "            loss = loss_fct(logit, y_batch)\n",
        "\n",
        "            batch_loss += loss.item()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            if(step % 10 == 0 and step != 0):\n",
        "                wandb.log({'train_loss': batch_loss / batch_count, 'train_lr': optimizer.param_groups[0]['lr']})\n",
        "                print(f\"Step : {step + 1}, train Loss : {batch_loss / batch_count:.4f}\")                      \n",
        "                # reset \n",
        "                batch_loss, batch_count = 0,0\n",
        "\n",
        "        wandb.log({'total_train_loss': total_loss / (step + 1), 'total_train_lr': optimizer.param_groups[0]['lr'], \"epoch\" : (epoch + 1)})\n",
        "  \n",
        "\n",
        "        print(f\"Epoch {epoch} Total Mean Loss : {total_loss/(step+1):.4f}\")\n",
        "        print(f\"*****Epoch {epoch} Train Finish*****\\n\")\n",
        "        save_checkpoint(\".\", model, optimizer, scheduler, epoch, total_loss/(step+1))\n",
        "           \n",
        "        if valid_dataloader is not None:\n",
        "            print(f\"*****Epoch {epoch} Valid Start*****\")\n",
        "            valid_loss, valid_pearson, valid_f1 = validate(model, loss_fct, valid_dataloader)\n",
        "            print(f\"Epoch {epoch} Valid Loss : {valid_loss} Valid Pearsonr : {valid_pearson} ValidF1 : {valid_f1}\")\n",
        "            print(f\"*****Epoch {epoch} Valid Finish*****\\n\")\n",
        "  \n",
        "\n",
        "    print('** Train Completed! **')"
      ],
      "metadata": {
        "id": "XxYteWmE6V4P"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install audtorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5TBQtpU6YKU",
        "outputId": "218e4e66-d526-41f8-810e-47ffda8b2acc"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: audtorch in /usr/local/lib/python3.7/dist-packages (0.6.4)\n",
            "Requirement already satisfied: librosa>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from audtorch) (0.8.1)\n",
            "Requirement already satisfied: audiofile in /usr/local/lib/python3.7/dist-packages (from audtorch) (1.1.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from audtorch) (1.3.5)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from audtorch) (0.8.9)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from audtorch) (1.11.0+cu113)\n",
            "Requirement already satisfied: resampy in /usr/local/lib/python3.7/dist-packages (from audtorch) (0.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from audtorch) (4.64.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from audtorch) (1.21.6)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.8.0->audtorch) (4.4.2)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.8.0->audtorch) (1.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.8.0->audtorch) (21.3)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.8.0->audtorch) (1.6.0)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.8.0->audtorch) (2.1.9)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.8.0->audtorch) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.8.0->audtorch) (1.1.0)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.8.0->audtorch) (0.51.2)\n",
            "Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.8.0->audtorch) (0.10.3.post1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa>=0.8.0->audtorch) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa>=0.8.0->audtorch) (0.34.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->librosa>=0.8.0->audtorch) (3.0.9)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa>=0.8.0->audtorch) (1.4.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa>=0.8.0->audtorch) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa>=0.8.0->audtorch) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa>=0.8.0->audtorch) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa>=0.8.0->audtorch) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa>=0.8.0->audtorch) (2.10)\n",
            "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.7/dist-packages (from resampy->audtorch) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa>=0.8.0->audtorch) (3.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile>=0.10.2->librosa>=0.8.0->audtorch) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa>=0.8.0->audtorch) (2.21)\n",
            "Requirement already satisfied: sox in /usr/local/lib/python3.7/dist-packages (from audiofile->audtorch) (1.4.1)\n",
            "Requirement already satisfied: audeer in /usr/local/lib/python3.7/dist-packages (from audiofile->audtorch) (1.18.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->audtorch) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->audtorch) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->audtorch) (4.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from audtorch.metrics.functional import pearsonr\n",
        "from sklearn.metrics import f1_score\n",
        "from scipy import stats"
      ],
      "metadata": {
        "id": "9BaewSjr6ab9"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model, loss_fct, valid_dataloader):\n",
        "\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    total_loss = 0\n",
        "    total_acc = 0\n",
        "    all_prediction = []\n",
        "    all_reallabel = []\n",
        "\n",
        "    for step, batch in enumerate(valid_dataloader):\n",
        "        batch = tuple(items.to(device) for items in batch)\n",
        "\n",
        "        (x_batch_one, x_batch_two, batch_y) = batch\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logit = model(x_batch_one, x_batch_two)\n",
        "\n",
        "        logit = logit*5\n",
        "        loss = loss_fct(logit, batch_y)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "\n",
        "        logit = logit[:].cpu()\n",
        "        batch_y = batch_y.cpu()\n",
        "\n",
        "        print(f'Step: {step},  Pearson: {pearsonr(logit, batch_y)}')\n",
        "\n",
        "        all_prediction = all_prediction + logit.tolist()\n",
        "        all_reallabel = all_reallabel + batch_y.tolist()\n",
        "\n",
        "    #pearson\n",
        "\n",
        "    pred = torch.Tensor(all_prediction) # x\n",
        "    real = torch.Tensor(all_reallabel) # y\n",
        "    \n",
        "    pearson = pearsonr(pred, real)\n",
        "    \n",
        "    #loss\n",
        "    total_loss = total_loss / (step+1)\n",
        "\n",
        "    #f1\n",
        "    fone = f1_process(pred, real)\n",
        "\n",
        "    wandb.log({'total_valid_loss': total_loss, \"total_f1_score \": fone, \"total_pearsonr\" : pearson})  \n",
        "    print('total_valid_loss : ', total_loss, \"total_f1_score : \",  fone,  \"total_pearsonr :\", pearson)  \n",
        "    return total_loss, pearson, fone"
      ],
      "metadata": {
        "id": "XF3bVjFW6b9j"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def f1_process(pred, real):\n",
        "    bin_real = []\n",
        "    bin_pred = []\n",
        "\n",
        "    for index in range(len(real)):\n",
        "        if real[index] < 3:\n",
        "            bin_real.append(0)\n",
        "        else:\n",
        "            bin_real.append(1)\n",
        "    \n",
        "        if pred[index] < 3:\n",
        "            bin_pred.append(0)\n",
        "        else:\n",
        "            bin_pred.append(1)\n",
        "\n",
        "    return f1_score(bin_real, bin_pred)"
      ],
      "metadata": {
        "id": "z6O9KoTA6gjk"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size = 8,\n",
        "    sampler = RandomSampler(train_dataset),\n",
        "    collate_fn = CustomCollateFn,\n",
        ")\n",
        "valid_dataloader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size = 16,\n",
        "    sampler = SequentialSampler(val_dataset),\n",
        "    collate_fn = CustomCollateFn_dev,\n",
        ")\n",
        "test_dataloader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size = 16,\n",
        "    sampler = SequentialSampler(test_dataset),\n",
        "    collate_fn = CustomCollateFn_dev,\n",
        ") "
      ],
      "metadata": {
        "id": "0bzXglmq6itc"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\n",
        "    \n",
        "    \"name\" : \"sts_v2\",   \n",
        "    \"method\": \"bayes\",\n",
        "    \"metric\": {\n",
        "        \"name\" : \"total_pearsonr\", \n",
        "        \"goal\" : \"maximize\"\n",
        "                },\n",
        "    \n",
        "    \"parameters\": { \n",
        "        \"epochs\" : {\n",
        "            \"distribution\" : \"categorical\",\n",
        "            \"values\" : [4]},\n",
        "        \"learning_rate\" : {\n",
        "            \"distribution\" : \"categorical\",\n",
        "            \"values\" : [1e-5, 3e-5, 5e-5]},                     \n",
        "        \"eps\" : {\n",
        "            \"distribution\" : \"categorical\",\n",
        "            \"values\" : [1e-8]\n",
        "        },\n",
        "        \"train_batch_size\" : {\n",
        "            \"distribution\" : \"categorical\",\n",
        "            \"values\" : [8]\n",
        "        },\n",
        "        \"valid_batch_size\" : {\n",
        "            \"distribution\" : \"categorical\",\n",
        "            \"values\" : [16]\n",
        "        },\n",
        "        \"warm_up_ratio\" : {\n",
        "            \"distribution\" : \"categorical\",\n",
        "            \"values\" : [0, 0.1]  #[0, 0.1, 0.2]\n",
        "        },\n",
        "    },         \n",
        "    \"early_terminate\" : {\n",
        "        \"type\": \"hyperband\", # metric이 2번 이상 개선되지 않을 경우 조기 종료\n",
        "        \"min_iter\" : 2,\n",
        "        \"eta\" : 2\n",
        "        }\n",
        "}"
      ],
      "metadata": {
        "id": "q7O6TvnKOpY7"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_sweeep(config=None):\n",
        "    \"\"\"\n",
        "    설정에 맞춰서 wandb sweep 실행.\n",
        "    \"\"\"\n",
        "    model = CustomPooling() \n",
        "    wandb.init(config=config)\n",
        "    w_config = wandb.config   \n",
        "    \n",
        "    optimizer = AdamW(model.parameters(), lr = w_config.learning_rate, eps =  w_config.eps) \n",
        "    num_training_steps = w_config.epochs * len(train_dataloader)\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer=optimizer, num_warmup_steps= (num_training_steps * w_config.warm_up_ratio),\n",
        "                                                num_training_steps = num_training_steps)\n",
        "    loss_fct = MSELoss()\n",
        "    train(model, loss_fct, scheduler, optimizer, train_dataloader, valid_dataloader, w_config.epochs)"
      ],
      "metadata": {
        "id": "j5GWB_QpOnGd"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb\n",
        "!wandb login\n",
        "import wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTVsT0nhRGb9",
        "outputId": "2f596645-5365-421f-b548-dd25c5bebcfb"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.12.17)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.5.12)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.27)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.9)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.7/dist-packages (from wandb) (1.2.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.2.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2022.5.18.1)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkdb\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_id = wandb.sweep(sweep_config, project = \"sts_v2\")\n",
        "wandb.agent(sweep_id, run_sweeep, count = 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "54f1944f3aed4d75a2c089c1c9f877f2",
            "a579ea4d869044adb7cd0e809c206938",
            "323860e327684c23b8beab7b4d27e438",
            "afba433550c14db7b1a2af359ce181f6",
            "2758574720f34ccd9335d949bd9e532b",
            "ff344e262f6d4d2b9332d8101a9b83da",
            "2359337145ff4905a40e91a93decdbf5",
            "5555aa4c56e2481c9c635f38929b705c",
            "4128eb1524724093a7add0d9c0dd975f",
            "787403590e4c40ac93736464e25d7452",
            "11db8f1dd0e4479fbad04f25999ca774",
            "2dab60499a6c480b90080fc2fbc9da10",
            "324c6e3625014d838eac05e3f5f21afe",
            "0aa71a4963fe4d92ad48e046afef8349",
            "d3c407dbdf5a431da837ef5244d7cb6f",
            "1837c4ffff994ee095251beaf4d99ea3",
            "fcf6c4ebd1114ee9916cb552eeb0f304",
            "add65d3950174a4494ef485af765b88f",
            "5b9e85a793ee479682abe295b2856994",
            "dbbc609dcdc041eab6bf407daa68edf1",
            "98c9e943d8714afa8a749cc7d8bdad12",
            "9f87702cfa214ec9abcefd3d3c349a25",
            "0bd17ee6cc814091be818a71f7994029",
            "eab8ce553e9e4e708edddaa9164c29dc",
            "9dc39149f29e4e549a3115d7f3590cae",
            "b28b42cbe7694f5e98f399fee480ebe9",
            "5742004d2abb4fc193f50b41590ae132",
            "9b811af0b0474774811fe89367996f66",
            "eafb673318cd403c80146af8ca6eefad",
            "40fae4b24ba7438798336e34e56ad277",
            "1444e5c3b0fd48c7b612cd27584d8dbc",
            "cb02b96f05a749c5848cb9c68a1d864b",
            "6aab5e212f2f45cbb713fcc319b97c55",
            "024d1808ab9346b2b8da10a4b7c7aa81",
            "5f0e74dc0914480bb8d9a698c783750f",
            "895c35ff80ef472eb642af97c84be4ac",
            "a9b1ce3276a44b5cbad53f4e71c5a07d",
            "b773350b293e487aa17891ffce9fe535",
            "9fcbe0aaf76843078fceb1b8046da27f",
            "45eb0073dc6d480a8ce19895c37c405f"
          ]
        },
        "id": "5ga9X0L5Q7pH",
        "outputId": "c3bc5e01-9a2e-4fcd-fc6c-1f88a5a97157"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: sy73kc4x\n",
            "Sweep URL: https://wandb.ai/kdb/sts_v2/sweeps/sy73kc4x\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 6xqc4oyo with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \teps: 1e-08\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 3e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_batch_size: 8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tvalid_batch_size: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twarm_up_ratio: 0\n",
            "Some weights of the model checkpoint at klue/roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkdb\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.17"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/NLP/wandb/run-20220529_135152-6xqc4oyo</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/kdb/sts_v2/runs/6xqc4oyo\" target=\"_blank\">dutiful-sweep-1</a></strong> to <a href=\"https://wandb.ai/kdb/sts_v2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/kdb/sts_v2/sweeps/sy73kc4x\" target=\"_blank\">https://wandb.ai/kdb/sts_v2/sweeps/sy73kc4x</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "****** STARTING TO TRAIN EPOCH #0 ******\n",
            "Step : 11, train Loss : 0.0964\n",
            "Step : 21, train Loss : 0.0447\n",
            "Step : 31, train Loss : 0.0431\n",
            "Step : 41, train Loss : 0.0269\n",
            "Step : 51, train Loss : 0.0303\n",
            "Step : 61, train Loss : 0.0309\n",
            "Step : 71, train Loss : 0.0394\n",
            "Step : 81, train Loss : 0.0240\n",
            "Step : 91, train Loss : 0.0437\n",
            "Step : 101, train Loss : 0.0417\n",
            "Step : 111, train Loss : 0.0309\n",
            "Step : 121, train Loss : 0.0292\n",
            "Step : 131, train Loss : 0.0307\n",
            "Step : 141, train Loss : 0.0303\n",
            "Step : 151, train Loss : 0.0283\n",
            "Step : 161, train Loss : 0.0298\n",
            "Step : 171, train Loss : 0.0327\n",
            "Step : 181, train Loss : 0.0208\n",
            "Step : 191, train Loss : 0.0275\n",
            "Step : 201, train Loss : 0.0282\n",
            "Step : 211, train Loss : 0.0235\n",
            "Step : 221, train Loss : 0.0378\n",
            "Step : 231, train Loss : 0.0301\n",
            "Step : 241, train Loss : 0.0352\n",
            "Step : 251, train Loss : 0.0275\n",
            "Step : 261, train Loss : 0.0216\n",
            "Step : 271, train Loss : 0.0271\n",
            "Step : 281, train Loss : 0.0242\n",
            "Step : 291, train Loss : 0.0327\n",
            "Step : 301, train Loss : 0.0338\n",
            "Step : 311, train Loss : 0.0245\n",
            "Step : 321, train Loss : 0.0154\n",
            "Step : 331, train Loss : 0.0223\n",
            "Step : 341, train Loss : 0.0162\n",
            "Step : 351, train Loss : 0.0276\n",
            "Step : 361, train Loss : 0.0253\n",
            "Step : 371, train Loss : 0.0263\n",
            "Step : 381, train Loss : 0.0209\n",
            "Step : 391, train Loss : 0.0313\n",
            "Step : 401, train Loss : 0.0279\n",
            "Step : 411, train Loss : 0.0205\n",
            "Step : 421, train Loss : 0.0305\n",
            "Step : 431, train Loss : 0.0254\n",
            "Step : 441, train Loss : 0.0207\n",
            "Step : 451, train Loss : 0.0199\n",
            "Step : 461, train Loss : 0.0241\n",
            "Step : 471, train Loss : 0.0205\n",
            "Step : 481, train Loss : 0.0261\n",
            "Step : 491, train Loss : 0.0230\n",
            "Step : 501, train Loss : 0.0207\n",
            "Step : 511, train Loss : 0.0280\n",
            "Step : 521, train Loss : 0.0253\n",
            "Step : 531, train Loss : 0.0257\n",
            "Step : 541, train Loss : 0.0236\n",
            "Step : 551, train Loss : 0.0229\n",
            "Step : 561, train Loss : 0.0204\n",
            "Step : 571, train Loss : 0.0256\n",
            "Step : 581, train Loss : 0.0292\n",
            "Step : 591, train Loss : 0.0203\n",
            "Step : 601, train Loss : 0.0179\n",
            "Step : 611, train Loss : 0.0297\n",
            "Step : 621, train Loss : 0.0162\n",
            "Step : 631, train Loss : 0.0248\n",
            "Step : 641, train Loss : 0.0231\n",
            "Step : 651, train Loss : 0.0207\n",
            "Step : 661, train Loss : 0.0323\n",
            "Step : 671, train Loss : 0.0296\n",
            "Step : 681, train Loss : 0.0228\n",
            "Step : 691, train Loss : 0.0257\n",
            "Step : 701, train Loss : 0.0188\n",
            "Step : 711, train Loss : 0.0184\n",
            "Step : 721, train Loss : 0.0265\n",
            "Step : 731, train Loss : 0.0242\n",
            "Step : 741, train Loss : 0.0230\n",
            "Step : 751, train Loss : 0.0275\n",
            "Step : 761, train Loss : 0.0231\n",
            "Step : 771, train Loss : 0.0217\n",
            "Step : 781, train Loss : 0.0301\n",
            "Step : 791, train Loss : 0.0288\n",
            "Step : 801, train Loss : 0.0143\n",
            "Step : 811, train Loss : 0.0217\n",
            "Step : 821, train Loss : 0.0122\n",
            "Step : 831, train Loss : 0.0302\n",
            "Step : 841, train Loss : 0.0169\n",
            "Step : 851, train Loss : 0.0183\n",
            "Step : 861, train Loss : 0.0168\n",
            "Step : 871, train Loss : 0.0192\n",
            "Step : 881, train Loss : 0.0205\n",
            "Step : 891, train Loss : 0.0272\n",
            "Step : 901, train Loss : 0.0231\n",
            "Step : 911, train Loss : 0.0222\n",
            "Step : 921, train Loss : 0.0200\n",
            "Step : 931, train Loss : 0.0270\n",
            "Step : 941, train Loss : 0.0223\n",
            "Step : 951, train Loss : 0.0179\n",
            "Step : 961, train Loss : 0.0230\n",
            "Step : 971, train Loss : 0.0239\n",
            "Step : 981, train Loss : 0.0166\n",
            "Step : 991, train Loss : 0.0184\n",
            "Step : 1001, train Loss : 0.0170\n",
            "Step : 1011, train Loss : 0.0247\n",
            "Step : 1021, train Loss : 0.0158\n",
            "Step : 1031, train Loss : 0.0226\n",
            "Step : 1041, train Loss : 0.0199\n",
            "Step : 1051, train Loss : 0.0282\n",
            "Step : 1061, train Loss : 0.0249\n",
            "Step : 1071, train Loss : 0.0222\n",
            "Step : 1081, train Loss : 0.0190\n",
            "Step : 1091, train Loss : 0.0220\n",
            "Step : 1101, train Loss : 0.0302\n",
            "Step : 1111, train Loss : 0.0250\n",
            "Step : 1121, train Loss : 0.0243\n",
            "Step : 1131, train Loss : 0.0261\n",
            "Step : 1141, train Loss : 0.0358\n",
            "Step : 1151, train Loss : 0.0152\n",
            "Step : 1161, train Loss : 0.0245\n",
            "Step : 1171, train Loss : 0.0328\n",
            "Step : 1181, train Loss : 0.0185\n",
            "Step : 1191, train Loss : 0.0255\n",
            "Step : 1201, train Loss : 0.0214\n",
            "Step : 1211, train Loss : 0.0192\n",
            "Step : 1221, train Loss : 0.0229\n",
            "Step : 1231, train Loss : 0.0204\n",
            "Step : 1241, train Loss : 0.0288\n",
            "Step : 1251, train Loss : 0.0228\n",
            "Step : 1261, train Loss : 0.0279\n",
            "Step : 1271, train Loss : 0.0284\n",
            "Step : 1281, train Loss : 0.0276\n",
            "Step : 1291, train Loss : 0.0204\n",
            "Step : 1301, train Loss : 0.0227\n",
            "Step : 1311, train Loss : 0.0237\n",
            "Step : 1321, train Loss : 0.0182\n",
            "Step : 1331, train Loss : 0.0223\n",
            "Step : 1341, train Loss : 0.0224\n",
            "Step : 1351, train Loss : 0.0245\n",
            "Step : 1361, train Loss : 0.0167\n",
            "Step : 1371, train Loss : 0.0193\n",
            "Step : 1381, train Loss : 0.0178\n",
            "Step : 1391, train Loss : 0.0226\n",
            "Step : 1401, train Loss : 0.0204\n",
            "Step : 1411, train Loss : 0.0188\n",
            "Step : 1421, train Loss : 0.0261\n",
            "Step : 1431, train Loss : 0.0158\n",
            "Step : 1441, train Loss : 0.0206\n",
            "Step : 1451, train Loss : 0.0222\n",
            "Step : 1461, train Loss : 0.0244\n",
            "Step : 1471, train Loss : 0.0222\n",
            "Step : 1481, train Loss : 0.0356\n",
            "Step : 1491, train Loss : 0.0243\n",
            "Step : 1501, train Loss : 0.0241\n",
            "Step : 1511, train Loss : 0.0260\n",
            "Step : 1521, train Loss : 0.0178\n",
            "Step : 1531, train Loss : 0.0208\n",
            "Step : 1541, train Loss : 0.0191\n",
            "Step : 1551, train Loss : 0.0182\n",
            "Step : 1561, train Loss : 0.0169\n",
            "Step : 1571, train Loss : 0.0239\n",
            "Step : 1581, train Loss : 0.0212\n",
            "Step : 1591, train Loss : 0.0284\n",
            "Step : 1601, train Loss : 0.0227\n",
            "Step : 1611, train Loss : 0.0233\n",
            "Step : 1621, train Loss : 0.0254\n",
            "Step : 1631, train Loss : 0.0181\n",
            "Step : 1641, train Loss : 0.0200\n",
            "Step : 1651, train Loss : 0.0176\n",
            "Step : 1661, train Loss : 0.0239\n",
            "Step : 1671, train Loss : 0.0207\n",
            "Step : 1681, train Loss : 0.0195\n",
            "Step : 1691, train Loss : 0.0244\n",
            "Step : 1701, train Loss : 0.0200\n",
            "Step : 1711, train Loss : 0.0178\n",
            "Step : 1721, train Loss : 0.0168\n",
            "Step : 1731, train Loss : 0.0181\n",
            "Step : 1741, train Loss : 0.0165\n",
            "Step : 1751, train Loss : 0.0275\n",
            "Step : 1761, train Loss : 0.0231\n",
            "Step : 1771, train Loss : 0.0208\n",
            "Step : 1781, train Loss : 0.0181\n",
            "Step : 1791, train Loss : 0.0190\n",
            "Step : 1801, train Loss : 0.0273\n",
            "Step : 1811, train Loss : 0.0296\n",
            "Step : 1821, train Loss : 0.0230\n",
            "Step : 1831, train Loss : 0.0150\n",
            "Step : 1841, train Loss : 0.0187\n",
            "Step : 1851, train Loss : 0.0225\n",
            "Step : 1861, train Loss : 0.0213\n",
            "Step : 1871, train Loss : 0.0141\n",
            "Step : 1881, train Loss : 0.0210\n",
            "Step : 1891, train Loss : 0.0197\n",
            "Step : 1901, train Loss : 0.0240\n",
            "Step : 1911, train Loss : 0.0271\n",
            "Step : 1921, train Loss : 0.0241\n",
            "Step : 1931, train Loss : 0.0243\n",
            "Step : 1941, train Loss : 0.0257\n",
            "Step : 1951, train Loss : 0.0271\n",
            "Epoch 0 Total Mean Loss : 0.0243\n",
            "*****Epoch 0 Train Finish*****\n",
            "\n",
            "SAVING EPOCH 0 ...\n",
            "*****Epoch 0 Valid Start*****\n",
            "Step: 0,  Pearson: tensor([0.8165])\n",
            "Step: 1,  Pearson: tensor([0.8834])\n",
            "Step: 2,  Pearson: tensor([0.9160])\n",
            "Step: 3,  Pearson: tensor([0.9005])\n",
            "Step: 4,  Pearson: tensor([0.9069])\n",
            "Step: 5,  Pearson: tensor([0.9798])\n",
            "Step: 6,  Pearson: tensor([0.8629])\n",
            "Step: 7,  Pearson: tensor([0.9414])\n",
            "Step: 8,  Pearson: tensor([0.9273])\n",
            "Step: 9,  Pearson: tensor([0.9349])\n",
            "Step: 10,  Pearson: tensor([0.7494])\n",
            "Step: 11,  Pearson: tensor([0.8839])\n",
            "Step: 12,  Pearson: tensor([0.9181])\n",
            "Step: 13,  Pearson: tensor([0.8940])\n",
            "Step: 14,  Pearson: tensor([0.9724])\n",
            "Step: 15,  Pearson: tensor([0.9509])\n",
            "Step: 16,  Pearson: tensor([0.9366])\n",
            "Step: 17,  Pearson: tensor([0.9538])\n",
            "Step: 18,  Pearson: tensor([0.7603])\n",
            "Step: 19,  Pearson: tensor([0.9327])\n",
            "Step: 20,  Pearson: tensor([0.8332])\n",
            "Step: 21,  Pearson: tensor([0.9477])\n",
            "Step: 22,  Pearson: tensor([0.9165])\n",
            "Step: 23,  Pearson: tensor([0.7836])\n",
            "Step: 24,  Pearson: tensor([0.9115])\n",
            "Step: 25,  Pearson: tensor([0.9239])\n",
            "Step: 26,  Pearson: tensor([0.8933])\n",
            "Step: 27,  Pearson: tensor([0.9335])\n",
            "Step: 28,  Pearson: tensor([0.9491])\n",
            "Step: 29,  Pearson: tensor([0.7935])\n",
            "Step: 30,  Pearson: tensor([0.8773])\n",
            "Step: 31,  Pearson: tensor([0.9421])\n",
            "Step: 32,  Pearson: tensor([0.9118])\n",
            "Step: 33,  Pearson: tensor([0.7905])\n",
            "Step: 34,  Pearson: tensor([0.9513])\n",
            "Step: 35,  Pearson: tensor([0.9378])\n",
            "Step: 36,  Pearson: tensor([0.8896])\n",
            "Step: 37,  Pearson: tensor([0.8722])\n",
            "Step: 38,  Pearson: tensor([0.8933])\n",
            "Step: 39,  Pearson: tensor([0.9256])\n",
            "Step: 40,  Pearson: tensor([0.9610])\n",
            "Step: 41,  Pearson: tensor([0.9248])\n",
            "Step: 42,  Pearson: tensor([0.9210])\n",
            "Step: 43,  Pearson: tensor([0.8976])\n",
            "Step: 44,  Pearson: tensor([0.9729])\n",
            "Step: 45,  Pearson: tensor([0.9384])\n",
            "Step: 46,  Pearson: tensor([0.8635])\n",
            "Step: 47,  Pearson: tensor([0.9636])\n",
            "Step: 48,  Pearson: tensor([0.8405])\n",
            "Step: 49,  Pearson: tensor([0.9156])\n",
            "Step: 50,  Pearson: tensor([0.9532])\n",
            "Step: 51,  Pearson: tensor([0.9549])\n",
            "Step: 52,  Pearson: tensor([0.9247])\n",
            "Step: 53,  Pearson: tensor([0.8977])\n",
            "Step: 54,  Pearson: tensor([0.9496])\n",
            "Step: 55,  Pearson: tensor([0.9529])\n",
            "Step: 56,  Pearson: tensor([0.8608])\n",
            "Step: 57,  Pearson: tensor([0.9411])\n",
            "Step: 58,  Pearson: tensor([0.9497])\n",
            "Step: 59,  Pearson: tensor([0.9568])\n",
            "Step: 60,  Pearson: tensor([0.9614])\n",
            "Step: 61,  Pearson: tensor([0.9698])\n",
            "Step: 62,  Pearson: tensor([0.8380])\n",
            "Step: 63,  Pearson: tensor([0.9171])\n",
            "Step: 64,  Pearson: tensor([0.9411])\n",
            "Step: 65,  Pearson: tensor([0.9494])\n",
            "Step: 66,  Pearson: tensor([0.9045])\n",
            "Step: 67,  Pearson: tensor([0.9602])\n",
            "Step: 68,  Pearson: tensor([0.9481])\n",
            "Step: 69,  Pearson: tensor([0.9297])\n",
            "Step: 70,  Pearson: tensor([0.9107])\n",
            "Step: 71,  Pearson: tensor([0.9114])\n",
            "Step: 72,  Pearson: tensor([0.9567])\n",
            "Step: 73,  Pearson: tensor([0.9041])\n",
            "Step: 74,  Pearson: tensor([0.8911])\n",
            "Step: 75,  Pearson: tensor([0.9697])\n",
            "Step: 76,  Pearson: tensor([0.9540])\n",
            "Step: 77,  Pearson: tensor([0.9292])\n",
            "Step: 78,  Pearson: tensor([0.7248])\n",
            "Step: 79,  Pearson: tensor([0.9338])\n",
            "Step: 80,  Pearson: tensor([0.9134])\n",
            "Step: 81,  Pearson: tensor([0.9511])\n",
            "Step: 82,  Pearson: tensor([0.9001])\n",
            "Step: 83,  Pearson: tensor([0.9134])\n",
            "Step: 84,  Pearson: tensor([0.9721])\n",
            "Step: 85,  Pearson: tensor([0.8808])\n",
            "Step: 86,  Pearson: tensor([0.8720])\n",
            "Step: 87,  Pearson: tensor([0.9650])\n",
            "Step: 88,  Pearson: tensor([0.9167])\n",
            "Step: 89,  Pearson: tensor([0.8707])\n",
            "Step: 90,  Pearson: tensor([0.9083])\n",
            "Step: 91,  Pearson: tensor([0.9438])\n",
            "Step: 92,  Pearson: tensor([0.9041])\n",
            "Step: 93,  Pearson: tensor([0.9740])\n",
            "Step: 94,  Pearson: tensor([0.9312])\n",
            "Step: 95,  Pearson: tensor([0.9097])\n",
            "Step: 96,  Pearson: tensor([0.9259])\n",
            "Step: 97,  Pearson: tensor([0.9573])\n",
            "Step: 98,  Pearson: tensor([0.9162])\n",
            "Step: 99,  Pearson: tensor([0.9475])\n",
            "Step: 100,  Pearson: tensor([0.9357])\n",
            "Step: 101,  Pearson: tensor([0.9184])\n",
            "Step: 102,  Pearson: tensor([0.8847])\n",
            "Step: 103,  Pearson: tensor([0.9262])\n",
            "Step: 104,  Pearson: tensor([0.9397])\n",
            "Step: 105,  Pearson: tensor([0.9003])\n",
            "Step: 106,  Pearson: tensor([0.8970])\n",
            "Step: 107,  Pearson: tensor([0.9170])\n",
            "Step: 108,  Pearson: tensor([0.9748])\n",
            "total_valid_loss :  0.599755899621806 total_f1_score :  0.8987411056376574 total_pearsonr : tensor([0.9150])\n",
            "Epoch 0 Valid Loss : 0.599755899621806 Valid Pearsonr : tensor([0.9150]) ValidF1 : 0.8987411056376574\n",
            "*****Epoch 0 Valid Finish*****\n",
            "\n",
            "****** STARTING TO TRAIN EPOCH #1 ******\n",
            "Step : 11, train Loss : 0.0137\n",
            "Step : 21, train Loss : 0.0095\n",
            "Step : 31, train Loss : 0.0106\n",
            "Step : 41, train Loss : 0.0095\n",
            "Step : 51, train Loss : 0.0069\n",
            "Step : 61, train Loss : 0.0083\n",
            "Step : 71, train Loss : 0.0115\n",
            "Step : 81, train Loss : 0.0092\n",
            "Step : 91, train Loss : 0.0113\n",
            "Step : 101, train Loss : 0.0109\n",
            "Step : 111, train Loss : 0.0075\n",
            "Step : 121, train Loss : 0.0103\n",
            "Step : 131, train Loss : 0.0110\n",
            "Step : 141, train Loss : 0.0074\n",
            "Step : 151, train Loss : 0.0088\n",
            "Step : 161, train Loss : 0.0086\n",
            "Step : 171, train Loss : 0.0098\n",
            "Step : 181, train Loss : 0.0109\n",
            "Step : 191, train Loss : 0.0092\n",
            "Step : 201, train Loss : 0.0101\n",
            "Step : 211, train Loss : 0.0103\n",
            "Step : 221, train Loss : 0.0081\n",
            "Step : 231, train Loss : 0.0123\n",
            "Step : 241, train Loss : 0.0095\n",
            "Step : 251, train Loss : 0.0088\n",
            "Step : 261, train Loss : 0.0137\n",
            "Step : 271, train Loss : 0.0095\n",
            "Step : 281, train Loss : 0.0101\n",
            "Step : 291, train Loss : 0.0120\n",
            "Step : 301, train Loss : 0.0113\n",
            "Step : 311, train Loss : 0.0085\n",
            "Step : 321, train Loss : 0.0081\n",
            "Step : 331, train Loss : 0.0063\n",
            "Step : 341, train Loss : 0.0079\n",
            "Step : 351, train Loss : 0.0105\n",
            "Step : 361, train Loss : 0.0131\n",
            "Step : 371, train Loss : 0.0121\n",
            "Step : 381, train Loss : 0.0116\n",
            "Step : 391, train Loss : 0.0088\n",
            "Step : 401, train Loss : 0.0085\n",
            "Step : 411, train Loss : 0.0088\n",
            "Step : 421, train Loss : 0.0098\n",
            "Step : 431, train Loss : 0.0077\n",
            "Step : 441, train Loss : 0.0089\n",
            "Step : 451, train Loss : 0.0122\n",
            "Step : 461, train Loss : 0.0160\n",
            "Step : 471, train Loss : 0.0082\n",
            "Step : 481, train Loss : 0.0097\n",
            "Step : 491, train Loss : 0.0092\n",
            "Step : 501, train Loss : 0.0138\n",
            "Step : 511, train Loss : 0.0115\n",
            "Step : 521, train Loss : 0.0089\n",
            "Step : 531, train Loss : 0.0155\n",
            "Step : 541, train Loss : 0.0070\n",
            "Step : 551, train Loss : 0.0121\n",
            "Step : 561, train Loss : 0.0077\n",
            "Step : 571, train Loss : 0.0093\n",
            "Step : 581, train Loss : 0.0087\n",
            "Step : 591, train Loss : 0.0126\n",
            "Step : 601, train Loss : 0.0084\n",
            "Step : 611, train Loss : 0.0092\n",
            "Step : 621, train Loss : 0.0114\n",
            "Step : 631, train Loss : 0.0078\n",
            "Step : 641, train Loss : 0.0082\n",
            "Step : 651, train Loss : 0.0075\n",
            "Step : 661, train Loss : 0.0075\n",
            "Step : 671, train Loss : 0.0107\n",
            "Step : 681, train Loss : 0.0095\n",
            "Step : 691, train Loss : 0.0087\n",
            "Step : 701, train Loss : 0.0118\n",
            "Step : 711, train Loss : 0.0112\n",
            "Step : 721, train Loss : 0.0083\n",
            "Step : 731, train Loss : 0.0126\n",
            "Step : 741, train Loss : 0.0087\n",
            "Step : 751, train Loss : 0.0169\n",
            "Step : 761, train Loss : 0.0113\n",
            "Step : 771, train Loss : 0.0102\n",
            "Step : 781, train Loss : 0.0104\n",
            "Step : 791, train Loss : 0.0102\n",
            "Step : 801, train Loss : 0.0080\n",
            "Step : 811, train Loss : 0.0075\n",
            "Step : 821, train Loss : 0.0119\n",
            "Step : 831, train Loss : 0.0095\n",
            "Step : 841, train Loss : 0.0127\n",
            "Step : 851, train Loss : 0.0105\n",
            "Step : 861, train Loss : 0.0102\n",
            "Step : 871, train Loss : 0.0101\n",
            "Step : 881, train Loss : 0.0078\n",
            "Step : 891, train Loss : 0.0081\n",
            "Step : 901, train Loss : 0.0070\n",
            "Step : 911, train Loss : 0.0089\n",
            "Step : 921, train Loss : 0.0100\n",
            "Step : 931, train Loss : 0.0089\n",
            "Step : 941, train Loss : 0.0099\n",
            "Step : 951, train Loss : 0.0074\n",
            "Step : 961, train Loss : 0.0091\n",
            "Step : 971, train Loss : 0.0093\n",
            "Step : 981, train Loss : 0.0113\n",
            "Step : 991, train Loss : 0.0071\n",
            "Step : 1001, train Loss : 0.0078\n",
            "Step : 1011, train Loss : 0.0076\n",
            "Step : 1021, train Loss : 0.0107\n",
            "Step : 1031, train Loss : 0.0090\n",
            "Step : 1041, train Loss : 0.0125\n",
            "Step : 1051, train Loss : 0.0095\n",
            "Step : 1061, train Loss : 0.0082\n",
            "Step : 1071, train Loss : 0.0089\n",
            "Step : 1081, train Loss : 0.0118\n",
            "Step : 1091, train Loss : 0.0125\n",
            "Step : 1101, train Loss : 0.0088\n",
            "Step : 1111, train Loss : 0.0085\n",
            "Step : 1121, train Loss : 0.0102\n",
            "Step : 1131, train Loss : 0.0099\n",
            "Step : 1141, train Loss : 0.0089\n",
            "Step : 1151, train Loss : 0.0087\n",
            "Step : 1161, train Loss : 0.0140\n",
            "Step : 1171, train Loss : 0.0091\n",
            "Step : 1181, train Loss : 0.0103\n",
            "Step : 1191, train Loss : 0.0111\n",
            "Step : 1201, train Loss : 0.0105\n",
            "Step : 1211, train Loss : 0.0085\n",
            "Step : 1221, train Loss : 0.0117\n",
            "Step : 1231, train Loss : 0.0070\n",
            "Step : 1241, train Loss : 0.0111\n",
            "Step : 1251, train Loss : 0.0096\n",
            "Step : 1261, train Loss : 0.0156\n",
            "Step : 1271, train Loss : 0.0112\n",
            "Step : 1281, train Loss : 0.0090\n",
            "Step : 1291, train Loss : 0.0173\n",
            "Step : 1301, train Loss : 0.0151\n",
            "Step : 1311, train Loss : 0.0087\n",
            "Step : 1321, train Loss : 0.0084\n",
            "Step : 1331, train Loss : 0.0070\n",
            "Step : 1341, train Loss : 0.0080\n",
            "Step : 1351, train Loss : 0.0103\n",
            "Step : 1361, train Loss : 0.0108\n",
            "Step : 1371, train Loss : 0.0166\n",
            "Step : 1381, train Loss : 0.0092\n",
            "Step : 1391, train Loss : 0.0128\n",
            "Step : 1401, train Loss : 0.0108\n",
            "Step : 1411, train Loss : 0.0094\n",
            "Step : 1421, train Loss : 0.0078\n",
            "Step : 1431, train Loss : 0.0087\n",
            "Step : 1441, train Loss : 0.0101\n",
            "Step : 1451, train Loss : 0.0080\n",
            "Step : 1461, train Loss : 0.0110\n",
            "Step : 1471, train Loss : 0.0104\n",
            "Step : 1481, train Loss : 0.0097\n",
            "Step : 1491, train Loss : 0.0085\n",
            "Step : 1501, train Loss : 0.0109\n",
            "Step : 1511, train Loss : 0.0100\n",
            "Step : 1521, train Loss : 0.0082\n",
            "Step : 1531, train Loss : 0.0108\n",
            "Step : 1541, train Loss : 0.0151\n",
            "Step : 1551, train Loss : 0.0091\n",
            "Step : 1561, train Loss : 0.0110\n",
            "Step : 1571, train Loss : 0.0105\n",
            "Step : 1581, train Loss : 0.0108\n",
            "Step : 1591, train Loss : 0.0104\n",
            "Step : 1601, train Loss : 0.0063\n",
            "Step : 1611, train Loss : 0.0080\n",
            "Step : 1621, train Loss : 0.0124\n",
            "Step : 1631, train Loss : 0.0114\n",
            "Step : 1641, train Loss : 0.0080\n",
            "Step : 1651, train Loss : 0.0094\n",
            "Step : 1661, train Loss : 0.0099\n",
            "Step : 1671, train Loss : 0.0125\n",
            "Step : 1681, train Loss : 0.0113\n",
            "Step : 1691, train Loss : 0.0094\n",
            "Step : 1701, train Loss : 0.0064\n",
            "Step : 1711, train Loss : 0.0078\n",
            "Step : 1721, train Loss : 0.0092\n",
            "Step : 1731, train Loss : 0.0141\n",
            "Step : 1741, train Loss : 0.0076\n",
            "Step : 1751, train Loss : 0.0130\n",
            "Step : 1761, train Loss : 0.0094\n",
            "Step : 1771, train Loss : 0.0093\n",
            "Step : 1781, train Loss : 0.0112\n",
            "Step : 1791, train Loss : 0.0109\n",
            "Step : 1801, train Loss : 0.0082\n",
            "Step : 1811, train Loss : 0.0107\n",
            "Step : 1821, train Loss : 0.0111\n",
            "Step : 1831, train Loss : 0.0088\n",
            "Step : 1841, train Loss : 0.0131\n",
            "Step : 1851, train Loss : 0.0100\n",
            "Step : 1861, train Loss : 0.0082\n",
            "Step : 1871, train Loss : 0.0087\n",
            "Step : 1881, train Loss : 0.0117\n",
            "Step : 1891, train Loss : 0.0084\n",
            "Step : 1901, train Loss : 0.0103\n",
            "Step : 1911, train Loss : 0.0089\n",
            "Step : 1921, train Loss : 0.0108\n",
            "Step : 1931, train Loss : 0.0126\n",
            "Step : 1941, train Loss : 0.0091\n",
            "Step : 1951, train Loss : 0.0094\n",
            "Epoch 1 Total Mean Loss : 0.0100\n",
            "*****Epoch 1 Train Finish*****\n",
            "\n",
            "SAVING EPOCH 1 ...\n",
            "*****Epoch 1 Valid Start*****\n",
            "Step: 0,  Pearson: tensor([0.8738])\n",
            "Step: 1,  Pearson: tensor([0.9421])\n",
            "Step: 2,  Pearson: tensor([0.9532])\n",
            "Step: 3,  Pearson: tensor([0.8684])\n",
            "Step: 4,  Pearson: tensor([0.8888])\n",
            "Step: 5,  Pearson: tensor([0.9792])\n",
            "Step: 6,  Pearson: tensor([0.9223])\n",
            "Step: 7,  Pearson: tensor([0.9525])\n",
            "Step: 8,  Pearson: tensor([0.9534])\n",
            "Step: 9,  Pearson: tensor([0.9261])\n",
            "Step: 10,  Pearson: tensor([0.8100])\n",
            "Step: 11,  Pearson: tensor([0.9079])\n",
            "Step: 12,  Pearson: tensor([0.8611])\n",
            "Step: 13,  Pearson: tensor([0.9465])\n",
            "Step: 14,  Pearson: tensor([0.9516])\n",
            "Step: 15,  Pearson: tensor([0.9324])\n",
            "Step: 16,  Pearson: tensor([0.9673])\n",
            "Step: 17,  Pearson: tensor([0.9254])\n",
            "Step: 18,  Pearson: tensor([0.7882])\n",
            "Step: 19,  Pearson: tensor([0.9571])\n",
            "Step: 20,  Pearson: tensor([0.9001])\n",
            "Step: 21,  Pearson: tensor([0.9415])\n",
            "Step: 22,  Pearson: tensor([0.9389])\n",
            "Step: 23,  Pearson: tensor([0.7601])\n",
            "Step: 24,  Pearson: tensor([0.9095])\n",
            "Step: 25,  Pearson: tensor([0.9285])\n",
            "Step: 26,  Pearson: tensor([0.9377])\n",
            "Step: 27,  Pearson: tensor([0.9244])\n",
            "Step: 28,  Pearson: tensor([0.9430])\n",
            "Step: 29,  Pearson: tensor([0.8906])\n",
            "Step: 30,  Pearson: tensor([0.9354])\n",
            "Step: 31,  Pearson: tensor([0.9603])\n",
            "Step: 32,  Pearson: tensor([0.8725])\n",
            "Step: 33,  Pearson: tensor([0.8616])\n",
            "Step: 34,  Pearson: tensor([0.9506])\n",
            "Step: 35,  Pearson: tensor([0.9472])\n",
            "Step: 36,  Pearson: tensor([0.8828])\n",
            "Step: 37,  Pearson: tensor([0.8661])\n",
            "Step: 38,  Pearson: tensor([0.9052])\n",
            "Step: 39,  Pearson: tensor([0.9468])\n",
            "Step: 40,  Pearson: tensor([0.9494])\n",
            "Step: 41,  Pearson: tensor([0.9597])\n",
            "Step: 42,  Pearson: tensor([0.8973])\n",
            "Step: 43,  Pearson: tensor([0.8943])\n",
            "Step: 44,  Pearson: tensor([0.9736])\n",
            "Step: 45,  Pearson: tensor([0.9044])\n",
            "Step: 46,  Pearson: tensor([0.9082])\n",
            "Step: 47,  Pearson: tensor([0.9467])\n",
            "Step: 48,  Pearson: tensor([0.8165])\n",
            "Step: 49,  Pearson: tensor([0.9084])\n",
            "Step: 50,  Pearson: tensor([0.9326])\n",
            "Step: 51,  Pearson: tensor([0.9570])\n",
            "Step: 52,  Pearson: tensor([0.9332])\n",
            "Step: 53,  Pearson: tensor([0.9204])\n",
            "Step: 54,  Pearson: tensor([0.9409])\n",
            "Step: 55,  Pearson: tensor([0.9399])\n",
            "Step: 56,  Pearson: tensor([0.8469])\n",
            "Step: 57,  Pearson: tensor([0.9369])\n",
            "Step: 58,  Pearson: tensor([0.9609])\n",
            "Step: 59,  Pearson: tensor([0.9302])\n",
            "Step: 60,  Pearson: tensor([0.9702])\n",
            "Step: 61,  Pearson: tensor([0.9560])\n",
            "Step: 62,  Pearson: tensor([0.8351])\n",
            "Step: 63,  Pearson: tensor([0.9496])\n",
            "Step: 64,  Pearson: tensor([0.9117])\n",
            "Step: 65,  Pearson: tensor([0.9473])\n",
            "Step: 66,  Pearson: tensor([0.8873])\n",
            "Step: 67,  Pearson: tensor([0.9735])\n",
            "Step: 68,  Pearson: tensor([0.9478])\n",
            "Step: 69,  Pearson: tensor([0.9318])\n",
            "Step: 70,  Pearson: tensor([0.9282])\n",
            "Step: 71,  Pearson: tensor([0.9134])\n",
            "Step: 72,  Pearson: tensor([0.9417])\n",
            "Step: 73,  Pearson: tensor([0.8994])\n",
            "Step: 74,  Pearson: tensor([0.9218])\n",
            "Step: 75,  Pearson: tensor([0.9812])\n",
            "Step: 76,  Pearson: tensor([0.9446])\n",
            "Step: 77,  Pearson: tensor([0.9261])\n",
            "Step: 78,  Pearson: tensor([0.7628])\n",
            "Step: 79,  Pearson: tensor([0.9264])\n",
            "Step: 80,  Pearson: tensor([0.9449])\n",
            "Step: 81,  Pearson: tensor([0.9414])\n",
            "Step: 82,  Pearson: tensor([0.9410])\n",
            "Step: 83,  Pearson: tensor([0.8907])\n",
            "Step: 84,  Pearson: tensor([0.9487])\n",
            "Step: 85,  Pearson: tensor([0.9085])\n",
            "Step: 86,  Pearson: tensor([0.8805])\n",
            "Step: 87,  Pearson: tensor([0.9668])\n",
            "Step: 88,  Pearson: tensor([0.8844])\n",
            "Step: 89,  Pearson: tensor([0.8058])\n",
            "Step: 90,  Pearson: tensor([0.9243])\n",
            "Step: 91,  Pearson: tensor([0.9508])\n",
            "Step: 92,  Pearson: tensor([0.9016])\n",
            "Step: 93,  Pearson: tensor([0.9536])\n",
            "Step: 94,  Pearson: tensor([0.8732])\n",
            "Step: 95,  Pearson: tensor([0.9234])\n",
            "Step: 96,  Pearson: tensor([0.9515])\n",
            "Step: 97,  Pearson: tensor([0.9461])\n",
            "Step: 98,  Pearson: tensor([0.9408])\n",
            "Step: 99,  Pearson: tensor([0.9548])\n",
            "Step: 100,  Pearson: tensor([0.9485])\n",
            "Step: 101,  Pearson: tensor([0.9378])\n",
            "Step: 102,  Pearson: tensor([0.8808])\n",
            "Step: 103,  Pearson: tensor([0.9423])\n",
            "Step: 104,  Pearson: tensor([0.8900])\n",
            "Step: 105,  Pearson: tensor([0.9104])\n",
            "Step: 106,  Pearson: tensor([0.9354])\n",
            "Step: 107,  Pearson: tensor([0.9167])\n",
            "Step: 108,  Pearson: tensor([0.9821])\n",
            "total_valid_loss :  0.5265638374407356 total_f1_score :  0.9079754601226994 total_pearsonr : tensor([0.9186])\n",
            "Epoch 1 Valid Loss : 0.5265638374407356 Valid Pearsonr : tensor([0.9186]) ValidF1 : 0.9079754601226994\n",
            "*****Epoch 1 Valid Finish*****\n",
            "\n",
            "****** STARTING TO TRAIN EPOCH #2 ******\n",
            "Step : 11, train Loss : 0.0059\n",
            "Step : 21, train Loss : 0.0072\n",
            "Step : 31, train Loss : 0.0056\n",
            "Step : 41, train Loss : 0.0049\n",
            "Step : 51, train Loss : 0.0074\n",
            "Step : 61, train Loss : 0.0043\n",
            "Step : 71, train Loss : 0.0070\n",
            "Step : 81, train Loss : 0.0048\n",
            "Step : 91, train Loss : 0.0045\n",
            "Step : 101, train Loss : 0.0071\n",
            "Step : 111, train Loss : 0.0054\n",
            "Step : 121, train Loss : 0.0081\n",
            "Step : 131, train Loss : 0.0056\n",
            "Step : 141, train Loss : 0.0061\n",
            "Step : 151, train Loss : 0.0059\n",
            "Step : 161, train Loss : 0.0056\n",
            "Step : 171, train Loss : 0.0060\n",
            "Step : 181, train Loss : 0.0057\n",
            "Step : 191, train Loss : 0.0099\n",
            "Step : 201, train Loss : 0.0054\n",
            "Step : 211, train Loss : 0.0053\n",
            "Step : 221, train Loss : 0.0063\n",
            "Step : 231, train Loss : 0.0061\n",
            "Step : 241, train Loss : 0.0051\n",
            "Step : 251, train Loss : 0.0056\n",
            "Step : 261, train Loss : 0.0082\n",
            "Step : 271, train Loss : 0.0042\n",
            "Step : 281, train Loss : 0.0060\n",
            "Step : 291, train Loss : 0.0054\n",
            "Step : 301, train Loss : 0.0073\n",
            "Step : 311, train Loss : 0.0080\n",
            "Step : 321, train Loss : 0.0070\n",
            "Step : 331, train Loss : 0.0065\n",
            "Step : 341, train Loss : 0.0058\n",
            "Step : 351, train Loss : 0.0042\n",
            "Step : 361, train Loss : 0.0074\n",
            "Step : 371, train Loss : 0.0043\n",
            "Step : 381, train Loss : 0.0057\n",
            "Step : 391, train Loss : 0.0059\n",
            "Step : 401, train Loss : 0.0048\n",
            "Step : 411, train Loss : 0.0051\n",
            "Step : 421, train Loss : 0.0048\n",
            "Step : 431, train Loss : 0.0044\n",
            "Step : 441, train Loss : 0.0066\n",
            "Step : 451, train Loss : 0.0065\n",
            "Step : 461, train Loss : 0.0041\n",
            "Step : 471, train Loss : 0.0052\n",
            "Step : 481, train Loss : 0.0055\n",
            "Step : 491, train Loss : 0.0055\n",
            "Step : 501, train Loss : 0.0061\n",
            "Step : 511, train Loss : 0.0054\n",
            "Step : 521, train Loss : 0.0068\n",
            "Step : 531, train Loss : 0.0057\n",
            "Step : 541, train Loss : 0.0066\n",
            "Step : 551, train Loss : 0.0060\n",
            "Step : 561, train Loss : 0.0074\n",
            "Step : 571, train Loss : 0.0052\n",
            "Step : 581, train Loss : 0.0077\n",
            "Step : 591, train Loss : 0.0059\n",
            "Step : 601, train Loss : 0.0046\n",
            "Step : 611, train Loss : 0.0064\n",
            "Step : 621, train Loss : 0.0067\n",
            "Step : 631, train Loss : 0.0059\n",
            "Step : 641, train Loss : 0.0062\n",
            "Step : 651, train Loss : 0.0069\n",
            "Step : 661, train Loss : 0.0068\n",
            "Step : 671, train Loss : 0.0065\n",
            "Step : 681, train Loss : 0.0057\n",
            "Step : 691, train Loss : 0.0057\n",
            "Step : 701, train Loss : 0.0060\n",
            "Step : 711, train Loss : 0.0065\n",
            "Step : 721, train Loss : 0.0048\n",
            "Step : 731, train Loss : 0.0068\n",
            "Step : 741, train Loss : 0.0069\n",
            "Step : 751, train Loss : 0.0042\n",
            "Step : 761, train Loss : 0.0047\n",
            "Step : 771, train Loss : 0.0059\n",
            "Step : 781, train Loss : 0.0076\n",
            "Step : 791, train Loss : 0.0065\n",
            "Step : 801, train Loss : 0.0041\n",
            "Step : 811, train Loss : 0.0042\n",
            "Step : 821, train Loss : 0.0053\n",
            "Step : 831, train Loss : 0.0064\n",
            "Step : 841, train Loss : 0.0059\n",
            "Step : 851, train Loss : 0.0044\n",
            "Step : 861, train Loss : 0.0102\n",
            "Step : 871, train Loss : 0.0051\n",
            "Step : 881, train Loss : 0.0051\n",
            "Step : 891, train Loss : 0.0061\n",
            "Step : 901, train Loss : 0.0047\n",
            "Step : 911, train Loss : 0.0054\n",
            "Step : 921, train Loss : 0.0052\n",
            "Step : 931, train Loss : 0.0051\n",
            "Step : 941, train Loss : 0.0058\n",
            "Step : 951, train Loss : 0.0064\n",
            "Step : 961, train Loss : 0.0054\n",
            "Step : 971, train Loss : 0.0042\n",
            "Step : 981, train Loss : 0.0078\n",
            "Step : 991, train Loss : 0.0045\n",
            "Step : 1001, train Loss : 0.0073\n",
            "Step : 1011, train Loss : 0.0060\n",
            "Step : 1021, train Loss : 0.0067\n",
            "Step : 1031, train Loss : 0.0048\n",
            "Step : 1041, train Loss : 0.0074\n",
            "Step : 1051, train Loss : 0.0056\n",
            "Step : 1061, train Loss : 0.0070\n",
            "Step : 1071, train Loss : 0.0079\n",
            "Step : 1081, train Loss : 0.0050\n",
            "Step : 1091, train Loss : 0.0066\n",
            "Step : 1101, train Loss : 0.0044\n",
            "Step : 1111, train Loss : 0.0052\n",
            "Step : 1121, train Loss : 0.0046\n",
            "Step : 1131, train Loss : 0.0065\n",
            "Step : 1141, train Loss : 0.0049\n",
            "Step : 1151, train Loss : 0.0074\n",
            "Step : 1161, train Loss : 0.0051\n",
            "Step : 1171, train Loss : 0.0058\n",
            "Step : 1181, train Loss : 0.0049\n",
            "Step : 1191, train Loss : 0.0067\n",
            "Step : 1201, train Loss : 0.0056\n",
            "Step : 1211, train Loss : 0.0047\n",
            "Step : 1221, train Loss : 0.0053\n",
            "Step : 1231, train Loss : 0.0054\n",
            "Step : 1241, train Loss : 0.0073\n",
            "Step : 1251, train Loss : 0.0051\n",
            "Step : 1261, train Loss : 0.0062\n",
            "Step : 1271, train Loss : 0.0046\n",
            "Step : 1281, train Loss : 0.0073\n",
            "Step : 1291, train Loss : 0.0046\n",
            "Step : 1301, train Loss : 0.0046\n",
            "Step : 1311, train Loss : 0.0058\n",
            "Step : 1321, train Loss : 0.0041\n",
            "Step : 1331, train Loss : 0.0035\n",
            "Step : 1341, train Loss : 0.0049\n",
            "Step : 1351, train Loss : 0.0052\n",
            "Step : 1361, train Loss : 0.0045\n",
            "Step : 1371, train Loss : 0.0071\n",
            "Step : 1381, train Loss : 0.0050\n",
            "Step : 1391, train Loss : 0.0041\n",
            "Step : 1401, train Loss : 0.0064\n",
            "Step : 1411, train Loss : 0.0050\n",
            "Step : 1421, train Loss : 0.0043\n",
            "Step : 1431, train Loss : 0.0051\n",
            "Step : 1441, train Loss : 0.0048\n",
            "Step : 1451, train Loss : 0.0056\n",
            "Step : 1461, train Loss : 0.0039\n",
            "Step : 1471, train Loss : 0.0043\n",
            "Step : 1481, train Loss : 0.0043\n",
            "Step : 1491, train Loss : 0.0045\n",
            "Step : 1501, train Loss : 0.0048\n",
            "Step : 1511, train Loss : 0.0042\n",
            "Step : 1521, train Loss : 0.0063\n",
            "Step : 1531, train Loss : 0.0034\n",
            "Step : 1541, train Loss : 0.0042\n",
            "Step : 1551, train Loss : 0.0063\n",
            "Step : 1561, train Loss : 0.0050\n",
            "Step : 1571, train Loss : 0.0052\n",
            "Step : 1581, train Loss : 0.0059\n",
            "Step : 1591, train Loss : 0.0049\n",
            "Step : 1601, train Loss : 0.0046\n",
            "Step : 1611, train Loss : 0.0058\n",
            "Step : 1621, train Loss : 0.0048\n",
            "Step : 1631, train Loss : 0.0049\n",
            "Step : 1641, train Loss : 0.0055\n",
            "Step : 1651, train Loss : 0.0060\n",
            "Step : 1661, train Loss : 0.0062\n",
            "Step : 1671, train Loss : 0.0062\n",
            "Step : 1681, train Loss : 0.0052\n",
            "Step : 1691, train Loss : 0.0059\n",
            "Step : 1701, train Loss : 0.0047\n",
            "Step : 1711, train Loss : 0.0060\n",
            "Step : 1721, train Loss : 0.0043\n",
            "Step : 1731, train Loss : 0.0052\n",
            "Step : 1741, train Loss : 0.0072\n",
            "Step : 1751, train Loss : 0.0048\n",
            "Step : 1761, train Loss : 0.0061\n",
            "Step : 1771, train Loss : 0.0041\n",
            "Step : 1781, train Loss : 0.0080\n",
            "Step : 1791, train Loss : 0.0060\n",
            "Step : 1801, train Loss : 0.0047\n",
            "Step : 1811, train Loss : 0.0058\n",
            "Step : 1821, train Loss : 0.0051\n",
            "Step : 1831, train Loss : 0.0044\n",
            "Step : 1841, train Loss : 0.0063\n",
            "Step : 1851, train Loss : 0.0049\n",
            "Step : 1861, train Loss : 0.0078\n",
            "Step : 1871, train Loss : 0.0056\n",
            "Step : 1881, train Loss : 0.0059\n",
            "Step : 1891, train Loss : 0.0075\n",
            "Step : 1901, train Loss : 0.0046\n",
            "Step : 1911, train Loss : 0.0049\n",
            "Step : 1921, train Loss : 0.0057\n",
            "Step : 1931, train Loss : 0.0049\n",
            "Step : 1941, train Loss : 0.0075\n",
            "Step : 1951, train Loss : 0.0060\n",
            "Epoch 2 Total Mean Loss : 0.0057\n",
            "*****Epoch 2 Train Finish*****\n",
            "\n",
            "SAVING EPOCH 2 ...\n",
            "*****Epoch 2 Valid Start*****\n",
            "Step: 0,  Pearson: tensor([0.8823])\n",
            "Step: 1,  Pearson: tensor([0.9448])\n",
            "Step: 2,  Pearson: tensor([0.9326])\n",
            "Step: 3,  Pearson: tensor([0.8934])\n",
            "Step: 4,  Pearson: tensor([0.8585])\n",
            "Step: 5,  Pearson: tensor([0.9761])\n",
            "Step: 6,  Pearson: tensor([0.9033])\n",
            "Step: 7,  Pearson: tensor([0.9446])\n",
            "Step: 8,  Pearson: tensor([0.9579])\n",
            "Step: 9,  Pearson: tensor([0.9408])\n",
            "Step: 10,  Pearson: tensor([0.7983])\n",
            "Step: 11,  Pearson: tensor([0.9002])\n",
            "Step: 12,  Pearson: tensor([0.8868])\n",
            "Step: 13,  Pearson: tensor([0.9117])\n",
            "Step: 14,  Pearson: tensor([0.9717])\n",
            "Step: 15,  Pearson: tensor([0.9658])\n",
            "Step: 16,  Pearson: tensor([0.9696])\n",
            "Step: 17,  Pearson: tensor([0.9431])\n",
            "Step: 18,  Pearson: tensor([0.7483])\n",
            "Step: 19,  Pearson: tensor([0.9373])\n",
            "Step: 20,  Pearson: tensor([0.8829])\n",
            "Step: 21,  Pearson: tensor([0.9495])\n",
            "Step: 22,  Pearson: tensor([0.9448])\n",
            "Step: 23,  Pearson: tensor([0.7906])\n",
            "Step: 24,  Pearson: tensor([0.9139])\n",
            "Step: 25,  Pearson: tensor([0.9270])\n",
            "Step: 26,  Pearson: tensor([0.9024])\n",
            "Step: 27,  Pearson: tensor([0.9193])\n",
            "Step: 28,  Pearson: tensor([0.9562])\n",
            "Step: 29,  Pearson: tensor([0.8468])\n",
            "Step: 30,  Pearson: tensor([0.9341])\n",
            "Step: 31,  Pearson: tensor([0.9569])\n",
            "Step: 32,  Pearson: tensor([0.9166])\n",
            "Step: 33,  Pearson: tensor([0.8697])\n",
            "Step: 34,  Pearson: tensor([0.9462])\n",
            "Step: 35,  Pearson: tensor([0.9503])\n",
            "Step: 36,  Pearson: tensor([0.9108])\n",
            "Step: 37,  Pearson: tensor([0.8776])\n",
            "Step: 38,  Pearson: tensor([0.8976])\n",
            "Step: 39,  Pearson: tensor([0.9423])\n",
            "Step: 40,  Pearson: tensor([0.9614])\n",
            "Step: 41,  Pearson: tensor([0.9604])\n",
            "Step: 42,  Pearson: tensor([0.9053])\n",
            "Step: 43,  Pearson: tensor([0.9009])\n",
            "Step: 44,  Pearson: tensor([0.9782])\n",
            "Step: 45,  Pearson: tensor([0.9216])\n",
            "Step: 46,  Pearson: tensor([0.9334])\n",
            "Step: 47,  Pearson: tensor([0.9527])\n",
            "Step: 48,  Pearson: tensor([0.8355])\n",
            "Step: 49,  Pearson: tensor([0.8855])\n",
            "Step: 50,  Pearson: tensor([0.9398])\n",
            "Step: 51,  Pearson: tensor([0.9446])\n",
            "Step: 52,  Pearson: tensor([0.9316])\n",
            "Step: 53,  Pearson: tensor([0.9100])\n",
            "Step: 54,  Pearson: tensor([0.9529])\n",
            "Step: 55,  Pearson: tensor([0.9454])\n",
            "Step: 56,  Pearson: tensor([0.9060])\n",
            "Step: 57,  Pearson: tensor([0.9537])\n",
            "Step: 58,  Pearson: tensor([0.9645])\n",
            "Step: 59,  Pearson: tensor([0.9344])\n",
            "Step: 60,  Pearson: tensor([0.9640])\n",
            "Step: 61,  Pearson: tensor([0.9630])\n",
            "Step: 62,  Pearson: tensor([0.8332])\n",
            "Step: 63,  Pearson: tensor([0.9596])\n",
            "Step: 64,  Pearson: tensor([0.9500])\n",
            "Step: 65,  Pearson: tensor([0.9553])\n",
            "Step: 66,  Pearson: tensor([0.9165])\n",
            "Step: 67,  Pearson: tensor([0.9777])\n",
            "Step: 68,  Pearson: tensor([0.9376])\n",
            "Step: 69,  Pearson: tensor([0.9255])\n",
            "Step: 70,  Pearson: tensor([0.9437])\n",
            "Step: 71,  Pearson: tensor([0.9098])\n",
            "Step: 72,  Pearson: tensor([0.9641])\n",
            "Step: 73,  Pearson: tensor([0.8808])\n",
            "Step: 74,  Pearson: tensor([0.9160])\n",
            "Step: 75,  Pearson: tensor([0.9845])\n",
            "Step: 76,  Pearson: tensor([0.9430])\n",
            "Step: 77,  Pearson: tensor([0.9393])\n",
            "Step: 78,  Pearson: tensor([0.7633])\n",
            "Step: 79,  Pearson: tensor([0.9379])\n",
            "Step: 80,  Pearson: tensor([0.9433])\n",
            "Step: 81,  Pearson: tensor([0.9610])\n",
            "Step: 82,  Pearson: tensor([0.9582])\n",
            "Step: 83,  Pearson: tensor([0.9201])\n",
            "Step: 84,  Pearson: tensor([0.9631])\n",
            "Step: 85,  Pearson: tensor([0.8883])\n",
            "Step: 86,  Pearson: tensor([0.8734])\n",
            "Step: 87,  Pearson: tensor([0.9611])\n",
            "Step: 88,  Pearson: tensor([0.9040])\n",
            "Step: 89,  Pearson: tensor([0.8391])\n",
            "Step: 90,  Pearson: tensor([0.9461])\n",
            "Step: 91,  Pearson: tensor([0.9596])\n",
            "Step: 92,  Pearson: tensor([0.9184])\n",
            "Step: 93,  Pearson: tensor([0.9688])\n",
            "Step: 94,  Pearson: tensor([0.9078])\n",
            "Step: 95,  Pearson: tensor([0.9158])\n",
            "Step: 96,  Pearson: tensor([0.9147])\n",
            "Step: 97,  Pearson: tensor([0.9521])\n",
            "Step: 98,  Pearson: tensor([0.9021])\n",
            "Step: 99,  Pearson: tensor([0.9678])\n",
            "Step: 100,  Pearson: tensor([0.9475])\n",
            "Step: 101,  Pearson: tensor([0.9309])\n",
            "Step: 102,  Pearson: tensor([0.9221])\n",
            "Step: 103,  Pearson: tensor([0.9437])\n",
            "Step: 104,  Pearson: tensor([0.8933])\n",
            "Step: 105,  Pearson: tensor([0.9142])\n",
            "Step: 106,  Pearson: tensor([0.9278])\n",
            "Step: 107,  Pearson: tensor([0.9151])\n",
            "Step: 108,  Pearson: tensor([0.9808])\n",
            "total_valid_loss :  0.5131733629408233 total_f1_score :  0.910803324099723 total_pearsonr : tensor([0.9228])\n",
            "Epoch 2 Valid Loss : 0.5131733629408233 Valid Pearsonr : tensor([0.9228]) ValidF1 : 0.910803324099723\n",
            "*****Epoch 2 Valid Finish*****\n",
            "\n",
            "****** STARTING TO TRAIN EPOCH #3 ******\n",
            "Step : 11, train Loss : 0.0041\n",
            "Step : 21, train Loss : 0.0040\n",
            "Step : 31, train Loss : 0.0042\n",
            "Step : 41, train Loss : 0.0041\n",
            "Step : 51, train Loss : 0.0043\n",
            "Step : 61, train Loss : 0.0037\n",
            "Step : 71, train Loss : 0.0034\n",
            "Step : 81, train Loss : 0.0036\n",
            "Step : 91, train Loss : 0.0028\n",
            "Step : 101, train Loss : 0.0034\n",
            "Step : 111, train Loss : 0.0049\n",
            "Step : 121, train Loss : 0.0036\n",
            "Step : 131, train Loss : 0.0047\n",
            "Step : 141, train Loss : 0.0038\n",
            "Step : 151, train Loss : 0.0051\n",
            "Step : 161, train Loss : 0.0036\n",
            "Step : 171, train Loss : 0.0039\n",
            "Step : 181, train Loss : 0.0027\n",
            "Step : 191, train Loss : 0.0034\n",
            "Step : 201, train Loss : 0.0033\n",
            "Step : 211, train Loss : 0.0055\n",
            "Step : 221, train Loss : 0.0034\n",
            "Step : 231, train Loss : 0.0031\n",
            "Step : 241, train Loss : 0.0039\n",
            "Step : 251, train Loss : 0.0039\n",
            "Step : 261, train Loss : 0.0042\n",
            "Step : 271, train Loss : 0.0030\n",
            "Step : 281, train Loss : 0.0040\n",
            "Step : 291, train Loss : 0.0034\n",
            "Step : 301, train Loss : 0.0030\n",
            "Step : 311, train Loss : 0.0040\n",
            "Step : 321, train Loss : 0.0035\n",
            "Step : 331, train Loss : 0.0027\n",
            "Step : 341, train Loss : 0.0038\n",
            "Step : 351, train Loss : 0.0041\n",
            "Step : 361, train Loss : 0.0036\n",
            "Step : 371, train Loss : 0.0027\n",
            "Step : 381, train Loss : 0.0047\n",
            "Step : 391, train Loss : 0.0044\n",
            "Step : 401, train Loss : 0.0043\n",
            "Step : 411, train Loss : 0.0032\n",
            "Step : 421, train Loss : 0.0038\n",
            "Step : 431, train Loss : 0.0036\n",
            "Step : 441, train Loss : 0.0035\n",
            "Step : 451, train Loss : 0.0061\n",
            "Step : 461, train Loss : 0.0041\n",
            "Step : 471, train Loss : 0.0036\n",
            "Step : 481, train Loss : 0.0030\n",
            "Step : 491, train Loss : 0.0031\n",
            "Step : 501, train Loss : 0.0028\n",
            "Step : 511, train Loss : 0.0033\n",
            "Step : 521, train Loss : 0.0037\n",
            "Step : 531, train Loss : 0.0036\n",
            "Step : 541, train Loss : 0.0028\n",
            "Step : 551, train Loss : 0.0053\n",
            "Step : 561, train Loss : 0.0032\n",
            "Step : 571, train Loss : 0.0027\n",
            "Step : 581, train Loss : 0.0031\n",
            "Step : 591, train Loss : 0.0048\n",
            "Step : 601, train Loss : 0.0038\n",
            "Step : 611, train Loss : 0.0036\n",
            "Step : 621, train Loss : 0.0037\n",
            "Step : 631, train Loss : 0.0026\n",
            "Step : 641, train Loss : 0.0034\n",
            "Step : 651, train Loss : 0.0032\n",
            "Step : 661, train Loss : 0.0030\n",
            "Step : 671, train Loss : 0.0039\n",
            "Step : 681, train Loss : 0.0033\n",
            "Step : 691, train Loss : 0.0049\n",
            "Step : 701, train Loss : 0.0041\n",
            "Step : 711, train Loss : 0.0033\n",
            "Step : 721, train Loss : 0.0032\n",
            "Step : 731, train Loss : 0.0040\n",
            "Step : 741, train Loss : 0.0039\n",
            "Step : 751, train Loss : 0.0025\n",
            "Step : 761, train Loss : 0.0047\n",
            "Step : 771, train Loss : 0.0027\n",
            "Step : 781, train Loss : 0.0031\n",
            "Step : 791, train Loss : 0.0022\n",
            "Step : 801, train Loss : 0.0044\n",
            "Step : 811, train Loss : 0.0035\n",
            "Step : 821, train Loss : 0.0039\n",
            "Step : 831, train Loss : 0.0034\n",
            "Step : 841, train Loss : 0.0038\n",
            "Step : 851, train Loss : 0.0039\n",
            "Step : 861, train Loss : 0.0048\n",
            "Step : 871, train Loss : 0.0030\n",
            "Step : 881, train Loss : 0.0041\n",
            "Step : 891, train Loss : 0.0034\n",
            "Step : 901, train Loss : 0.0041\n",
            "Step : 911, train Loss : 0.0032\n",
            "Step : 921, train Loss : 0.0038\n",
            "Step : 931, train Loss : 0.0041\n",
            "Step : 941, train Loss : 0.0033\n",
            "Step : 951, train Loss : 0.0030\n",
            "Step : 961, train Loss : 0.0033\n",
            "Step : 971, train Loss : 0.0027\n",
            "Step : 981, train Loss : 0.0043\n",
            "Step : 991, train Loss : 0.0040\n",
            "Step : 1001, train Loss : 0.0050\n",
            "Step : 1011, train Loss : 0.0034\n",
            "Step : 1021, train Loss : 0.0032\n",
            "Step : 1031, train Loss : 0.0044\n",
            "Step : 1041, train Loss : 0.0036\n",
            "Step : 1051, train Loss : 0.0033\n",
            "Step : 1061, train Loss : 0.0025\n",
            "Step : 1071, train Loss : 0.0035\n",
            "Step : 1081, train Loss : 0.0037\n",
            "Step : 1091, train Loss : 0.0032\n",
            "Step : 1101, train Loss : 0.0031\n",
            "Step : 1111, train Loss : 0.0032\n",
            "Step : 1121, train Loss : 0.0039\n",
            "Step : 1131, train Loss : 0.0047\n",
            "Step : 1141, train Loss : 0.0034\n",
            "Step : 1151, train Loss : 0.0033\n",
            "Step : 1161, train Loss : 0.0038\n",
            "Step : 1171, train Loss : 0.0034\n",
            "Step : 1181, train Loss : 0.0027\n",
            "Step : 1191, train Loss : 0.0036\n",
            "Step : 1201, train Loss : 0.0040\n",
            "Step : 1211, train Loss : 0.0042\n",
            "Step : 1221, train Loss : 0.0035\n",
            "Step : 1231, train Loss : 0.0031\n",
            "Step : 1241, train Loss : 0.0026\n",
            "Step : 1251, train Loss : 0.0036\n",
            "Step : 1261, train Loss : 0.0036\n",
            "Step : 1271, train Loss : 0.0034\n",
            "Step : 1281, train Loss : 0.0041\n",
            "Step : 1291, train Loss : 0.0033\n",
            "Step : 1301, train Loss : 0.0028\n",
            "Step : 1311, train Loss : 0.0022\n",
            "Step : 1321, train Loss : 0.0036\n",
            "Step : 1331, train Loss : 0.0030\n",
            "Step : 1341, train Loss : 0.0032\n",
            "Step : 1351, train Loss : 0.0025\n",
            "Step : 1361, train Loss : 0.0034\n",
            "Step : 1371, train Loss : 0.0039\n",
            "Step : 1381, train Loss : 0.0052\n",
            "Step : 1391, train Loss : 0.0030\n",
            "Step : 1401, train Loss : 0.0032\n",
            "Step : 1411, train Loss : 0.0036\n",
            "Step : 1421, train Loss : 0.0039\n",
            "Step : 1431, train Loss : 0.0029\n",
            "Step : 1441, train Loss : 0.0023\n",
            "Step : 1451, train Loss : 0.0029\n",
            "Step : 1461, train Loss : 0.0029\n",
            "Step : 1471, train Loss : 0.0035\n",
            "Step : 1481, train Loss : 0.0046\n",
            "Step : 1491, train Loss : 0.0032\n",
            "Step : 1501, train Loss : 0.0029\n",
            "Step : 1511, train Loss : 0.0048\n",
            "Step : 1521, train Loss : 0.0029\n",
            "Step : 1531, train Loss : 0.0030\n",
            "Step : 1541, train Loss : 0.0023\n",
            "Step : 1551, train Loss : 0.0043\n",
            "Step : 1561, train Loss : 0.0028\n",
            "Step : 1571, train Loss : 0.0027\n",
            "Step : 1581, train Loss : 0.0041\n",
            "Step : 1591, train Loss : 0.0036\n",
            "Step : 1601, train Loss : 0.0040\n",
            "Step : 1611, train Loss : 0.0028\n",
            "Step : 1621, train Loss : 0.0030\n",
            "Step : 1631, train Loss : 0.0030\n",
            "Step : 1641, train Loss : 0.0037\n",
            "Step : 1651, train Loss : 0.0025\n",
            "Step : 1661, train Loss : 0.0027\n",
            "Step : 1671, train Loss : 0.0026\n",
            "Step : 1681, train Loss : 0.0046\n",
            "Step : 1691, train Loss : 0.0026\n",
            "Step : 1701, train Loss : 0.0028\n",
            "Step : 1711, train Loss : 0.0030\n",
            "Step : 1721, train Loss : 0.0026\n",
            "Step : 1731, train Loss : 0.0039\n",
            "Step : 1741, train Loss : 0.0030\n",
            "Step : 1751, train Loss : 0.0037\n",
            "Step : 1761, train Loss : 0.0039\n",
            "Step : 1771, train Loss : 0.0042\n",
            "Step : 1781, train Loss : 0.0031\n",
            "Step : 1791, train Loss : 0.0055\n",
            "Step : 1801, train Loss : 0.0027\n",
            "Step : 1811, train Loss : 0.0029\n",
            "Step : 1821, train Loss : 0.0034\n",
            "Step : 1831, train Loss : 0.0041\n",
            "Step : 1841, train Loss : 0.0031\n",
            "Step : 1851, train Loss : 0.0033\n",
            "Step : 1861, train Loss : 0.0032\n",
            "Step : 1871, train Loss : 0.0033\n",
            "Step : 1881, train Loss : 0.0029\n",
            "Step : 1891, train Loss : 0.0020\n",
            "Step : 1901, train Loss : 0.0050\n",
            "Step : 1911, train Loss : 0.0037\n",
            "Step : 1921, train Loss : 0.0025\n",
            "Step : 1931, train Loss : 0.0026\n",
            "Step : 1941, train Loss : 0.0028\n",
            "Step : 1951, train Loss : 0.0029\n",
            "Epoch 3 Total Mean Loss : 0.0035\n",
            "*****Epoch 3 Train Finish*****\n",
            "\n",
            "SAVING EPOCH 3 ...\n",
            "*****Epoch 3 Valid Start*****\n",
            "Step: 0,  Pearson: tensor([0.8681])\n",
            "Step: 1,  Pearson: tensor([0.9559])\n",
            "Step: 2,  Pearson: tensor([0.9551])\n",
            "Step: 3,  Pearson: tensor([0.8987])\n",
            "Step: 4,  Pearson: tensor([0.8779])\n",
            "Step: 5,  Pearson: tensor([0.9826])\n",
            "Step: 6,  Pearson: tensor([0.9062])\n",
            "Step: 7,  Pearson: tensor([0.9478])\n",
            "Step: 8,  Pearson: tensor([0.9470])\n",
            "Step: 9,  Pearson: tensor([0.9343])\n",
            "Step: 10,  Pearson: tensor([0.8055])\n",
            "Step: 11,  Pearson: tensor([0.9010])\n",
            "Step: 12,  Pearson: tensor([0.8870])\n",
            "Step: 13,  Pearson: tensor([0.9213])\n",
            "Step: 14,  Pearson: tensor([0.9684])\n",
            "Step: 15,  Pearson: tensor([0.9586])\n",
            "Step: 16,  Pearson: tensor([0.9706])\n",
            "Step: 17,  Pearson: tensor([0.9405])\n",
            "Step: 18,  Pearson: tensor([0.7517])\n",
            "Step: 19,  Pearson: tensor([0.9540])\n",
            "Step: 20,  Pearson: tensor([0.8961])\n",
            "Step: 21,  Pearson: tensor([0.9501])\n",
            "Step: 22,  Pearson: tensor([0.9577])\n",
            "Step: 23,  Pearson: tensor([0.7641])\n",
            "Step: 24,  Pearson: tensor([0.9120])\n",
            "Step: 25,  Pearson: tensor([0.9327])\n",
            "Step: 26,  Pearson: tensor([0.9005])\n",
            "Step: 27,  Pearson: tensor([0.9230])\n",
            "Step: 28,  Pearson: tensor([0.9600])\n",
            "Step: 29,  Pearson: tensor([0.8618])\n",
            "Step: 30,  Pearson: tensor([0.9305])\n",
            "Step: 31,  Pearson: tensor([0.9672])\n",
            "Step: 32,  Pearson: tensor([0.9040])\n",
            "Step: 33,  Pearson: tensor([0.8644])\n",
            "Step: 34,  Pearson: tensor([0.9658])\n",
            "Step: 35,  Pearson: tensor([0.9485])\n",
            "Step: 36,  Pearson: tensor([0.9022])\n",
            "Step: 37,  Pearson: tensor([0.8824])\n",
            "Step: 38,  Pearson: tensor([0.9028])\n",
            "Step: 39,  Pearson: tensor([0.9436])\n",
            "Step: 40,  Pearson: tensor([0.9601])\n",
            "Step: 41,  Pearson: tensor([0.9594])\n",
            "Step: 42,  Pearson: tensor([0.9042])\n",
            "Step: 43,  Pearson: tensor([0.8937])\n",
            "Step: 44,  Pearson: tensor([0.9813])\n",
            "Step: 45,  Pearson: tensor([0.9343])\n",
            "Step: 46,  Pearson: tensor([0.9282])\n",
            "Step: 47,  Pearson: tensor([0.9659])\n",
            "Step: 48,  Pearson: tensor([0.8527])\n",
            "Step: 49,  Pearson: tensor([0.8957])\n",
            "Step: 50,  Pearson: tensor([0.9506])\n",
            "Step: 51,  Pearson: tensor([0.9580])\n",
            "Step: 52,  Pearson: tensor([0.9327])\n",
            "Step: 53,  Pearson: tensor([0.9160])\n",
            "Step: 54,  Pearson: tensor([0.9578])\n",
            "Step: 55,  Pearson: tensor([0.9404])\n",
            "Step: 56,  Pearson: tensor([0.8997])\n",
            "Step: 57,  Pearson: tensor([0.9498])\n",
            "Step: 58,  Pearson: tensor([0.9720])\n",
            "Step: 59,  Pearson: tensor([0.9360])\n",
            "Step: 60,  Pearson: tensor([0.9679])\n",
            "Step: 61,  Pearson: tensor([0.9680])\n",
            "Step: 62,  Pearson: tensor([0.8547])\n",
            "Step: 63,  Pearson: tensor([0.9581])\n",
            "Step: 64,  Pearson: tensor([0.9433])\n",
            "Step: 65,  Pearson: tensor([0.9592])\n",
            "Step: 66,  Pearson: tensor([0.9166])\n",
            "Step: 67,  Pearson: tensor([0.9801])\n",
            "Step: 68,  Pearson: tensor([0.9587])\n",
            "Step: 69,  Pearson: tensor([0.9203])\n",
            "Step: 70,  Pearson: tensor([0.9462])\n",
            "Step: 71,  Pearson: tensor([0.9121])\n",
            "Step: 72,  Pearson: tensor([0.9597])\n",
            "Step: 73,  Pearson: tensor([0.9003])\n",
            "Step: 74,  Pearson: tensor([0.9252])\n",
            "Step: 75,  Pearson: tensor([0.9845])\n",
            "Step: 76,  Pearson: tensor([0.9380])\n",
            "Step: 77,  Pearson: tensor([0.9394])\n",
            "Step: 78,  Pearson: tensor([0.7587])\n",
            "Step: 79,  Pearson: tensor([0.9410])\n",
            "Step: 80,  Pearson: tensor([0.9477])\n",
            "Step: 81,  Pearson: tensor([0.9589])\n",
            "Step: 82,  Pearson: tensor([0.9450])\n",
            "Step: 83,  Pearson: tensor([0.9076])\n",
            "Step: 84,  Pearson: tensor([0.9629])\n",
            "Step: 85,  Pearson: tensor([0.8886])\n",
            "Step: 86,  Pearson: tensor([0.8723])\n",
            "Step: 87,  Pearson: tensor([0.9637])\n",
            "Step: 88,  Pearson: tensor([0.9137])\n",
            "Step: 89,  Pearson: tensor([0.8396])\n",
            "Step: 90,  Pearson: tensor([0.9458])\n",
            "Step: 91,  Pearson: tensor([0.9537])\n",
            "Step: 92,  Pearson: tensor([0.9107])\n",
            "Step: 93,  Pearson: tensor([0.9716])\n",
            "Step: 94,  Pearson: tensor([0.9144])\n",
            "Step: 95,  Pearson: tensor([0.9164])\n",
            "Step: 96,  Pearson: tensor([0.9197])\n",
            "Step: 97,  Pearson: tensor([0.9543])\n",
            "Step: 98,  Pearson: tensor([0.9217])\n",
            "Step: 99,  Pearson: tensor([0.9656])\n",
            "Step: 100,  Pearson: tensor([0.9510])\n",
            "Step: 101,  Pearson: tensor([0.9363])\n",
            "Step: 102,  Pearson: tensor([0.9263])\n",
            "Step: 103,  Pearson: tensor([0.9282])\n",
            "Step: 104,  Pearson: tensor([0.8804])\n",
            "Step: 105,  Pearson: tensor([0.9146])\n",
            "Step: 106,  Pearson: tensor([0.9375])\n",
            "Step: 107,  Pearson: tensor([0.9321])\n",
            "Step: 108,  Pearson: tensor([0.9843])\n",
            "total_valid_loss :  0.4936748839436321 total_f1_score :  0.9114064230343301 total_pearsonr : tensor([0.9259])\n",
            "Epoch 3 Valid Loss : 0.4936748839436321 Valid Pearsonr : tensor([0.9259]) ValidF1 : 0.9114064230343301\n",
            "*****Epoch 3 Valid Finish*****\n",
            "\n",
            "** Train Completed! **\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "54f1944f3aed4d75a2c089c1c9f877f2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▆█</td></tr><tr><td>total_f1_score </td><td>▁▆██</td></tr><tr><td>total_pearsonr</td><td>▁▃▆█</td></tr><tr><td>total_train_loss</td><td>█▃▂▁</td></tr><tr><td>total_train_lr</td><td>█▆▃▁</td></tr><tr><td>total_valid_loss</td><td>█▃▂▁</td></tr><tr><td>train_loss</td><td>▆█▆▅▄▅▅▅▅▄▂▃▃▂▃▂▃▂▂▃▂▂▂▂▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_lr</td><td>███▇▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>4</td></tr><tr><td>total_f1_score </td><td>0.91141</td></tr><tr><td>total_pearsonr</td><td>0.92591</td></tr><tr><td>total_train_loss</td><td>0.00352</td></tr><tr><td>total_train_lr</td><td>0.0</td></tr><tr><td>total_valid_loss</td><td>0.49367</td></tr><tr><td>train_loss</td><td>0.00295</td></tr><tr><td>train_lr</td><td>0.0</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">dutiful-sweep-1</strong>: <a href=\"https://wandb.ai/kdb/sts_v2/runs/6xqc4oyo\" target=\"_blank\">https://wandb.ai/kdb/sts_v2/runs/6xqc4oyo</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20220529_135152-6xqc4oyo/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ydod97j4 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \teps: 1e-08\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_batch_size: 8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tvalid_batch_size: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twarm_up_ratio: 0\n",
            "Some weights of the model checkpoint at klue/roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.17"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/NLP/wandb/run-20220529_152429-ydod97j4</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/kdb/sts_v2/runs/ydod97j4\" target=\"_blank\">valiant-sweep-2</a></strong> to <a href=\"https://wandb.ai/kdb/sts_v2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/kdb/sts_v2/sweeps/sy73kc4x\" target=\"_blank\">https://wandb.ai/kdb/sts_v2/sweeps/sy73kc4x</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "****** STARTING TO TRAIN EPOCH #0 ******\n",
            "Step : 11, train Loss : 0.0964\n",
            "Step : 21, train Loss : 0.0323\n",
            "Step : 31, train Loss : 0.0396\n",
            "Step : 41, train Loss : 0.0497\n",
            "Step : 51, train Loss : 0.0319\n",
            "Step : 61, train Loss : 0.0309\n",
            "Step : 71, train Loss : 0.0228\n",
            "Step : 81, train Loss : 0.0262\n",
            "Step : 91, train Loss : 0.0314\n",
            "Step : 101, train Loss : 0.0350\n",
            "Step : 111, train Loss : 0.0390\n",
            "Step : 121, train Loss : 0.0364\n",
            "Step : 131, train Loss : 0.0193\n",
            "Step : 141, train Loss : 0.0300\n",
            "Step : 151, train Loss : 0.0240\n",
            "Step : 161, train Loss : 0.0246\n",
            "Step : 171, train Loss : 0.0241\n",
            "Step : 181, train Loss : 0.0393\n",
            "Step : 191, train Loss : 0.0287\n",
            "Step : 201, train Loss : 0.0308\n",
            "Step : 211, train Loss : 0.0288\n",
            "Step : 221, train Loss : 0.0352\n",
            "Step : 231, train Loss : 0.0388\n",
            "Step : 241, train Loss : 0.0196\n",
            "Step : 251, train Loss : 0.0340\n",
            "Step : 261, train Loss : 0.0253\n",
            "Step : 271, train Loss : 0.0372\n",
            "Step : 281, train Loss : 0.0347\n",
            "Step : 291, train Loss : 0.0290\n",
            "Step : 301, train Loss : 0.0270\n",
            "Step : 311, train Loss : 0.0241\n",
            "Step : 321, train Loss : 0.0296\n",
            "Step : 331, train Loss : 0.0234\n",
            "Step : 341, train Loss : 0.0335\n",
            "Step : 351, train Loss : 0.0303\n",
            "Step : 361, train Loss : 0.0346\n",
            "Step : 371, train Loss : 0.0351\n",
            "Step : 381, train Loss : 0.0338\n",
            "Step : 391, train Loss : 0.0346\n",
            "Step : 401, train Loss : 0.0315\n",
            "Step : 411, train Loss : 0.0405\n",
            "Step : 421, train Loss : 0.0275\n",
            "Step : 431, train Loss : 0.0278\n",
            "Step : 441, train Loss : 0.0247\n",
            "Step : 451, train Loss : 0.0302\n",
            "Step : 461, train Loss : 0.0287\n",
            "Step : 471, train Loss : 0.0322\n",
            "Step : 481, train Loss : 0.0465\n",
            "Step : 491, train Loss : 0.0281\n",
            "Step : 501, train Loss : 0.0277\n",
            "Step : 511, train Loss : 0.0351\n",
            "Step : 521, train Loss : 0.0290\n",
            "Step : 531, train Loss : 0.0212\n",
            "Step : 541, train Loss : 0.0177\n",
            "Step : 551, train Loss : 0.0238\n",
            "Step : 561, train Loss : 0.0286\n",
            "Step : 571, train Loss : 0.0209\n",
            "Step : 581, train Loss : 0.0238\n",
            "Step : 591, train Loss : 0.0320\n",
            "Step : 601, train Loss : 0.0277\n",
            "Step : 611, train Loss : 0.0249\n",
            "Step : 621, train Loss : 0.0406\n",
            "Step : 631, train Loss : 0.0315\n",
            "Step : 641, train Loss : 0.0238\n",
            "Step : 651, train Loss : 0.0315\n",
            "Step : 661, train Loss : 0.0372\n",
            "Step : 671, train Loss : 0.0428\n",
            "Step : 681, train Loss : 0.0240\n",
            "Step : 691, train Loss : 0.0345\n",
            "Step : 701, train Loss : 0.0272\n",
            "Step : 711, train Loss : 0.0234\n",
            "Step : 721, train Loss : 0.0253\n",
            "Step : 731, train Loss : 0.0233\n",
            "Step : 741, train Loss : 0.0241\n",
            "Step : 751, train Loss : 0.0295\n",
            "Step : 761, train Loss : 0.0282\n",
            "Step : 771, train Loss : 0.0341\n",
            "Step : 781, train Loss : 0.0253\n",
            "Step : 791, train Loss : 0.0344\n",
            "Step : 801, train Loss : 0.0365\n",
            "Step : 811, train Loss : 0.0316\n",
            "Step : 821, train Loss : 0.0311\n",
            "Step : 831, train Loss : 0.0236\n",
            "Step : 841, train Loss : 0.0245\n",
            "Step : 851, train Loss : 0.0312\n",
            "Step : 861, train Loss : 0.0363\n",
            "Step : 871, train Loss : 0.0330\n",
            "Step : 881, train Loss : 0.0219\n",
            "Step : 891, train Loss : 0.0332\n",
            "Step : 901, train Loss : 0.0175\n",
            "Step : 911, train Loss : 0.0327\n",
            "Step : 921, train Loss : 0.0373\n",
            "Step : 931, train Loss : 0.0263\n",
            "Step : 941, train Loss : 0.0335\n",
            "Step : 951, train Loss : 0.0231\n",
            "Step : 961, train Loss : 0.0262\n",
            "Step : 971, train Loss : 0.0234\n",
            "Step : 981, train Loss : 0.0255\n",
            "Step : 991, train Loss : 0.0231\n",
            "Step : 1001, train Loss : 0.0201\n",
            "Step : 1011, train Loss : 0.0338\n",
            "Step : 1021, train Loss : 0.0209\n",
            "Step : 1031, train Loss : 0.0203\n",
            "Step : 1041, train Loss : 0.0302\n",
            "Step : 1051, train Loss : 0.0266\n",
            "Step : 1061, train Loss : 0.0294\n",
            "Step : 1071, train Loss : 0.0245\n",
            "Step : 1081, train Loss : 0.0186\n",
            "Step : 1091, train Loss : 0.0242\n",
            "Step : 1101, train Loss : 0.0275\n",
            "Step : 1111, train Loss : 0.0368\n",
            "Step : 1121, train Loss : 0.0264\n",
            "Step : 1131, train Loss : 0.0214\n",
            "Step : 1141, train Loss : 0.0252\n",
            "Step : 1151, train Loss : 0.0297\n",
            "Step : 1161, train Loss : 0.0261\n",
            "Step : 1171, train Loss : 0.0231\n",
            "Step : 1181, train Loss : 0.0221\n",
            "Step : 1191, train Loss : 0.0268\n",
            "Step : 1201, train Loss : 0.0189\n",
            "Step : 1211, train Loss : 0.0303\n",
            "Step : 1221, train Loss : 0.0371\n",
            "Step : 1231, train Loss : 0.0454\n",
            "Step : 1241, train Loss : 0.0207\n",
            "Step : 1251, train Loss : 0.0331\n",
            "Step : 1261, train Loss : 0.0335\n",
            "Step : 1271, train Loss : 0.0256\n",
            "Step : 1281, train Loss : 0.0245\n",
            "Step : 1291, train Loss : 0.0216\n",
            "Step : 1301, train Loss : 0.0255\n",
            "Step : 1311, train Loss : 0.0324\n",
            "Step : 1321, train Loss : 0.0248\n",
            "Step : 1331, train Loss : 0.0375\n",
            "Step : 1341, train Loss : 0.0168\n",
            "Step : 1351, train Loss : 0.0341\n",
            "Step : 1361, train Loss : 0.0251\n",
            "Step : 1371, train Loss : 0.0193\n",
            "Step : 1381, train Loss : 0.0190\n",
            "Step : 1391, train Loss : 0.0261\n",
            "Step : 1401, train Loss : 0.0198\n",
            "Step : 1411, train Loss : 0.0369\n",
            "Step : 1421, train Loss : 0.0289\n",
            "Step : 1431, train Loss : 0.0371\n",
            "Step : 1441, train Loss : 0.0289\n",
            "Step : 1451, train Loss : 0.0279\n",
            "Step : 1461, train Loss : 0.0298\n",
            "Step : 1471, train Loss : 0.0287\n",
            "Step : 1481, train Loss : 0.0275\n",
            "Step : 1491, train Loss : 0.0323\n",
            "Step : 1501, train Loss : 0.0242\n",
            "Step : 1511, train Loss : 0.0276\n",
            "Step : 1521, train Loss : 0.0229\n",
            "Step : 1531, train Loss : 0.0250\n",
            "Step : 1541, train Loss : 0.0222\n",
            "Step : 1551, train Loss : 0.0275\n",
            "Step : 1561, train Loss : 0.0307\n",
            "Step : 1571, train Loss : 0.0298\n",
            "Step : 1581, train Loss : 0.0203\n",
            "Step : 1591, train Loss : 0.0214\n",
            "Step : 1601, train Loss : 0.0332\n",
            "Step : 1611, train Loss : 0.0177\n",
            "Step : 1621, train Loss : 0.0293\n",
            "Step : 1631, train Loss : 0.0283\n",
            "Step : 1641, train Loss : 0.0221\n",
            "Step : 1651, train Loss : 0.0294\n",
            "Step : 1661, train Loss : 0.0252\n",
            "Step : 1671, train Loss : 0.0231\n",
            "Step : 1681, train Loss : 0.0372\n",
            "Step : 1691, train Loss : 0.0359\n",
            "Step : 1701, train Loss : 0.0245\n",
            "Step : 1711, train Loss : 0.0262\n",
            "Step : 1721, train Loss : 0.0312\n",
            "Step : 1731, train Loss : 0.0211\n",
            "Step : 1741, train Loss : 0.0278\n",
            "Step : 1751, train Loss : 0.0322\n",
            "Step : 1761, train Loss : 0.0205\n",
            "Step : 1771, train Loss : 0.0260\n",
            "Step : 1781, train Loss : 0.0225\n",
            "Step : 1791, train Loss : 0.0232\n",
            "Step : 1801, train Loss : 0.0177\n",
            "Step : 1811, train Loss : 0.0388\n",
            "Step : 1821, train Loss : 0.0194\n",
            "Step : 1831, train Loss : 0.0226\n",
            "Step : 1841, train Loss : 0.0242\n",
            "Step : 1851, train Loss : 0.0223\n",
            "Step : 1861, train Loss : 0.0266\n",
            "Step : 1871, train Loss : 0.0250\n",
            "Step : 1881, train Loss : 0.0224\n",
            "Step : 1891, train Loss : 0.0245\n",
            "Step : 1901, train Loss : 0.0205\n",
            "Step : 1911, train Loss : 0.0365\n",
            "Step : 1921, train Loss : 0.0247\n",
            "Step : 1931, train Loss : 0.0174\n",
            "Step : 1941, train Loss : 0.0180\n",
            "Step : 1951, train Loss : 0.0182\n",
            "Epoch 0 Total Mean Loss : 0.0285\n",
            "*****Epoch 0 Train Finish*****\n",
            "\n",
            "SAVING EPOCH 0 ...\n",
            "*****Epoch 0 Valid Start*****\n",
            "Step: 0,  Pearson: tensor([0.8688])\n",
            "Step: 1,  Pearson: tensor([0.8629])\n",
            "Step: 2,  Pearson: tensor([0.9464])\n",
            "Step: 3,  Pearson: tensor([0.8117])\n",
            "Step: 4,  Pearson: tensor([0.8601])\n",
            "Step: 5,  Pearson: tensor([0.9551])\n",
            "Step: 6,  Pearson: tensor([0.8443])\n",
            "Step: 7,  Pearson: tensor([0.9268])\n",
            "Step: 8,  Pearson: tensor([0.9228])\n",
            "Step: 9,  Pearson: tensor([0.9475])\n",
            "Step: 10,  Pearson: tensor([0.7631])\n",
            "Step: 11,  Pearson: tensor([0.8504])\n",
            "Step: 12,  Pearson: tensor([0.8968])\n",
            "Step: 13,  Pearson: tensor([0.9228])\n",
            "Step: 14,  Pearson: tensor([0.9459])\n",
            "Step: 15,  Pearson: tensor([0.9443])\n",
            "Step: 16,  Pearson: tensor([0.9191])\n",
            "Step: 17,  Pearson: tensor([0.9360])\n",
            "Step: 18,  Pearson: tensor([0.7777])\n",
            "Step: 19,  Pearson: tensor([0.9336])\n",
            "Step: 20,  Pearson: tensor([0.9169])\n",
            "Step: 21,  Pearson: tensor([0.9163])\n",
            "Step: 22,  Pearson: tensor([0.9136])\n",
            "Step: 23,  Pearson: tensor([0.7494])\n",
            "Step: 24,  Pearson: tensor([0.9021])\n",
            "Step: 25,  Pearson: tensor([0.9286])\n",
            "Step: 26,  Pearson: tensor([0.9503])\n",
            "Step: 27,  Pearson: tensor([0.9364])\n",
            "Step: 28,  Pearson: tensor([0.9144])\n",
            "Step: 29,  Pearson: tensor([0.7714])\n",
            "Step: 30,  Pearson: tensor([0.9059])\n",
            "Step: 31,  Pearson: tensor([0.9457])\n",
            "Step: 32,  Pearson: tensor([0.9365])\n",
            "Step: 33,  Pearson: tensor([0.8589])\n",
            "Step: 34,  Pearson: tensor([0.9675])\n",
            "Step: 35,  Pearson: tensor([0.9483])\n",
            "Step: 36,  Pearson: tensor([0.8611])\n",
            "Step: 37,  Pearson: tensor([0.8469])\n",
            "Step: 38,  Pearson: tensor([0.8888])\n",
            "Step: 39,  Pearson: tensor([0.9102])\n",
            "Step: 40,  Pearson: tensor([0.9197])\n",
            "Step: 41,  Pearson: tensor([0.9370])\n",
            "Step: 42,  Pearson: tensor([0.8539])\n",
            "Step: 43,  Pearson: tensor([0.8582])\n",
            "Step: 44,  Pearson: tensor([0.9368])\n",
            "Step: 45,  Pearson: tensor([0.8872])\n",
            "Step: 46,  Pearson: tensor([0.8444])\n",
            "Step: 47,  Pearson: tensor([0.9801])\n",
            "Step: 48,  Pearson: tensor([0.8599])\n",
            "Step: 49,  Pearson: tensor([0.8879])\n",
            "Step: 50,  Pearson: tensor([0.9661])\n",
            "Step: 51,  Pearson: tensor([0.9352])\n",
            "Step: 52,  Pearson: tensor([0.8840])\n",
            "Step: 53,  Pearson: tensor([0.8303])\n",
            "Step: 54,  Pearson: tensor([0.9354])\n",
            "Step: 55,  Pearson: tensor([0.8844])\n",
            "Step: 56,  Pearson: tensor([0.8499])\n",
            "Step: 57,  Pearson: tensor([0.9439])\n",
            "Step: 58,  Pearson: tensor([0.9231])\n",
            "Step: 59,  Pearson: tensor([0.9188])\n",
            "Step: 60,  Pearson: tensor([0.9747])\n",
            "Step: 61,  Pearson: tensor([0.9202])\n",
            "Step: 62,  Pearson: tensor([0.7807])\n",
            "Step: 63,  Pearson: tensor([0.8965])\n",
            "Step: 64,  Pearson: tensor([0.9196])\n",
            "Step: 65,  Pearson: tensor([0.9554])\n",
            "Step: 66,  Pearson: tensor([0.9150])\n",
            "Step: 67,  Pearson: tensor([0.9538])\n",
            "Step: 68,  Pearson: tensor([0.8937])\n",
            "Step: 69,  Pearson: tensor([0.8824])\n",
            "Step: 70,  Pearson: tensor([0.8816])\n",
            "Step: 71,  Pearson: tensor([0.9299])\n",
            "Step: 72,  Pearson: tensor([0.9486])\n",
            "Step: 73,  Pearson: tensor([0.8835])\n",
            "Step: 74,  Pearson: tensor([0.8862])\n",
            "Step: 75,  Pearson: tensor([0.9769])\n",
            "Step: 76,  Pearson: tensor([0.9646])\n",
            "Step: 77,  Pearson: tensor([0.9078])\n",
            "Step: 78,  Pearson: tensor([0.7806])\n",
            "Step: 79,  Pearson: tensor([0.9305])\n",
            "Step: 80,  Pearson: tensor([0.8722])\n",
            "Step: 81,  Pearson: tensor([0.9317])\n",
            "Step: 82,  Pearson: tensor([0.8997])\n",
            "Step: 83,  Pearson: tensor([0.9046])\n",
            "Step: 84,  Pearson: tensor([0.9763])\n",
            "Step: 85,  Pearson: tensor([0.9003])\n",
            "Step: 86,  Pearson: tensor([0.8111])\n",
            "Step: 87,  Pearson: tensor([0.9515])\n",
            "Step: 88,  Pearson: tensor([0.8752])\n",
            "Step: 89,  Pearson: tensor([0.8521])\n",
            "Step: 90,  Pearson: tensor([0.9435])\n",
            "Step: 91,  Pearson: tensor([0.9378])\n",
            "Step: 92,  Pearson: tensor([0.9052])\n",
            "Step: 93,  Pearson: tensor([0.9578])\n",
            "Step: 94,  Pearson: tensor([0.9232])\n",
            "Step: 95,  Pearson: tensor([0.9109])\n",
            "Step: 96,  Pearson: tensor([0.8664])\n",
            "Step: 97,  Pearson: tensor([0.9385])\n",
            "Step: 98,  Pearson: tensor([0.9122])\n",
            "Step: 99,  Pearson: tensor([0.9248])\n",
            "Step: 100,  Pearson: tensor([0.9328])\n",
            "Step: 101,  Pearson: tensor([0.8902])\n",
            "Step: 102,  Pearson: tensor([0.8683])\n",
            "Step: 103,  Pearson: tensor([0.8929])\n",
            "Step: 104,  Pearson: tensor([0.9378])\n",
            "Step: 105,  Pearson: tensor([0.8786])\n",
            "Step: 106,  Pearson: tensor([0.8925])\n",
            "Step: 107,  Pearson: tensor([0.8912])\n",
            "Step: 108,  Pearson: tensor([0.9793])\n",
            "total_valid_loss :  0.638405391245807 total_f1_score :  0.8911000552791597 total_pearsonr : tensor([0.9030])\n",
            "Epoch 0 Valid Loss : 0.638405391245807 Valid Pearsonr : tensor([0.9030]) ValidF1 : 0.8911000552791597\n",
            "*****Epoch 0 Valid Finish*****\n",
            "\n",
            "****** STARTING TO TRAIN EPOCH #1 ******\n",
            "Step : 11, train Loss : 0.0112\n",
            "Step : 21, train Loss : 0.0184\n",
            "Step : 31, train Loss : 0.0193\n",
            "Step : 41, train Loss : 0.0181\n",
            "Step : 51, train Loss : 0.0130\n",
            "Step : 61, train Loss : 0.0176\n",
            "Step : 71, train Loss : 0.0133\n",
            "Step : 81, train Loss : 0.0138\n",
            "Step : 91, train Loss : 0.0166\n",
            "Step : 101, train Loss : 0.0098\n",
            "Step : 111, train Loss : 0.0119\n",
            "Step : 121, train Loss : 0.0131\n",
            "Step : 131, train Loss : 0.0110\n",
            "Step : 141, train Loss : 0.0251\n",
            "Step : 151, train Loss : 0.0123\n",
            "Step : 161, train Loss : 0.0117\n",
            "Step : 171, train Loss : 0.0134\n",
            "Step : 181, train Loss : 0.0194\n",
            "Step : 191, train Loss : 0.0148\n",
            "Step : 201, train Loss : 0.0105\n",
            "Step : 211, train Loss : 0.0126\n",
            "Step : 221, train Loss : 0.0132\n",
            "Step : 231, train Loss : 0.0192\n",
            "Step : 241, train Loss : 0.0137\n",
            "Step : 251, train Loss : 0.0103\n",
            "Step : 261, train Loss : 0.0117\n",
            "Step : 271, train Loss : 0.0140\n",
            "Step : 281, train Loss : 0.0171\n",
            "Step : 291, train Loss : 0.0184\n",
            "Step : 301, train Loss : 0.0178\n",
            "Step : 311, train Loss : 0.0124\n",
            "Step : 321, train Loss : 0.0118\n",
            "Step : 331, train Loss : 0.0150\n",
            "Step : 341, train Loss : 0.0145\n",
            "Step : 351, train Loss : 0.0111\n",
            "Step : 361, train Loss : 0.0199\n",
            "Step : 371, train Loss : 0.0182\n",
            "Step : 381, train Loss : 0.0129\n",
            "Step : 391, train Loss : 0.0173\n",
            "Step : 401, train Loss : 0.0153\n",
            "Step : 411, train Loss : 0.0179\n",
            "Step : 421, train Loss : 0.0118\n",
            "Step : 431, train Loss : 0.0121\n",
            "Step : 441, train Loss : 0.0147\n",
            "Step : 451, train Loss : 0.0117\n",
            "Step : 461, train Loss : 0.0122\n",
            "Step : 471, train Loss : 0.0112\n",
            "Step : 481, train Loss : 0.0157\n",
            "Step : 491, train Loss : 0.0135\n",
            "Step : 501, train Loss : 0.0146\n",
            "Step : 511, train Loss : 0.0126\n",
            "Step : 521, train Loss : 0.0168\n",
            "Step : 531, train Loss : 0.0226\n",
            "Step : 541, train Loss : 0.0142\n",
            "Step : 551, train Loss : 0.0190\n",
            "Step : 561, train Loss : 0.0152\n",
            "Step : 571, train Loss : 0.0131\n",
            "Step : 581, train Loss : 0.0146\n",
            "Step : 591, train Loss : 0.0118\n",
            "Step : 601, train Loss : 0.0137\n",
            "Step : 611, train Loss : 0.0196\n",
            "Step : 621, train Loss : 0.0125\n",
            "Step : 631, train Loss : 0.0147\n",
            "Step : 641, train Loss : 0.0140\n",
            "Step : 651, train Loss : 0.0132\n",
            "Step : 661, train Loss : 0.0121\n",
            "Step : 671, train Loss : 0.0117\n",
            "Step : 681, train Loss : 0.0110\n",
            "Step : 691, train Loss : 0.0122\n",
            "Step : 701, train Loss : 0.0083\n",
            "Step : 711, train Loss : 0.0110\n",
            "Step : 721, train Loss : 0.0161\n",
            "Step : 731, train Loss : 0.0123\n",
            "Step : 741, train Loss : 0.0111\n",
            "Step : 751, train Loss : 0.0152\n",
            "Step : 761, train Loss : 0.0147\n",
            "Step : 771, train Loss : 0.0147\n",
            "Step : 781, train Loss : 0.0116\n",
            "Step : 791, train Loss : 0.0150\n",
            "Step : 801, train Loss : 0.0125\n",
            "Step : 811, train Loss : 0.0151\n",
            "Step : 821, train Loss : 0.0132\n",
            "Step : 831, train Loss : 0.0113\n",
            "Step : 841, train Loss : 0.0146\n",
            "Step : 851, train Loss : 0.0115\n",
            "Step : 861, train Loss : 0.0132\n",
            "Step : 871, train Loss : 0.0093\n",
            "Step : 881, train Loss : 0.0191\n",
            "Step : 891, train Loss : 0.0166\n",
            "Step : 901, train Loss : 0.0109\n",
            "Step : 911, train Loss : 0.0164\n",
            "Step : 921, train Loss : 0.0168\n",
            "Step : 931, train Loss : 0.0176\n",
            "Step : 941, train Loss : 0.0129\n",
            "Step : 951, train Loss : 0.0130\n",
            "Step : 961, train Loss : 0.0158\n",
            "Step : 971, train Loss : 0.0156\n",
            "Step : 981, train Loss : 0.0159\n",
            "Step : 991, train Loss : 0.0129\n",
            "Step : 1001, train Loss : 0.0166\n",
            "Step : 1011, train Loss : 0.0167\n",
            "Step : 1021, train Loss : 0.0136\n",
            "Step : 1031, train Loss : 0.0115\n",
            "Step : 1041, train Loss : 0.0129\n",
            "Step : 1051, train Loss : 0.0152\n",
            "Step : 1061, train Loss : 0.0119\n",
            "Step : 1071, train Loss : 0.0098\n",
            "Step : 1081, train Loss : 0.0133\n",
            "Step : 1091, train Loss : 0.0176\n",
            "Step : 1101, train Loss : 0.0131\n",
            "Step : 1111, train Loss : 0.0167\n",
            "Step : 1121, train Loss : 0.0138\n",
            "Step : 1131, train Loss : 0.0137\n",
            "Step : 1141, train Loss : 0.0137\n",
            "Step : 1151, train Loss : 0.0143\n",
            "Step : 1161, train Loss : 0.0136\n",
            "Step : 1171, train Loss : 0.0162\n",
            "Step : 1181, train Loss : 0.0137\n",
            "Step : 1191, train Loss : 0.0117\n",
            "Step : 1201, train Loss : 0.0134\n",
            "Step : 1211, train Loss : 0.0110\n",
            "Step : 1221, train Loss : 0.0129\n",
            "Step : 1231, train Loss : 0.0149\n",
            "Step : 1241, train Loss : 0.0141\n",
            "Step : 1251, train Loss : 0.0118\n",
            "Step : 1261, train Loss : 0.0138\n",
            "Step : 1271, train Loss : 0.0111\n",
            "Step : 1281, train Loss : 0.0181\n",
            "Step : 1291, train Loss : 0.0131\n",
            "Step : 1301, train Loss : 0.0222\n",
            "Step : 1311, train Loss : 0.0182\n",
            "Step : 1321, train Loss : 0.0136\n",
            "Step : 1331, train Loss : 0.0115\n",
            "Step : 1341, train Loss : 0.0137\n",
            "Step : 1351, train Loss : 0.0100\n",
            "Step : 1361, train Loss : 0.0144\n",
            "Step : 1371, train Loss : 0.0185\n",
            "Step : 1381, train Loss : 0.0208\n",
            "Step : 1391, train Loss : 0.0156\n",
            "Step : 1401, train Loss : 0.0113\n",
            "Step : 1411, train Loss : 0.0144\n",
            "Step : 1421, train Loss : 0.0176\n",
            "Step : 1431, train Loss : 0.0106\n",
            "Step : 1441, train Loss : 0.0094\n",
            "Step : 1451, train Loss : 0.0152\n",
            "Step : 1461, train Loss : 0.0132\n",
            "Step : 1471, train Loss : 0.0091\n",
            "Step : 1481, train Loss : 0.0148\n",
            "Step : 1491, train Loss : 0.0185\n",
            "Step : 1501, train Loss : 0.0125\n",
            "Step : 1511, train Loss : 0.0129\n",
            "Step : 1521, train Loss : 0.0155\n",
            "Step : 1531, train Loss : 0.0111\n",
            "Step : 1541, train Loss : 0.0121\n",
            "Step : 1551, train Loss : 0.0129\n",
            "Step : 1561, train Loss : 0.0113\n",
            "Step : 1571, train Loss : 0.0155\n",
            "Step : 1581, train Loss : 0.0131\n",
            "Step : 1591, train Loss : 0.0215\n",
            "Step : 1601, train Loss : 0.0135\n",
            "Step : 1611, train Loss : 0.0155\n",
            "Step : 1621, train Loss : 0.0129\n",
            "Step : 1631, train Loss : 0.0144\n",
            "Step : 1641, train Loss : 0.0126\n",
            "Step : 1651, train Loss : 0.0150\n",
            "Step : 1661, train Loss : 0.0122\n",
            "Step : 1671, train Loss : 0.0119\n",
            "Step : 1681, train Loss : 0.0110\n",
            "Step : 1691, train Loss : 0.0120\n",
            "Step : 1701, train Loss : 0.0173\n",
            "Step : 1711, train Loss : 0.0134\n",
            "Step : 1721, train Loss : 0.0105\n",
            "Step : 1731, train Loss : 0.0098\n",
            "Step : 1741, train Loss : 0.0162\n",
            "Step : 1751, train Loss : 0.0119\n",
            "Step : 1761, train Loss : 0.0143\n",
            "Step : 1771, train Loss : 0.0153\n",
            "Step : 1781, train Loss : 0.0171\n",
            "Step : 1791, train Loss : 0.0174\n",
            "Step : 1801, train Loss : 0.0152\n",
            "Step : 1811, train Loss : 0.0105\n",
            "Step : 1821, train Loss : 0.0122\n",
            "Step : 1831, train Loss : 0.0116\n",
            "Step : 1841, train Loss : 0.0098\n",
            "Step : 1851, train Loss : 0.0139\n",
            "Step : 1861, train Loss : 0.0098\n",
            "Step : 1871, train Loss : 0.0110\n",
            "Step : 1881, train Loss : 0.0160\n",
            "Step : 1891, train Loss : 0.0143\n",
            "Step : 1901, train Loss : 0.0194\n",
            "Step : 1911, train Loss : 0.0123\n",
            "Step : 1921, train Loss : 0.0096\n",
            "Step : 1931, train Loss : 0.0096\n",
            "Step : 1941, train Loss : 0.0115\n",
            "Step : 1951, train Loss : 0.0146\n",
            "Epoch 1 Total Mean Loss : 0.0140\n",
            "*****Epoch 1 Train Finish*****\n",
            "\n",
            "SAVING EPOCH 1 ...\n",
            "*****Epoch 1 Valid Start*****\n",
            "Step: 0,  Pearson: tensor([0.8523])\n",
            "Step: 1,  Pearson: tensor([0.8662])\n",
            "Step: 2,  Pearson: tensor([0.9536])\n",
            "Step: 3,  Pearson: tensor([0.8544])\n",
            "Step: 4,  Pearson: tensor([0.8730])\n",
            "Step: 5,  Pearson: tensor([0.9777])\n",
            "Step: 6,  Pearson: tensor([0.8311])\n",
            "Step: 7,  Pearson: tensor([0.9243])\n",
            "Step: 8,  Pearson: tensor([0.9477])\n",
            "Step: 9,  Pearson: tensor([0.9093])\n",
            "Step: 10,  Pearson: tensor([0.8267])\n",
            "Step: 11,  Pearson: tensor([0.9217])\n",
            "Step: 12,  Pearson: tensor([0.9128])\n",
            "Step: 13,  Pearson: tensor([0.9204])\n",
            "Step: 14,  Pearson: tensor([0.9480])\n",
            "Step: 15,  Pearson: tensor([0.9845])\n",
            "Step: 16,  Pearson: tensor([0.9615])\n",
            "Step: 17,  Pearson: tensor([0.9283])\n",
            "Step: 18,  Pearson: tensor([0.6672])\n",
            "Step: 19,  Pearson: tensor([0.9297])\n",
            "Step: 20,  Pearson: tensor([0.9216])\n",
            "Step: 21,  Pearson: tensor([0.9385])\n",
            "Step: 22,  Pearson: tensor([0.8869])\n",
            "Step: 23,  Pearson: tensor([0.7352])\n",
            "Step: 24,  Pearson: tensor([0.9028])\n",
            "Step: 25,  Pearson: tensor([0.9075])\n",
            "Step: 26,  Pearson: tensor([0.9296])\n",
            "Step: 27,  Pearson: tensor([0.9274])\n",
            "Step: 28,  Pearson: tensor([0.9245])\n",
            "Step: 29,  Pearson: tensor([0.8497])\n",
            "Step: 30,  Pearson: tensor([0.8987])\n",
            "Step: 31,  Pearson: tensor([0.9685])\n",
            "Step: 32,  Pearson: tensor([0.9335])\n",
            "Step: 33,  Pearson: tensor([0.8260])\n",
            "Step: 34,  Pearson: tensor([0.9482])\n",
            "Step: 35,  Pearson: tensor([0.9218])\n",
            "Step: 36,  Pearson: tensor([0.8512])\n",
            "Step: 37,  Pearson: tensor([0.8490])\n",
            "Step: 38,  Pearson: tensor([0.8765])\n",
            "Step: 39,  Pearson: tensor([0.9314])\n",
            "Step: 40,  Pearson: tensor([0.8783])\n",
            "Step: 41,  Pearson: tensor([0.9591])\n",
            "Step: 42,  Pearson: tensor([0.9120])\n",
            "Step: 43,  Pearson: tensor([0.8413])\n",
            "Step: 44,  Pearson: tensor([0.9851])\n",
            "Step: 45,  Pearson: tensor([0.8994])\n",
            "Step: 46,  Pearson: tensor([0.8692])\n",
            "Step: 47,  Pearson: tensor([0.9653])\n",
            "Step: 48,  Pearson: tensor([0.8378])\n",
            "Step: 49,  Pearson: tensor([0.8792])\n",
            "Step: 50,  Pearson: tensor([0.9468])\n",
            "Step: 51,  Pearson: tensor([0.9666])\n",
            "Step: 52,  Pearson: tensor([0.9226])\n",
            "Step: 53,  Pearson: tensor([0.8702])\n",
            "Step: 54,  Pearson: tensor([0.8846])\n",
            "Step: 55,  Pearson: tensor([0.9506])\n",
            "Step: 56,  Pearson: tensor([0.8586])\n",
            "Step: 57,  Pearson: tensor([0.9338])\n",
            "Step: 58,  Pearson: tensor([0.9443])\n",
            "Step: 59,  Pearson: tensor([0.8755])\n",
            "Step: 60,  Pearson: tensor([0.9645])\n",
            "Step: 61,  Pearson: tensor([0.9566])\n",
            "Step: 62,  Pearson: tensor([0.8307])\n",
            "Step: 63,  Pearson: tensor([0.9467])\n",
            "Step: 64,  Pearson: tensor([0.9351])\n",
            "Step: 65,  Pearson: tensor([0.9614])\n",
            "Step: 66,  Pearson: tensor([0.9356])\n",
            "Step: 67,  Pearson: tensor([0.9168])\n",
            "Step: 68,  Pearson: tensor([0.9723])\n",
            "Step: 69,  Pearson: tensor([0.8736])\n",
            "Step: 70,  Pearson: tensor([0.9010])\n",
            "Step: 71,  Pearson: tensor([0.9185])\n",
            "Step: 72,  Pearson: tensor([0.9294])\n",
            "Step: 73,  Pearson: tensor([0.8300])\n",
            "Step: 74,  Pearson: tensor([0.8516])\n",
            "Step: 75,  Pearson: tensor([0.9612])\n",
            "Step: 76,  Pearson: tensor([0.9604])\n",
            "Step: 77,  Pearson: tensor([0.9249])\n",
            "Step: 78,  Pearson: tensor([0.7405])\n",
            "Step: 79,  Pearson: tensor([0.9284])\n",
            "Step: 80,  Pearson: tensor([0.8595])\n",
            "Step: 81,  Pearson: tensor([0.8887])\n",
            "Step: 82,  Pearson: tensor([0.9550])\n",
            "Step: 83,  Pearson: tensor([0.8829])\n",
            "Step: 84,  Pearson: tensor([0.9663])\n",
            "Step: 85,  Pearson: tensor([0.8548])\n",
            "Step: 86,  Pearson: tensor([0.7727])\n",
            "Step: 87,  Pearson: tensor([0.9212])\n",
            "Step: 88,  Pearson: tensor([0.8675])\n",
            "Step: 89,  Pearson: tensor([0.8224])\n",
            "Step: 90,  Pearson: tensor([0.9539])\n",
            "Step: 91,  Pearson: tensor([0.9506])\n",
            "Step: 92,  Pearson: tensor([0.9212])\n",
            "Step: 93,  Pearson: tensor([0.9511])\n",
            "Step: 94,  Pearson: tensor([0.9130])\n",
            "Step: 95,  Pearson: tensor([0.9180])\n",
            "Step: 96,  Pearson: tensor([0.8795])\n",
            "Step: 97,  Pearson: tensor([0.9492])\n",
            "Step: 98,  Pearson: tensor([0.8830])\n",
            "Step: 99,  Pearson: tensor([0.9594])\n",
            "Step: 100,  Pearson: tensor([0.9292])\n",
            "Step: 101,  Pearson: tensor([0.8900])\n",
            "Step: 102,  Pearson: tensor([0.8927])\n",
            "Step: 103,  Pearson: tensor([0.9354])\n",
            "Step: 104,  Pearson: tensor([0.9261])\n",
            "Step: 105,  Pearson: tensor([0.8832])\n",
            "Step: 106,  Pearson: tensor([0.9258])\n",
            "Step: 107,  Pearson: tensor([0.8935])\n",
            "Step: 108,  Pearson: tensor([0.9757])\n",
            "total_valid_loss :  0.6143630877000477 total_f1_score :  0.8957871396895788 total_pearsonr : tensor([0.9071])\n",
            "Epoch 1 Valid Loss : 0.6143630877000477 Valid Pearsonr : tensor([0.9071]) ValidF1 : 0.8957871396895788\n",
            "*****Epoch 1 Valid Finish*****\n",
            "\n",
            "****** STARTING TO TRAIN EPOCH #2 ******\n",
            "Step : 11, train Loss : 0.0083\n",
            "Step : 21, train Loss : 0.0069\n",
            "Step : 31, train Loss : 0.0053\n",
            "Step : 41, train Loss : 0.0101\n",
            "Step : 51, train Loss : 0.0070\n",
            "Step : 61, train Loss : 0.0087\n",
            "Step : 71, train Loss : 0.0064\n",
            "Step : 81, train Loss : 0.0074\n",
            "Step : 91, train Loss : 0.0090\n",
            "Step : 101, train Loss : 0.0091\n",
            "Step : 111, train Loss : 0.0080\n",
            "Step : 121, train Loss : 0.0083\n",
            "Step : 131, train Loss : 0.0090\n",
            "Step : 141, train Loss : 0.0072\n",
            "Step : 151, train Loss : 0.0094\n",
            "Step : 161, train Loss : 0.0089\n",
            "Step : 171, train Loss : 0.0071\n",
            "Step : 181, train Loss : 0.0073\n",
            "Step : 191, train Loss : 0.0074\n",
            "Step : 201, train Loss : 0.0104\n",
            "Step : 211, train Loss : 0.0076\n",
            "Step : 221, train Loss : 0.0104\n",
            "Step : 231, train Loss : 0.0085\n",
            "Step : 241, train Loss : 0.0087\n",
            "Step : 251, train Loss : 0.0114\n",
            "Step : 261, train Loss : 0.0076\n",
            "Step : 271, train Loss : 0.0067\n",
            "Step : 281, train Loss : 0.0073\n",
            "Step : 291, train Loss : 0.0071\n",
            "Step : 301, train Loss : 0.0073\n",
            "Step : 311, train Loss : 0.0072\n",
            "Step : 321, train Loss : 0.0085\n",
            "Step : 331, train Loss : 0.0094\n",
            "Step : 341, train Loss : 0.0078\n",
            "Step : 351, train Loss : 0.0075\n",
            "Step : 361, train Loss : 0.0066\n",
            "Step : 371, train Loss : 0.0062\n",
            "Step : 381, train Loss : 0.0073\n",
            "Step : 391, train Loss : 0.0074\n",
            "Step : 401, train Loss : 0.0101\n",
            "Step : 411, train Loss : 0.0087\n",
            "Step : 421, train Loss : 0.0071\n",
            "Step : 431, train Loss : 0.0066\n",
            "Step : 441, train Loss : 0.0071\n",
            "Step : 451, train Loss : 0.0068\n",
            "Step : 461, train Loss : 0.0083\n",
            "Step : 471, train Loss : 0.0067\n",
            "Step : 481, train Loss : 0.0059\n",
            "Step : 491, train Loss : 0.0101\n",
            "Step : 501, train Loss : 0.0066\n",
            "Step : 511, train Loss : 0.0062\n",
            "Step : 521, train Loss : 0.0080\n",
            "Step : 531, train Loss : 0.0066\n",
            "Step : 541, train Loss : 0.0074\n",
            "Step : 551, train Loss : 0.0089\n",
            "Step : 561, train Loss : 0.0080\n",
            "Step : 571, train Loss : 0.0074\n",
            "Step : 581, train Loss : 0.0074\n",
            "Step : 591, train Loss : 0.0069\n",
            "Step : 601, train Loss : 0.0063\n",
            "Step : 611, train Loss : 0.0069\n",
            "Step : 621, train Loss : 0.0081\n",
            "Step : 631, train Loss : 0.0059\n",
            "Step : 641, train Loss : 0.0084\n",
            "Step : 651, train Loss : 0.0079\n",
            "Step : 661, train Loss : 0.0061\n",
            "Step : 671, train Loss : 0.0091\n",
            "Step : 681, train Loss : 0.0094\n",
            "Step : 691, train Loss : 0.0070\n",
            "Step : 701, train Loss : 0.0088\n",
            "Step : 711, train Loss : 0.0069\n",
            "Step : 721, train Loss : 0.0080\n",
            "Step : 731, train Loss : 0.0087\n",
            "Step : 741, train Loss : 0.0078\n",
            "Step : 751, train Loss : 0.0088\n",
            "Step : 761, train Loss : 0.0083\n",
            "Step : 771, train Loss : 0.0068\n",
            "Step : 781, train Loss : 0.0080\n",
            "Step : 791, train Loss : 0.0065\n",
            "Step : 801, train Loss : 0.0049\n",
            "Step : 811, train Loss : 0.0106\n",
            "Step : 821, train Loss : 0.0066\n",
            "Step : 831, train Loss : 0.0072\n",
            "Step : 841, train Loss : 0.0063\n",
            "Step : 851, train Loss : 0.0089\n",
            "Step : 861, train Loss : 0.0067\n",
            "Step : 871, train Loss : 0.0076\n",
            "Step : 881, train Loss : 0.0077\n",
            "Step : 891, train Loss : 0.0052\n",
            "Step : 901, train Loss : 0.0056\n",
            "Step : 911, train Loss : 0.0075\n",
            "Step : 921, train Loss : 0.0061\n",
            "Step : 931, train Loss : 0.0063\n",
            "Step : 941, train Loss : 0.0085\n",
            "Step : 951, train Loss : 0.0079\n",
            "Step : 961, train Loss : 0.0072\n",
            "Step : 971, train Loss : 0.0048\n",
            "Step : 981, train Loss : 0.0071\n",
            "Step : 991, train Loss : 0.0068\n",
            "Step : 1001, train Loss : 0.0102\n",
            "Step : 1011, train Loss : 0.0060\n",
            "Step : 1021, train Loss : 0.0080\n",
            "Step : 1031, train Loss : 0.0099\n",
            "Step : 1041, train Loss : 0.0068\n",
            "Step : 1051, train Loss : 0.0080\n",
            "Step : 1061, train Loss : 0.0072\n",
            "Step : 1071, train Loss : 0.0082\n",
            "Step : 1081, train Loss : 0.0074\n",
            "Step : 1091, train Loss : 0.0055\n",
            "Step : 1101, train Loss : 0.0081\n",
            "Step : 1111, train Loss : 0.0076\n",
            "Step : 1121, train Loss : 0.0062\n",
            "Step : 1131, train Loss : 0.0075\n",
            "Step : 1141, train Loss : 0.0059\n",
            "Step : 1151, train Loss : 0.0045\n",
            "Step : 1161, train Loss : 0.0083\n",
            "Step : 1171, train Loss : 0.0084\n",
            "Step : 1181, train Loss : 0.0105\n",
            "Step : 1191, train Loss : 0.0067\n",
            "Step : 1201, train Loss : 0.0059\n",
            "Step : 1211, train Loss : 0.0079\n",
            "Step : 1221, train Loss : 0.0051\n",
            "Step : 1231, train Loss : 0.0069\n",
            "Step : 1241, train Loss : 0.0086\n",
            "Step : 1251, train Loss : 0.0080\n",
            "Step : 1261, train Loss : 0.0067\n",
            "Step : 1271, train Loss : 0.0059\n",
            "Step : 1281, train Loss : 0.0063\n",
            "Step : 1291, train Loss : 0.0049\n",
            "Step : 1301, train Loss : 0.0046\n",
            "Step : 1311, train Loss : 0.0068\n",
            "Step : 1321, train Loss : 0.0076\n",
            "Step : 1331, train Loss : 0.0062\n",
            "Step : 1341, train Loss : 0.0057\n",
            "Step : 1351, train Loss : 0.0050\n",
            "Step : 1361, train Loss : 0.0066\n",
            "Step : 1371, train Loss : 0.0052\n",
            "Step : 1381, train Loss : 0.0063\n",
            "Step : 1391, train Loss : 0.0081\n",
            "Step : 1401, train Loss : 0.0070\n",
            "Step : 1411, train Loss : 0.0066\n",
            "Step : 1421, train Loss : 0.0053\n",
            "Step : 1431, train Loss : 0.0068\n",
            "Step : 1441, train Loss : 0.0075\n",
            "Step : 1451, train Loss : 0.0073\n",
            "Step : 1461, train Loss : 0.0065\n",
            "Step : 1471, train Loss : 0.0063\n",
            "Step : 1481, train Loss : 0.0059\n",
            "Step : 1491, train Loss : 0.0112\n",
            "Step : 1501, train Loss : 0.0110\n",
            "Step : 1511, train Loss : 0.0086\n",
            "Step : 1521, train Loss : 0.0059\n",
            "Step : 1531, train Loss : 0.0068\n",
            "Step : 1541, train Loss : 0.0090\n",
            "Step : 1551, train Loss : 0.0074\n",
            "Step : 1561, train Loss : 0.0057\n",
            "Step : 1571, train Loss : 0.0074\n",
            "Step : 1581, train Loss : 0.0044\n",
            "Step : 1591, train Loss : 0.0095\n",
            "Step : 1601, train Loss : 0.0069\n",
            "Step : 1611, train Loss : 0.0066\n",
            "Step : 1621, train Loss : 0.0087\n",
            "Step : 1631, train Loss : 0.0074\n",
            "Step : 1641, train Loss : 0.0081\n",
            "Step : 1651, train Loss : 0.0086\n",
            "Step : 1661, train Loss : 0.0076\n",
            "Step : 1671, train Loss : 0.0061\n",
            "Step : 1681, train Loss : 0.0078\n",
            "Step : 1691, train Loss : 0.0078\n",
            "Step : 1701, train Loss : 0.0072\n",
            "Step : 1711, train Loss : 0.0082\n",
            "Step : 1721, train Loss : 0.0081\n",
            "Step : 1731, train Loss : 0.0052\n",
            "Step : 1741, train Loss : 0.0089\n",
            "Step : 1751, train Loss : 0.0092\n",
            "Step : 1761, train Loss : 0.0082\n",
            "Step : 1771, train Loss : 0.0077\n",
            "Step : 1781, train Loss : 0.0084\n",
            "Step : 1791, train Loss : 0.0065\n",
            "Step : 1801, train Loss : 0.0080\n",
            "Step : 1811, train Loss : 0.0060\n",
            "Step : 1821, train Loss : 0.0062\n",
            "Step : 1831, train Loss : 0.0071\n",
            "Step : 1841, train Loss : 0.0066\n",
            "Step : 1851, train Loss : 0.0051\n",
            "Step : 1861, train Loss : 0.0048\n",
            "Step : 1871, train Loss : 0.0060\n",
            "Step : 1881, train Loss : 0.0094\n",
            "Step : 1891, train Loss : 0.0078\n",
            "Step : 1901, train Loss : 0.0061\n",
            "Step : 1911, train Loss : 0.0078\n",
            "Step : 1921, train Loss : 0.0073\n",
            "Step : 1931, train Loss : 0.0069\n",
            "Step : 1941, train Loss : 0.0071\n",
            "Step : 1951, train Loss : 0.0082\n",
            "Epoch 2 Total Mean Loss : 0.0074\n",
            "*****Epoch 2 Train Finish*****\n",
            "\n",
            "SAVING EPOCH 2 ...\n",
            "*****Epoch 2 Valid Start*****\n",
            "Step: 0,  Pearson: tensor([0.8473])\n",
            "Step: 1,  Pearson: tensor([0.9220])\n",
            "Step: 2,  Pearson: tensor([0.9664])\n",
            "Step: 3,  Pearson: tensor([0.8768])\n",
            "Step: 4,  Pearson: tensor([0.8458])\n",
            "Step: 5,  Pearson: tensor([0.9708])\n",
            "Step: 6,  Pearson: tensor([0.8735])\n",
            "Step: 7,  Pearson: tensor([0.9252])\n",
            "Step: 8,  Pearson: tensor([0.9260])\n",
            "Step: 9,  Pearson: tensor([0.9327])\n",
            "Step: 10,  Pearson: tensor([0.8230])\n",
            "Step: 11,  Pearson: tensor([0.9058])\n",
            "Step: 12,  Pearson: tensor([0.8965])\n",
            "Step: 13,  Pearson: tensor([0.9249])\n",
            "Step: 14,  Pearson: tensor([0.9433])\n",
            "Step: 15,  Pearson: tensor([0.9680])\n",
            "Step: 16,  Pearson: tensor([0.9567])\n",
            "Step: 17,  Pearson: tensor([0.9388])\n",
            "Step: 18,  Pearson: tensor([0.7564])\n",
            "Step: 19,  Pearson: tensor([0.9496])\n",
            "Step: 20,  Pearson: tensor([0.9194])\n",
            "Step: 21,  Pearson: tensor([0.9282])\n",
            "Step: 22,  Pearson: tensor([0.8922])\n",
            "Step: 23,  Pearson: tensor([0.8134])\n",
            "Step: 24,  Pearson: tensor([0.8741])\n",
            "Step: 25,  Pearson: tensor([0.9112])\n",
            "Step: 26,  Pearson: tensor([0.9246])\n",
            "Step: 27,  Pearson: tensor([0.9117])\n",
            "Step: 28,  Pearson: tensor([0.9609])\n",
            "Step: 29,  Pearson: tensor([0.8516])\n",
            "Step: 30,  Pearson: tensor([0.8903])\n",
            "Step: 31,  Pearson: tensor([0.9693])\n",
            "Step: 32,  Pearson: tensor([0.9507])\n",
            "Step: 33,  Pearson: tensor([0.8387])\n",
            "Step: 34,  Pearson: tensor([0.9810])\n",
            "Step: 35,  Pearson: tensor([0.9331])\n",
            "Step: 36,  Pearson: tensor([0.8932])\n",
            "Step: 37,  Pearson: tensor([0.8357])\n",
            "Step: 38,  Pearson: tensor([0.8799])\n",
            "Step: 39,  Pearson: tensor([0.9262])\n",
            "Step: 40,  Pearson: tensor([0.9234])\n",
            "Step: 41,  Pearson: tensor([0.9626])\n",
            "Step: 42,  Pearson: tensor([0.8821])\n",
            "Step: 43,  Pearson: tensor([0.8591])\n",
            "Step: 44,  Pearson: tensor([0.9816])\n",
            "Step: 45,  Pearson: tensor([0.9038])\n",
            "Step: 46,  Pearson: tensor([0.9022])\n",
            "Step: 47,  Pearson: tensor([0.9505])\n",
            "Step: 48,  Pearson: tensor([0.8894])\n",
            "Step: 49,  Pearson: tensor([0.9257])\n",
            "Step: 50,  Pearson: tensor([0.9265])\n",
            "Step: 51,  Pearson: tensor([0.9564])\n",
            "Step: 52,  Pearson: tensor([0.9219])\n",
            "Step: 53,  Pearson: tensor([0.8610])\n",
            "Step: 54,  Pearson: tensor([0.9136])\n",
            "Step: 55,  Pearson: tensor([0.9315])\n",
            "Step: 56,  Pearson: tensor([0.8463])\n",
            "Step: 57,  Pearson: tensor([0.9671])\n",
            "Step: 58,  Pearson: tensor([0.9404])\n",
            "Step: 59,  Pearson: tensor([0.8795])\n",
            "Step: 60,  Pearson: tensor([0.9655])\n",
            "Step: 61,  Pearson: tensor([0.9682])\n",
            "Step: 62,  Pearson: tensor([0.8357])\n",
            "Step: 63,  Pearson: tensor([0.9510])\n",
            "Step: 64,  Pearson: tensor([0.9172])\n",
            "Step: 65,  Pearson: tensor([0.9631])\n",
            "Step: 66,  Pearson: tensor([0.9316])\n",
            "Step: 67,  Pearson: tensor([0.9668])\n",
            "Step: 68,  Pearson: tensor([0.9776])\n",
            "Step: 69,  Pearson: tensor([0.8871])\n",
            "Step: 70,  Pearson: tensor([0.9005])\n",
            "Step: 71,  Pearson: tensor([0.9376])\n",
            "Step: 72,  Pearson: tensor([0.9260])\n",
            "Step: 73,  Pearson: tensor([0.8604])\n",
            "Step: 74,  Pearson: tensor([0.8752])\n",
            "Step: 75,  Pearson: tensor([0.9731])\n",
            "Step: 76,  Pearson: tensor([0.9651])\n",
            "Step: 77,  Pearson: tensor([0.9258])\n",
            "Step: 78,  Pearson: tensor([0.7268])\n",
            "Step: 79,  Pearson: tensor([0.9490])\n",
            "Step: 80,  Pearson: tensor([0.8572])\n",
            "Step: 81,  Pearson: tensor([0.9300])\n",
            "Step: 82,  Pearson: tensor([0.9352])\n",
            "Step: 83,  Pearson: tensor([0.8757])\n",
            "Step: 84,  Pearson: tensor([0.9527])\n",
            "Step: 85,  Pearson: tensor([0.8938])\n",
            "Step: 86,  Pearson: tensor([0.8283])\n",
            "Step: 87,  Pearson: tensor([0.9670])\n",
            "Step: 88,  Pearson: tensor([0.9013])\n",
            "Step: 89,  Pearson: tensor([0.8444])\n",
            "Step: 90,  Pearson: tensor([0.9488])\n",
            "Step: 91,  Pearson: tensor([0.9610])\n",
            "Step: 92,  Pearson: tensor([0.9342])\n",
            "Step: 93,  Pearson: tensor([0.9615])\n",
            "Step: 94,  Pearson: tensor([0.9572])\n",
            "Step: 95,  Pearson: tensor([0.9109])\n",
            "Step: 96,  Pearson: tensor([0.9082])\n",
            "Step: 97,  Pearson: tensor([0.9510])\n",
            "Step: 98,  Pearson: tensor([0.8682])\n",
            "Step: 99,  Pearson: tensor([0.9545])\n",
            "Step: 100,  Pearson: tensor([0.9192])\n",
            "Step: 101,  Pearson: tensor([0.9373])\n",
            "Step: 102,  Pearson: tensor([0.8947])\n",
            "Step: 103,  Pearson: tensor([0.9311])\n",
            "Step: 104,  Pearson: tensor([0.9423])\n",
            "Step: 105,  Pearson: tensor([0.8791])\n",
            "Step: 106,  Pearson: tensor([0.8883])\n",
            "Step: 107,  Pearson: tensor([0.8923])\n",
            "Step: 108,  Pearson: tensor([0.9845])\n",
            "total_valid_loss :  0.5655031455766171 total_f1_score :  0.9021134593993326 total_pearsonr : tensor([0.9136])\n",
            "Epoch 2 Valid Loss : 0.5655031455766171 Valid Pearsonr : tensor([0.9136]) ValidF1 : 0.9021134593993326\n",
            "*****Epoch 2 Valid Finish*****\n",
            "\n",
            "****** STARTING TO TRAIN EPOCH #3 ******\n",
            "Step : 11, train Loss : 0.0038\n",
            "Step : 21, train Loss : 0.0050\n",
            "Step : 31, train Loss : 0.0047\n",
            "Step : 41, train Loss : 0.0037\n",
            "Step : 51, train Loss : 0.0043\n",
            "Step : 61, train Loss : 0.0051\n",
            "Step : 71, train Loss : 0.0048\n",
            "Step : 81, train Loss : 0.0049\n",
            "Step : 91, train Loss : 0.0039\n",
            "Step : 101, train Loss : 0.0048\n",
            "Step : 111, train Loss : 0.0048\n",
            "Step : 121, train Loss : 0.0044\n",
            "Step : 131, train Loss : 0.0064\n",
            "Step : 141, train Loss : 0.0047\n",
            "Step : 151, train Loss : 0.0042\n",
            "Step : 161, train Loss : 0.0040\n",
            "Step : 171, train Loss : 0.0043\n",
            "Step : 181, train Loss : 0.0047\n",
            "Step : 191, train Loss : 0.0047\n",
            "Step : 201, train Loss : 0.0043\n",
            "Step : 211, train Loss : 0.0069\n",
            "Step : 221, train Loss : 0.0050\n",
            "Step : 231, train Loss : 0.0035\n",
            "Step : 241, train Loss : 0.0051\n",
            "Step : 251, train Loss : 0.0039\n",
            "Step : 261, train Loss : 0.0044\n",
            "Step : 271, train Loss : 0.0038\n",
            "Step : 281, train Loss : 0.0051\n",
            "Step : 291, train Loss : 0.0044\n",
            "Step : 301, train Loss : 0.0044\n",
            "Step : 311, train Loss : 0.0031\n",
            "Step : 321, train Loss : 0.0043\n",
            "Step : 331, train Loss : 0.0041\n",
            "Step : 341, train Loss : 0.0057\n",
            "Step : 351, train Loss : 0.0042\n",
            "Step : 361, train Loss : 0.0039\n",
            "Step : 371, train Loss : 0.0034\n",
            "Step : 381, train Loss : 0.0040\n",
            "Step : 391, train Loss : 0.0053\n",
            "Step : 401, train Loss : 0.0040\n",
            "Step : 411, train Loss : 0.0035\n",
            "Step : 421, train Loss : 0.0047\n",
            "Step : 431, train Loss : 0.0046\n",
            "Step : 441, train Loss : 0.0041\n",
            "Step : 451, train Loss : 0.0046\n",
            "Step : 461, train Loss : 0.0031\n",
            "Step : 471, train Loss : 0.0037\n",
            "Step : 481, train Loss : 0.0058\n",
            "Step : 491, train Loss : 0.0038\n",
            "Step : 501, train Loss : 0.0041\n",
            "Step : 511, train Loss : 0.0047\n",
            "Step : 521, train Loss : 0.0055\n",
            "Step : 531, train Loss : 0.0057\n",
            "Step : 541, train Loss : 0.0048\n",
            "Step : 551, train Loss : 0.0043\n",
            "Step : 561, train Loss : 0.0032\n",
            "Step : 571, train Loss : 0.0040\n",
            "Step : 581, train Loss : 0.0041\n",
            "Step : 591, train Loss : 0.0062\n",
            "Step : 601, train Loss : 0.0045\n",
            "Step : 611, train Loss : 0.0041\n",
            "Step : 621, train Loss : 0.0036\n",
            "Step : 631, train Loss : 0.0030\n",
            "Step : 641, train Loss : 0.0059\n",
            "Step : 651, train Loss : 0.0047\n",
            "Step : 661, train Loss : 0.0041\n",
            "Step : 671, train Loss : 0.0039\n",
            "Step : 681, train Loss : 0.0035\n",
            "Step : 691, train Loss : 0.0040\n",
            "Step : 701, train Loss : 0.0043\n",
            "Step : 711, train Loss : 0.0042\n",
            "Step : 721, train Loss : 0.0049\n",
            "Step : 731, train Loss : 0.0045\n",
            "Step : 741, train Loss : 0.0044\n",
            "Step : 751, train Loss : 0.0051\n",
            "Step : 761, train Loss : 0.0047\n",
            "Step : 771, train Loss : 0.0041\n",
            "Step : 781, train Loss : 0.0057\n",
            "Step : 791, train Loss : 0.0041\n",
            "Step : 801, train Loss : 0.0028\n",
            "Step : 811, train Loss : 0.0036\n",
            "Step : 821, train Loss : 0.0042\n",
            "Step : 831, train Loss : 0.0039\n",
            "Step : 841, train Loss : 0.0042\n",
            "Step : 851, train Loss : 0.0043\n",
            "Step : 861, train Loss : 0.0045\n",
            "Step : 871, train Loss : 0.0038\n",
            "Step : 881, train Loss : 0.0054\n",
            "Step : 891, train Loss : 0.0043\n",
            "Step : 901, train Loss : 0.0045\n",
            "Step : 911, train Loss : 0.0043\n",
            "Step : 921, train Loss : 0.0042\n",
            "Step : 931, train Loss : 0.0032\n",
            "Step : 941, train Loss : 0.0048\n",
            "Step : 951, train Loss : 0.0043\n",
            "Step : 961, train Loss : 0.0049\n",
            "Step : 971, train Loss : 0.0043\n",
            "Step : 981, train Loss : 0.0036\n",
            "Step : 991, train Loss : 0.0053\n",
            "Step : 1001, train Loss : 0.0051\n",
            "Step : 1011, train Loss : 0.0052\n",
            "Step : 1021, train Loss : 0.0048\n",
            "Step : 1031, train Loss : 0.0054\n",
            "Step : 1041, train Loss : 0.0034\n",
            "Step : 1051, train Loss : 0.0054\n",
            "Step : 1061, train Loss : 0.0049\n",
            "Step : 1071, train Loss : 0.0038\n",
            "Step : 1081, train Loss : 0.0055\n",
            "Step : 1091, train Loss : 0.0047\n",
            "Step : 1101, train Loss : 0.0044\n",
            "Step : 1111, train Loss : 0.0052\n",
            "Step : 1121, train Loss : 0.0040\n",
            "Step : 1131, train Loss : 0.0032\n",
            "Step : 1141, train Loss : 0.0041\n",
            "Step : 1151, train Loss : 0.0038\n",
            "Step : 1161, train Loss : 0.0035\n",
            "Step : 1171, train Loss : 0.0040\n",
            "Step : 1181, train Loss : 0.0040\n",
            "Step : 1191, train Loss : 0.0054\n",
            "Step : 1201, train Loss : 0.0060\n",
            "Step : 1211, train Loss : 0.0040\n",
            "Step : 1221, train Loss : 0.0042\n",
            "Step : 1231, train Loss : 0.0060\n",
            "Step : 1241, train Loss : 0.0034\n",
            "Step : 1251, train Loss : 0.0053\n",
            "Step : 1261, train Loss : 0.0039\n",
            "Step : 1271, train Loss : 0.0038\n",
            "Step : 1281, train Loss : 0.0049\n",
            "Step : 1291, train Loss : 0.0044\n",
            "Step : 1301, train Loss : 0.0043\n",
            "Step : 1311, train Loss : 0.0042\n",
            "Step : 1321, train Loss : 0.0038\n",
            "Step : 1331, train Loss : 0.0048\n",
            "Step : 1341, train Loss : 0.0050\n",
            "Step : 1351, train Loss : 0.0039\n",
            "Step : 1361, train Loss : 0.0039\n",
            "Step : 1371, train Loss : 0.0048\n",
            "Step : 1381, train Loss : 0.0044\n",
            "Step : 1391, train Loss : 0.0033\n",
            "Step : 1401, train Loss : 0.0061\n",
            "Step : 1411, train Loss : 0.0042\n",
            "Step : 1421, train Loss : 0.0030\n",
            "Step : 1431, train Loss : 0.0040\n",
            "Step : 1441, train Loss : 0.0026\n",
            "Step : 1451, train Loss : 0.0047\n",
            "Step : 1461, train Loss : 0.0035\n",
            "Step : 1471, train Loss : 0.0035\n",
            "Step : 1481, train Loss : 0.0026\n",
            "Step : 1491, train Loss : 0.0046\n",
            "Step : 1501, train Loss : 0.0041\n",
            "Step : 1511, train Loss : 0.0041\n",
            "Step : 1521, train Loss : 0.0033\n",
            "Step : 1531, train Loss : 0.0043\n",
            "Step : 1541, train Loss : 0.0042\n",
            "Step : 1551, train Loss : 0.0046\n",
            "Step : 1561, train Loss : 0.0038\n",
            "Step : 1571, train Loss : 0.0043\n",
            "Step : 1581, train Loss : 0.0033\n",
            "Step : 1591, train Loss : 0.0041\n",
            "Step : 1601, train Loss : 0.0045\n",
            "Step : 1611, train Loss : 0.0041\n",
            "Step : 1621, train Loss : 0.0058\n",
            "Step : 1631, train Loss : 0.0038\n",
            "Step : 1641, train Loss : 0.0043\n",
            "Step : 1651, train Loss : 0.0025\n",
            "Step : 1661, train Loss : 0.0041\n",
            "Step : 1671, train Loss : 0.0044\n",
            "Step : 1681, train Loss : 0.0029\n",
            "Step : 1691, train Loss : 0.0044\n",
            "Step : 1701, train Loss : 0.0048\n",
            "Step : 1711, train Loss : 0.0040\n",
            "Step : 1721, train Loss : 0.0041\n",
            "Step : 1731, train Loss : 0.0042\n",
            "Step : 1741, train Loss : 0.0058\n",
            "Step : 1751, train Loss : 0.0044\n",
            "Step : 1761, train Loss : 0.0038\n",
            "Step : 1771, train Loss : 0.0031\n",
            "Step : 1781, train Loss : 0.0045\n",
            "Step : 1791, train Loss : 0.0037\n",
            "Step : 1801, train Loss : 0.0030\n",
            "Step : 1811, train Loss : 0.0036\n",
            "Step : 1821, train Loss : 0.0034\n",
            "Step : 1831, train Loss : 0.0041\n",
            "Step : 1841, train Loss : 0.0048\n",
            "Step : 1851, train Loss : 0.0038\n",
            "Step : 1861, train Loss : 0.0033\n",
            "Step : 1871, train Loss : 0.0037\n",
            "Step : 1881, train Loss : 0.0041\n",
            "Step : 1891, train Loss : 0.0039\n",
            "Step : 1901, train Loss : 0.0041\n",
            "Step : 1911, train Loss : 0.0042\n",
            "Step : 1921, train Loss : 0.0042\n",
            "Step : 1931, train Loss : 0.0041\n",
            "Step : 1941, train Loss : 0.0058\n",
            "Step : 1951, train Loss : 0.0042\n",
            "Epoch 3 Total Mean Loss : 0.0043\n",
            "*****Epoch 3 Train Finish*****\n",
            "\n",
            "SAVING EPOCH 3 ...\n",
            "*****Epoch 3 Valid Start*****\n",
            "Step: 0,  Pearson: tensor([0.8633])\n",
            "Step: 1,  Pearson: tensor([0.9179])\n",
            "Step: 2,  Pearson: tensor([0.9771])\n",
            "Step: 3,  Pearson: tensor([0.8938])\n",
            "Step: 4,  Pearson: tensor([0.8607])\n",
            "Step: 5,  Pearson: tensor([0.9767])\n",
            "Step: 6,  Pearson: tensor([0.8713])\n",
            "Step: 7,  Pearson: tensor([0.9265])\n",
            "Step: 8,  Pearson: tensor([0.9427])\n",
            "Step: 9,  Pearson: tensor([0.9183])\n",
            "Step: 10,  Pearson: tensor([0.8328])\n",
            "Step: 11,  Pearson: tensor([0.9418])\n",
            "Step: 12,  Pearson: tensor([0.8787])\n",
            "Step: 13,  Pearson: tensor([0.9196])\n",
            "Step: 14,  Pearson: tensor([0.9483])\n",
            "Step: 15,  Pearson: tensor([0.9699])\n",
            "Step: 16,  Pearson: tensor([0.9676])\n",
            "Step: 17,  Pearson: tensor([0.9480])\n",
            "Step: 18,  Pearson: tensor([0.7589])\n",
            "Step: 19,  Pearson: tensor([0.9491])\n",
            "Step: 20,  Pearson: tensor([0.9134])\n",
            "Step: 21,  Pearson: tensor([0.9346])\n",
            "Step: 22,  Pearson: tensor([0.9147])\n",
            "Step: 23,  Pearson: tensor([0.8069])\n",
            "Step: 24,  Pearson: tensor([0.8863])\n",
            "Step: 25,  Pearson: tensor([0.9064])\n",
            "Step: 26,  Pearson: tensor([0.9167])\n",
            "Step: 27,  Pearson: tensor([0.9438])\n",
            "Step: 28,  Pearson: tensor([0.9605])\n",
            "Step: 29,  Pearson: tensor([0.8588])\n",
            "Step: 30,  Pearson: tensor([0.9145])\n",
            "Step: 31,  Pearson: tensor([0.9680])\n",
            "Step: 32,  Pearson: tensor([0.9427])\n",
            "Step: 33,  Pearson: tensor([0.8221])\n",
            "Step: 34,  Pearson: tensor([0.9771])\n",
            "Step: 35,  Pearson: tensor([0.9402])\n",
            "Step: 36,  Pearson: tensor([0.8941])\n",
            "Step: 37,  Pearson: tensor([0.8401])\n",
            "Step: 38,  Pearson: tensor([0.8768])\n",
            "Step: 39,  Pearson: tensor([0.9210])\n",
            "Step: 40,  Pearson: tensor([0.9401])\n",
            "Step: 41,  Pearson: tensor([0.9581])\n",
            "Step: 42,  Pearson: tensor([0.8982])\n",
            "Step: 43,  Pearson: tensor([0.8515])\n",
            "Step: 44,  Pearson: tensor([0.9795])\n",
            "Step: 45,  Pearson: tensor([0.9087])\n",
            "Step: 46,  Pearson: tensor([0.8962])\n",
            "Step: 47,  Pearson: tensor([0.9598])\n",
            "Step: 48,  Pearson: tensor([0.8697])\n",
            "Step: 49,  Pearson: tensor([0.9287])\n",
            "Step: 50,  Pearson: tensor([0.9319])\n",
            "Step: 51,  Pearson: tensor([0.9642])\n",
            "Step: 52,  Pearson: tensor([0.9212])\n",
            "Step: 53,  Pearson: tensor([0.8873])\n",
            "Step: 54,  Pearson: tensor([0.9138])\n",
            "Step: 55,  Pearson: tensor([0.9233])\n",
            "Step: 56,  Pearson: tensor([0.8635])\n",
            "Step: 57,  Pearson: tensor([0.9701])\n",
            "Step: 58,  Pearson: tensor([0.9456])\n",
            "Step: 59,  Pearson: tensor([0.8948])\n",
            "Step: 60,  Pearson: tensor([0.9741])\n",
            "Step: 61,  Pearson: tensor([0.9820])\n",
            "Step: 62,  Pearson: tensor([0.8305])\n",
            "Step: 63,  Pearson: tensor([0.9508])\n",
            "Step: 64,  Pearson: tensor([0.9343])\n",
            "Step: 65,  Pearson: tensor([0.9637])\n",
            "Step: 66,  Pearson: tensor([0.9287])\n",
            "Step: 67,  Pearson: tensor([0.9663])\n",
            "Step: 68,  Pearson: tensor([0.9602])\n",
            "Step: 69,  Pearson: tensor([0.8923])\n",
            "Step: 70,  Pearson: tensor([0.9160])\n",
            "Step: 71,  Pearson: tensor([0.9240])\n",
            "Step: 72,  Pearson: tensor([0.9256])\n",
            "Step: 73,  Pearson: tensor([0.8613])\n",
            "Step: 74,  Pearson: tensor([0.8969])\n",
            "Step: 75,  Pearson: tensor([0.9714])\n",
            "Step: 76,  Pearson: tensor([0.9626])\n",
            "Step: 77,  Pearson: tensor([0.9361])\n",
            "Step: 78,  Pearson: tensor([0.7373])\n",
            "Step: 79,  Pearson: tensor([0.9507])\n",
            "Step: 80,  Pearson: tensor([0.8824])\n",
            "Step: 81,  Pearson: tensor([0.9310])\n",
            "Step: 82,  Pearson: tensor([0.9299])\n",
            "Step: 83,  Pearson: tensor([0.8711])\n",
            "Step: 84,  Pearson: tensor([0.9627])\n",
            "Step: 85,  Pearson: tensor([0.9054])\n",
            "Step: 86,  Pearson: tensor([0.8052])\n",
            "Step: 87,  Pearson: tensor([0.9680])\n",
            "Step: 88,  Pearson: tensor([0.8948])\n",
            "Step: 89,  Pearson: tensor([0.8521])\n",
            "Step: 90,  Pearson: tensor([0.9435])\n",
            "Step: 91,  Pearson: tensor([0.9621])\n",
            "Step: 92,  Pearson: tensor([0.9429])\n",
            "Step: 93,  Pearson: tensor([0.9655])\n",
            "Step: 94,  Pearson: tensor([0.9432])\n",
            "Step: 95,  Pearson: tensor([0.9019])\n",
            "Step: 96,  Pearson: tensor([0.8920])\n",
            "Step: 97,  Pearson: tensor([0.9480])\n",
            "Step: 98,  Pearson: tensor([0.8686])\n",
            "Step: 99,  Pearson: tensor([0.9578])\n",
            "Step: 100,  Pearson: tensor([0.9207])\n",
            "Step: 101,  Pearson: tensor([0.9193])\n",
            "Step: 102,  Pearson: tensor([0.9032])\n",
            "Step: 103,  Pearson: tensor([0.9460])\n",
            "Step: 104,  Pearson: tensor([0.9241])\n",
            "Step: 105,  Pearson: tensor([0.8979])\n",
            "Step: 106,  Pearson: tensor([0.8883])\n",
            "Step: 107,  Pearson: tensor([0.9177])\n",
            "Step: 108,  Pearson: tensor([0.9881])\n",
            "total_valid_loss :  0.5497265773902246 total_f1_score :  0.9053452115812918 total_pearsonr : tensor([0.9165])\n",
            "Epoch 3 Valid Loss : 0.5497265773902246 Valid Pearsonr : tensor([0.9165]) ValidF1 : 0.9053452115812918\n",
            "*****Epoch 3 Valid Finish*****\n",
            "\n",
            "** Train Completed! **\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4128eb1524724093a7add0d9c0dd975f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▆█</td></tr><tr><td>total_f1_score </td><td>▁▃▆█</td></tr><tr><td>total_pearsonr</td><td>▁▃▆█</td></tr><tr><td>total_train_loss</td><td>█▄▂▁</td></tr><tr><td>total_train_lr</td><td>█▆▃▁</td></tr><tr><td>total_valid_loss</td><td>█▆▂▁</td></tr><tr><td>train_loss</td><td>▆▄▇▅█▇▄▆▇▅▃▄▃▃▃▄▃▃▃▃▂▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_lr</td><td>███▇▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>4</td></tr><tr><td>total_f1_score </td><td>0.90535</td></tr><tr><td>total_pearsonr</td><td>0.91647</td></tr><tr><td>total_train_loss</td><td>0.00431</td></tr><tr><td>total_train_lr</td><td>0.0</td></tr><tr><td>total_valid_loss</td><td>0.54973</td></tr><tr><td>train_loss</td><td>0.00416</td></tr><tr><td>train_lr</td><td>0.0</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">valiant-sweep-2</strong>: <a href=\"https://wandb.ai/kdb/sts_v2/runs/ydod97j4\" target=\"_blank\">https://wandb.ai/kdb/sts_v2/runs/ydod97j4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20220529_152429-ydod97j4/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: hq2qnjn2 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \teps: 1e-08\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_batch_size: 8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tvalid_batch_size: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twarm_up_ratio: 0.1\n",
            "Some weights of the model checkpoint at klue/roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.17"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/NLP/wandb/run-20220529_165737-hq2qnjn2</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/kdb/sts_v2/runs/hq2qnjn2\" target=\"_blank\">fragrant-sweep-3</a></strong> to <a href=\"https://wandb.ai/kdb/sts_v2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/kdb/sts_v2/sweeps/sy73kc4x\" target=\"_blank\">https://wandb.ai/kdb/sts_v2/sweeps/sy73kc4x</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "****** STARTING TO TRAIN EPOCH #0 ******\n",
            "Step : 11, train Loss : 0.1946\n",
            "Step : 21, train Loss : 0.2356\n",
            "Step : 31, train Loss : 0.1915\n",
            "Step : 41, train Loss : 0.2071\n",
            "Step : 51, train Loss : 0.1777\n",
            "Step : 61, train Loss : 0.1913\n",
            "Step : 71, train Loss : 0.1877\n",
            "Step : 81, train Loss : 0.1372\n",
            "Step : 91, train Loss : 0.1491\n",
            "Step : 101, train Loss : 0.1367\n",
            "Step : 111, train Loss : 0.1016\n",
            "Step : 121, train Loss : 0.0832\n",
            "Step : 131, train Loss : 0.1073\n",
            "Step : 141, train Loss : 0.0757\n",
            "Step : 151, train Loss : 0.0866\n",
            "Step : 161, train Loss : 0.0704\n",
            "Step : 171, train Loss : 0.0477\n",
            "Step : 181, train Loss : 0.0488\n",
            "Step : 191, train Loss : 0.0493\n",
            "Step : 201, train Loss : 0.0549\n",
            "Step : 211, train Loss : 0.0525\n",
            "Step : 221, train Loss : 0.0493\n",
            "Step : 231, train Loss : 0.0422\n",
            "Step : 241, train Loss : 0.0539\n",
            "Step : 251, train Loss : 0.0453\n",
            "Step : 261, train Loss : 0.0306\n",
            "Step : 271, train Loss : 0.0330\n",
            "Step : 281, train Loss : 0.0356\n",
            "Step : 291, train Loss : 0.0322\n",
            "Step : 301, train Loss : 0.0403\n",
            "Step : 311, train Loss : 0.0251\n",
            "Step : 321, train Loss : 0.0254\n",
            "Step : 331, train Loss : 0.0407\n",
            "Step : 341, train Loss : 0.0249\n",
            "Step : 351, train Loss : 0.0312\n",
            "Step : 361, train Loss : 0.0444\n",
            "Step : 371, train Loss : 0.0206\n",
            "Step : 381, train Loss : 0.0299\n",
            "Step : 391, train Loss : 0.0239\n",
            "Step : 401, train Loss : 0.0223\n",
            "Step : 411, train Loss : 0.0223\n",
            "Step : 421, train Loss : 0.0303\n",
            "Step : 431, train Loss : 0.0242\n",
            "Step : 441, train Loss : 0.0238\n",
            "Step : 451, train Loss : 0.0308\n",
            "Step : 461, train Loss : 0.0275\n",
            "Step : 471, train Loss : 0.0292\n",
            "Step : 481, train Loss : 0.0213\n",
            "Step : 491, train Loss : 0.0308\n",
            "Step : 501, train Loss : 0.0210\n",
            "Step : 511, train Loss : 0.0227\n",
            "Step : 521, train Loss : 0.0171\n",
            "Step : 531, train Loss : 0.0239\n",
            "Step : 541, train Loss : 0.0202\n",
            "Step : 551, train Loss : 0.0232\n",
            "Step : 561, train Loss : 0.0246\n",
            "Step : 571, train Loss : 0.0189\n",
            "Step : 581, train Loss : 0.0236\n",
            "Step : 591, train Loss : 0.0235\n",
            "Step : 601, train Loss : 0.0358\n",
            "Step : 611, train Loss : 0.0174\n",
            "Step : 621, train Loss : 0.0157\n",
            "Step : 631, train Loss : 0.0174\n",
            "Step : 641, train Loss : 0.0228\n",
            "Step : 651, train Loss : 0.0142\n",
            "Step : 661, train Loss : 0.0229\n",
            "Step : 671, train Loss : 0.0227\n",
            "Step : 681, train Loss : 0.0212\n",
            "Step : 691, train Loss : 0.0250\n",
            "Step : 701, train Loss : 0.0216\n",
            "Step : 711, train Loss : 0.0319\n",
            "Step : 721, train Loss : 0.0229\n",
            "Step : 731, train Loss : 0.0320\n",
            "Step : 741, train Loss : 0.0261\n",
            "Step : 751, train Loss : 0.0241\n",
            "Step : 761, train Loss : 0.0212\n",
            "Step : 771, train Loss : 0.0237\n",
            "Step : 781, train Loss : 0.0168\n",
            "Step : 791, train Loss : 0.0154\n",
            "Step : 801, train Loss : 0.0176\n",
            "Step : 811, train Loss : 0.0237\n",
            "Step : 821, train Loss : 0.0169\n",
            "Step : 831, train Loss : 0.0228\n",
            "Step : 841, train Loss : 0.0212\n",
            "Step : 851, train Loss : 0.0181\n",
            "Step : 861, train Loss : 0.0257\n",
            "Step : 871, train Loss : 0.0183\n",
            "Step : 881, train Loss : 0.0230\n",
            "Step : 891, train Loss : 0.0218\n",
            "Step : 901, train Loss : 0.0248\n",
            "Step : 911, train Loss : 0.0193\n",
            "Step : 921, train Loss : 0.0292\n",
            "Step : 931, train Loss : 0.0205\n",
            "Step : 941, train Loss : 0.0191\n",
            "Step : 951, train Loss : 0.0279\n",
            "Step : 961, train Loss : 0.0273\n",
            "Step : 971, train Loss : 0.0192\n",
            "Step : 981, train Loss : 0.0201\n",
            "Step : 991, train Loss : 0.0174\n",
            "Step : 1001, train Loss : 0.0169\n",
            "Step : 1011, train Loss : 0.0203\n",
            "Step : 1021, train Loss : 0.0238\n",
            "Step : 1031, train Loss : 0.0162\n",
            "Step : 1041, train Loss : 0.0200\n",
            "Step : 1051, train Loss : 0.0200\n",
            "Step : 1061, train Loss : 0.0158\n",
            "Step : 1071, train Loss : 0.0368\n",
            "Step : 1081, train Loss : 0.0210\n",
            "Step : 1091, train Loss : 0.0195\n",
            "Step : 1101, train Loss : 0.0196\n",
            "Step : 1111, train Loss : 0.0180\n",
            "Step : 1121, train Loss : 0.0182\n",
            "Step : 1131, train Loss : 0.0247\n",
            "Step : 1141, train Loss : 0.0140\n",
            "Step : 1151, train Loss : 0.0166\n",
            "Step : 1161, train Loss : 0.0244\n",
            "Step : 1171, train Loss : 0.0251\n",
            "Step : 1181, train Loss : 0.0212\n",
            "Step : 1191, train Loss : 0.0193\n",
            "Step : 1201, train Loss : 0.0245\n",
            "Step : 1211, train Loss : 0.0198\n",
            "Step : 1221, train Loss : 0.0214\n",
            "Step : 1231, train Loss : 0.0264\n",
            "Step : 1241, train Loss : 0.0238\n",
            "Step : 1251, train Loss : 0.0149\n",
            "Step : 1261, train Loss : 0.0145\n",
            "Step : 1271, train Loss : 0.0169\n",
            "Step : 1281, train Loss : 0.0301\n",
            "Step : 1291, train Loss : 0.0194\n",
            "Step : 1301, train Loss : 0.0172\n",
            "Step : 1311, train Loss : 0.0206\n",
            "Step : 1321, train Loss : 0.0178\n",
            "Step : 1331, train Loss : 0.0234\n",
            "Step : 1341, train Loss : 0.0177\n",
            "Step : 1351, train Loss : 0.0140\n",
            "Step : 1361, train Loss : 0.0195\n",
            "Step : 1371, train Loss : 0.0161\n",
            "Step : 1381, train Loss : 0.0217\n",
            "Step : 1391, train Loss : 0.0187\n",
            "Step : 1401, train Loss : 0.0189\n",
            "Step : 1411, train Loss : 0.0224\n",
            "Step : 1421, train Loss : 0.0141\n",
            "Step : 1431, train Loss : 0.0185\n",
            "Step : 1441, train Loss : 0.0134\n",
            "Step : 1451, train Loss : 0.0248\n",
            "Step : 1461, train Loss : 0.0196\n",
            "Step : 1471, train Loss : 0.0167\n",
            "Step : 1481, train Loss : 0.0182\n",
            "Step : 1491, train Loss : 0.0145\n",
            "Step : 1501, train Loss : 0.0181\n",
            "Step : 1511, train Loss : 0.0205\n",
            "Step : 1521, train Loss : 0.0319\n",
            "Step : 1531, train Loss : 0.0254\n",
            "Step : 1541, train Loss : 0.0274\n",
            "Step : 1551, train Loss : 0.0212\n",
            "Step : 1561, train Loss : 0.0188\n",
            "Step : 1571, train Loss : 0.0186\n",
            "Step : 1581, train Loss : 0.0169\n",
            "Step : 1591, train Loss : 0.0225\n",
            "Step : 1601, train Loss : 0.0155\n",
            "Step : 1611, train Loss : 0.0179\n",
            "Step : 1621, train Loss : 0.0174\n",
            "Step : 1631, train Loss : 0.0170\n",
            "Step : 1641, train Loss : 0.0124\n",
            "Step : 1651, train Loss : 0.0136\n",
            "Step : 1661, train Loss : 0.0201\n",
            "Step : 1671, train Loss : 0.0177\n",
            "Step : 1681, train Loss : 0.0296\n",
            "Step : 1691, train Loss : 0.0168\n",
            "Step : 1701, train Loss : 0.0204\n",
            "Step : 1711, train Loss : 0.0202\n",
            "Step : 1721, train Loss : 0.0199\n",
            "Step : 1731, train Loss : 0.0175\n",
            "Step : 1741, train Loss : 0.0186\n",
            "Step : 1751, train Loss : 0.0167\n",
            "Step : 1761, train Loss : 0.0219\n",
            "Step : 1771, train Loss : 0.0197\n",
            "Step : 1781, train Loss : 0.0177\n",
            "Step : 1791, train Loss : 0.0207\n",
            "Step : 1801, train Loss : 0.0187\n",
            "Step : 1811, train Loss : 0.0174\n",
            "Step : 1821, train Loss : 0.0148\n",
            "Step : 1831, train Loss : 0.0200\n",
            "Step : 1841, train Loss : 0.0231\n",
            "Step : 1851, train Loss : 0.0188\n",
            "Step : 1861, train Loss : 0.0195\n",
            "Step : 1871, train Loss : 0.0265\n",
            "Step : 1881, train Loss : 0.0193\n",
            "Step : 1891, train Loss : 0.0206\n",
            "Step : 1901, train Loss : 0.0263\n",
            "Step : 1911, train Loss : 0.0295\n",
            "Step : 1921, train Loss : 0.0190\n",
            "Step : 1931, train Loss : 0.0220\n",
            "Step : 1941, train Loss : 0.0311\n",
            "Step : 1951, train Loss : 0.0127\n",
            "Epoch 0 Total Mean Loss : 0.0334\n",
            "*****Epoch 0 Train Finish*****\n",
            "\n",
            "SAVING EPOCH 0 ...\n",
            "*****Epoch 0 Valid Start*****\n",
            "Step: 0,  Pearson: tensor([0.8807])\n",
            "Step: 1,  Pearson: tensor([0.9119])\n",
            "Step: 2,  Pearson: tensor([0.9490])\n",
            "Step: 3,  Pearson: tensor([0.8929])\n",
            "Step: 4,  Pearson: tensor([0.9139])\n",
            "Step: 5,  Pearson: tensor([0.9811])\n",
            "Step: 6,  Pearson: tensor([0.9227])\n",
            "Step: 7,  Pearson: tensor([0.9349])\n",
            "Step: 8,  Pearson: tensor([0.9638])\n",
            "Step: 9,  Pearson: tensor([0.9591])\n",
            "Step: 10,  Pearson: tensor([0.8152])\n",
            "Step: 11,  Pearson: tensor([0.8807])\n",
            "Step: 12,  Pearson: tensor([0.8691])\n",
            "Step: 13,  Pearson: tensor([0.9413])\n",
            "Step: 14,  Pearson: tensor([0.9411])\n",
            "Step: 15,  Pearson: tensor([0.9627])\n",
            "Step: 16,  Pearson: tensor([0.9522])\n",
            "Step: 17,  Pearson: tensor([0.9645])\n",
            "Step: 18,  Pearson: tensor([0.7959])\n",
            "Step: 19,  Pearson: tensor([0.9450])\n",
            "Step: 20,  Pearson: tensor([0.9352])\n",
            "Step: 21,  Pearson: tensor([0.9194])\n",
            "Step: 22,  Pearson: tensor([0.9364])\n",
            "Step: 23,  Pearson: tensor([0.7450])\n",
            "Step: 24,  Pearson: tensor([0.9130])\n",
            "Step: 25,  Pearson: tensor([0.9554])\n",
            "Step: 26,  Pearson: tensor([0.8978])\n",
            "Step: 27,  Pearson: tensor([0.9641])\n",
            "Step: 28,  Pearson: tensor([0.9560])\n",
            "Step: 29,  Pearson: tensor([0.8386])\n",
            "Step: 30,  Pearson: tensor([0.9001])\n",
            "Step: 31,  Pearson: tensor([0.9719])\n",
            "Step: 32,  Pearson: tensor([0.8874])\n",
            "Step: 33,  Pearson: tensor([0.8483])\n",
            "Step: 34,  Pearson: tensor([0.9296])\n",
            "Step: 35,  Pearson: tensor([0.9464])\n",
            "Step: 36,  Pearson: tensor([0.8777])\n",
            "Step: 37,  Pearson: tensor([0.9111])\n",
            "Step: 38,  Pearson: tensor([0.8843])\n",
            "Step: 39,  Pearson: tensor([0.9446])\n",
            "Step: 40,  Pearson: tensor([0.9522])\n",
            "Step: 41,  Pearson: tensor([0.9516])\n",
            "Step: 42,  Pearson: tensor([0.8657])\n",
            "Step: 43,  Pearson: tensor([0.8571])\n",
            "Step: 44,  Pearson: tensor([0.9782])\n",
            "Step: 45,  Pearson: tensor([0.9122])\n",
            "Step: 46,  Pearson: tensor([0.8730])\n",
            "Step: 47,  Pearson: tensor([0.9601])\n",
            "Step: 48,  Pearson: tensor([0.8373])\n",
            "Step: 49,  Pearson: tensor([0.9033])\n",
            "Step: 50,  Pearson: tensor([0.9397])\n",
            "Step: 51,  Pearson: tensor([0.9462])\n",
            "Step: 52,  Pearson: tensor([0.9295])\n",
            "Step: 53,  Pearson: tensor([0.9037])\n",
            "Step: 54,  Pearson: tensor([0.9445])\n",
            "Step: 55,  Pearson: tensor([0.9415])\n",
            "Step: 56,  Pearson: tensor([0.8938])\n",
            "Step: 57,  Pearson: tensor([0.9511])\n",
            "Step: 58,  Pearson: tensor([0.9592])\n",
            "Step: 59,  Pearson: tensor([0.9661])\n",
            "Step: 60,  Pearson: tensor([0.9609])\n",
            "Step: 61,  Pearson: tensor([0.9505])\n",
            "Step: 62,  Pearson: tensor([0.8462])\n",
            "Step: 63,  Pearson: tensor([0.9390])\n",
            "Step: 64,  Pearson: tensor([0.9276])\n",
            "Step: 65,  Pearson: tensor([0.9375])\n",
            "Step: 66,  Pearson: tensor([0.9344])\n",
            "Step: 67,  Pearson: tensor([0.9724])\n",
            "Step: 68,  Pearson: tensor([0.9546])\n",
            "Step: 69,  Pearson: tensor([0.9510])\n",
            "Step: 70,  Pearson: tensor([0.8840])\n",
            "Step: 71,  Pearson: tensor([0.8734])\n",
            "Step: 72,  Pearson: tensor([0.9574])\n",
            "Step: 73,  Pearson: tensor([0.8852])\n",
            "Step: 74,  Pearson: tensor([0.9105])\n",
            "Step: 75,  Pearson: tensor([0.9808])\n",
            "Step: 76,  Pearson: tensor([0.9639])\n",
            "Step: 77,  Pearson: tensor([0.9263])\n",
            "Step: 78,  Pearson: tensor([0.7514])\n",
            "Step: 79,  Pearson: tensor([0.9306])\n",
            "Step: 80,  Pearson: tensor([0.9165])\n",
            "Step: 81,  Pearson: tensor([0.9582])\n",
            "Step: 82,  Pearson: tensor([0.9441])\n",
            "Step: 83,  Pearson: tensor([0.9029])\n",
            "Step: 84,  Pearson: tensor([0.9431])\n",
            "Step: 85,  Pearson: tensor([0.9105])\n",
            "Step: 86,  Pearson: tensor([0.9011])\n",
            "Step: 87,  Pearson: tensor([0.9763])\n",
            "Step: 88,  Pearson: tensor([0.9260])\n",
            "Step: 89,  Pearson: tensor([0.8012])\n",
            "Step: 90,  Pearson: tensor([0.9378])\n",
            "Step: 91,  Pearson: tensor([0.9461])\n",
            "Step: 92,  Pearson: tensor([0.9251])\n",
            "Step: 93,  Pearson: tensor([0.9759])\n",
            "Step: 94,  Pearson: tensor([0.8639])\n",
            "Step: 95,  Pearson: tensor([0.9485])\n",
            "Step: 96,  Pearson: tensor([0.8914])\n",
            "Step: 97,  Pearson: tensor([0.9468])\n",
            "Step: 98,  Pearson: tensor([0.8409])\n",
            "Step: 99,  Pearson: tensor([0.9351])\n",
            "Step: 100,  Pearson: tensor([0.9584])\n",
            "Step: 101,  Pearson: tensor([0.9316])\n",
            "Step: 102,  Pearson: tensor([0.9431])\n",
            "Step: 103,  Pearson: tensor([0.9122])\n",
            "Step: 104,  Pearson: tensor([0.8966])\n",
            "Step: 105,  Pearson: tensor([0.9126])\n",
            "Step: 106,  Pearson: tensor([0.9348])\n",
            "Step: 107,  Pearson: tensor([0.9164])\n",
            "Step: 108,  Pearson: tensor([0.9799])\n",
            "total_valid_loss :  0.5421124061039828 total_f1_score :  0.904367053620785 total_pearsonr : tensor([0.9187])\n",
            "Epoch 0 Valid Loss : 0.5421124061039828 Valid Pearsonr : tensor([0.9187]) ValidF1 : 0.904367053620785\n",
            "*****Epoch 0 Valid Finish*****\n",
            "\n",
            "****** STARTING TO TRAIN EPOCH #1 ******\n",
            "Step : 11, train Loss : 0.0148\n",
            "Step : 21, train Loss : 0.0142\n",
            "Step : 31, train Loss : 0.0113\n",
            "Step : 41, train Loss : 0.0108\n",
            "Step : 51, train Loss : 0.0116\n",
            "Step : 61, train Loss : 0.0110\n",
            "Step : 71, train Loss : 0.0129\n",
            "Step : 81, train Loss : 0.0122\n",
            "Step : 91, train Loss : 0.0102\n",
            "Step : 101, train Loss : 0.0136\n",
            "Step : 111, train Loss : 0.0127\n",
            "Step : 121, train Loss : 0.0095\n",
            "Step : 131, train Loss : 0.0087\n",
            "Step : 141, train Loss : 0.0109\n",
            "Step : 151, train Loss : 0.0097\n",
            "Step : 161, train Loss : 0.0139\n",
            "Step : 171, train Loss : 0.0122\n",
            "Step : 181, train Loss : 0.0125\n",
            "Step : 191, train Loss : 0.0097\n",
            "Step : 201, train Loss : 0.0146\n",
            "Step : 211, train Loss : 0.0095\n",
            "Step : 221, train Loss : 0.0144\n",
            "Step : 231, train Loss : 0.0135\n",
            "Step : 241, train Loss : 0.0165\n",
            "Step : 251, train Loss : 0.0088\n",
            "Step : 261, train Loss : 0.0187\n",
            "Step : 271, train Loss : 0.0121\n",
            "Step : 281, train Loss : 0.0129\n",
            "Step : 291, train Loss : 0.0148\n",
            "Step : 301, train Loss : 0.0082\n",
            "Step : 311, train Loss : 0.0137\n",
            "Step : 321, train Loss : 0.0123\n",
            "Step : 331, train Loss : 0.0108\n",
            "Step : 341, train Loss : 0.0119\n",
            "Step : 351, train Loss : 0.0123\n",
            "Step : 361, train Loss : 0.0111\n",
            "Step : 371, train Loss : 0.0122\n",
            "Step : 381, train Loss : 0.0132\n",
            "Step : 391, train Loss : 0.0100\n",
            "Step : 401, train Loss : 0.0117\n",
            "Step : 411, train Loss : 0.0164\n",
            "Step : 421, train Loss : 0.0137\n",
            "Step : 431, train Loss : 0.0104\n",
            "Step : 441, train Loss : 0.0160\n",
            "Step : 451, train Loss : 0.0105\n",
            "Step : 461, train Loss : 0.0087\n",
            "Step : 471, train Loss : 0.0170\n",
            "Step : 481, train Loss : 0.0171\n",
            "Step : 491, train Loss : 0.0103\n",
            "Step : 501, train Loss : 0.0130\n",
            "Step : 511, train Loss : 0.0156\n",
            "Step : 521, train Loss : 0.0113\n",
            "Step : 531, train Loss : 0.0102\n",
            "Step : 541, train Loss : 0.0104\n",
            "Step : 551, train Loss : 0.0134\n",
            "Step : 561, train Loss : 0.0093\n",
            "Step : 571, train Loss : 0.0107\n",
            "Step : 581, train Loss : 0.0093\n",
            "Step : 591, train Loss : 0.0123\n",
            "Step : 601, train Loss : 0.0146\n",
            "Step : 611, train Loss : 0.0142\n",
            "Step : 621, train Loss : 0.0133\n",
            "Step : 631, train Loss : 0.0136\n",
            "Step : 641, train Loss : 0.0169\n",
            "Step : 651, train Loss : 0.0113\n",
            "Step : 661, train Loss : 0.0095\n",
            "Step : 671, train Loss : 0.0091\n",
            "Step : 681, train Loss : 0.0120\n",
            "Step : 691, train Loss : 0.0085\n",
            "Step : 701, train Loss : 0.0112\n",
            "Step : 711, train Loss : 0.0133\n",
            "Step : 721, train Loss : 0.0095\n",
            "Step : 731, train Loss : 0.0107\n",
            "Step : 741, train Loss : 0.0083\n",
            "Step : 751, train Loss : 0.0089\n",
            "Step : 761, train Loss : 0.0100\n",
            "Step : 771, train Loss : 0.0122\n",
            "Step : 781, train Loss : 0.0072\n",
            "Step : 791, train Loss : 0.0111\n",
            "Step : 801, train Loss : 0.0093\n",
            "Step : 811, train Loss : 0.0110\n",
            "Step : 821, train Loss : 0.0154\n",
            "Step : 831, train Loss : 0.0192\n",
            "Step : 841, train Loss : 0.0115\n",
            "Step : 851, train Loss : 0.0133\n",
            "Step : 861, train Loss : 0.0098\n",
            "Step : 871, train Loss : 0.0130\n",
            "Step : 881, train Loss : 0.0090\n",
            "Step : 891, train Loss : 0.0150\n",
            "Step : 901, train Loss : 0.0116\n",
            "Step : 911, train Loss : 0.0122\n",
            "Step : 921, train Loss : 0.0115\n",
            "Step : 931, train Loss : 0.0141\n",
            "Step : 941, train Loss : 0.0143\n",
            "Step : 951, train Loss : 0.0169\n",
            "Step : 961, train Loss : 0.0100\n",
            "Step : 971, train Loss : 0.0170\n",
            "Step : 981, train Loss : 0.0180\n",
            "Step : 991, train Loss : 0.0130\n",
            "Step : 1001, train Loss : 0.0103\n",
            "Step : 1011, train Loss : 0.0168\n",
            "Step : 1021, train Loss : 0.0108\n",
            "Step : 1031, train Loss : 0.0111\n",
            "Step : 1041, train Loss : 0.0138\n",
            "Step : 1051, train Loss : 0.0149\n",
            "Step : 1061, train Loss : 0.0120\n",
            "Step : 1071, train Loss : 0.0107\n",
            "Step : 1081, train Loss : 0.0129\n",
            "Step : 1091, train Loss : 0.0105\n",
            "Step : 1101, train Loss : 0.0066\n",
            "Step : 1111, train Loss : 0.0099\n",
            "Step : 1121, train Loss : 0.0110\n",
            "Step : 1131, train Loss : 0.0104\n",
            "Step : 1141, train Loss : 0.0147\n",
            "Step : 1151, train Loss : 0.0151\n",
            "Step : 1161, train Loss : 0.0089\n",
            "Step : 1171, train Loss : 0.0105\n",
            "Step : 1181, train Loss : 0.0132\n",
            "Step : 1191, train Loss : 0.0111\n",
            "Step : 1201, train Loss : 0.0124\n",
            "Step : 1211, train Loss : 0.0111\n",
            "Step : 1221, train Loss : 0.0133\n",
            "Step : 1231, train Loss : 0.0096\n",
            "Step : 1241, train Loss : 0.0096\n",
            "Step : 1251, train Loss : 0.0091\n",
            "Step : 1261, train Loss : 0.0097\n",
            "Step : 1271, train Loss : 0.0111\n",
            "Step : 1281, train Loss : 0.0069\n",
            "Step : 1291, train Loss : 0.0109\n",
            "Step : 1301, train Loss : 0.0188\n",
            "Step : 1311, train Loss : 0.0092\n",
            "Step : 1321, train Loss : 0.0131\n",
            "Step : 1331, train Loss : 0.0125\n",
            "Step : 1341, train Loss : 0.0119\n",
            "Step : 1351, train Loss : 0.0125\n",
            "Step : 1361, train Loss : 0.0089\n",
            "Step : 1371, train Loss : 0.0106\n",
            "Step : 1381, train Loss : 0.0084\n",
            "Step : 1391, train Loss : 0.0104\n",
            "Step : 1401, train Loss : 0.0088\n",
            "Step : 1411, train Loss : 0.0136\n",
            "Step : 1421, train Loss : 0.0092\n",
            "Step : 1431, train Loss : 0.0181\n",
            "Step : 1441, train Loss : 0.0118\n",
            "Step : 1451, train Loss : 0.0104\n",
            "Step : 1461, train Loss : 0.0119\n",
            "Step : 1471, train Loss : 0.0132\n",
            "Step : 1481, train Loss : 0.0134\n",
            "Step : 1491, train Loss : 0.0107\n",
            "Step : 1501, train Loss : 0.0100\n",
            "Step : 1511, train Loss : 0.0093\n",
            "Step : 1521, train Loss : 0.0104\n",
            "Step : 1531, train Loss : 0.0095\n",
            "Step : 1541, train Loss : 0.0120\n",
            "Step : 1551, train Loss : 0.0126\n",
            "Step : 1561, train Loss : 0.0105\n",
            "Step : 1571, train Loss : 0.0088\n",
            "Step : 1581, train Loss : 0.0138\n",
            "Step : 1591, train Loss : 0.0095\n",
            "Step : 1601, train Loss : 0.0082\n",
            "Step : 1611, train Loss : 0.0091\n",
            "Step : 1621, train Loss : 0.0123\n",
            "Step : 1631, train Loss : 0.0123\n",
            "Step : 1641, train Loss : 0.0095\n",
            "Step : 1651, train Loss : 0.0126\n",
            "Step : 1661, train Loss : 0.0131\n",
            "Step : 1671, train Loss : 0.0098\n",
            "Step : 1681, train Loss : 0.0123\n",
            "Step : 1691, train Loss : 0.0109\n",
            "Step : 1701, train Loss : 0.0148\n",
            "Step : 1711, train Loss : 0.0070\n",
            "Step : 1721, train Loss : 0.0088\n",
            "Step : 1731, train Loss : 0.0100\n",
            "Step : 1741, train Loss : 0.0061\n",
            "Step : 1751, train Loss : 0.0107\n",
            "Step : 1761, train Loss : 0.0185\n",
            "Step : 1771, train Loss : 0.0114\n",
            "Step : 1781, train Loss : 0.0096\n",
            "Step : 1791, train Loss : 0.0101\n",
            "Step : 1801, train Loss : 0.0109\n",
            "Step : 1811, train Loss : 0.0152\n",
            "Step : 1821, train Loss : 0.0121\n",
            "Step : 1831, train Loss : 0.0163\n",
            "Step : 1841, train Loss : 0.0100\n",
            "Step : 1851, train Loss : 0.0078\n",
            "Step : 1861, train Loss : 0.0092\n",
            "Step : 1871, train Loss : 0.0106\n",
            "Step : 1881, train Loss : 0.0106\n",
            "Step : 1891, train Loss : 0.0093\n",
            "Step : 1901, train Loss : 0.0157\n",
            "Step : 1911, train Loss : 0.0120\n",
            "Step : 1921, train Loss : 0.0093\n",
            "Step : 1931, train Loss : 0.0097\n",
            "Step : 1941, train Loss : 0.0134\n",
            "Step : 1951, train Loss : 0.0074\n",
            "Epoch 1 Total Mean Loss : 0.0118\n",
            "*****Epoch 1 Train Finish*****\n",
            "\n",
            "SAVING EPOCH 1 ...\n",
            "*****Epoch 1 Valid Start*****\n",
            "Step: 0,  Pearson: tensor([0.8706])\n",
            "Step: 1,  Pearson: tensor([0.9149])\n",
            "Step: 2,  Pearson: tensor([0.9604])\n",
            "Step: 3,  Pearson: tensor([0.8995])\n",
            "Step: 4,  Pearson: tensor([0.9146])\n",
            "Step: 5,  Pearson: tensor([0.9824])\n",
            "Step: 6,  Pearson: tensor([0.8996])\n",
            "Step: 7,  Pearson: tensor([0.9315])\n",
            "Step: 8,  Pearson: tensor([0.9628])\n",
            "Step: 9,  Pearson: tensor([0.9499])\n",
            "Step: 10,  Pearson: tensor([0.7845])\n",
            "Step: 11,  Pearson: tensor([0.9022])\n",
            "Step: 12,  Pearson: tensor([0.8916])\n",
            "Step: 13,  Pearson: tensor([0.9454])\n",
            "Step: 14,  Pearson: tensor([0.9517])\n",
            "Step: 15,  Pearson: tensor([0.9627])\n",
            "Step: 16,  Pearson: tensor([0.9584])\n",
            "Step: 17,  Pearson: tensor([0.9573])\n",
            "Step: 18,  Pearson: tensor([0.7569])\n",
            "Step: 19,  Pearson: tensor([0.9499])\n",
            "Step: 20,  Pearson: tensor([0.9231])\n",
            "Step: 21,  Pearson: tensor([0.9322])\n",
            "Step: 22,  Pearson: tensor([0.9371])\n",
            "Step: 23,  Pearson: tensor([0.7482])\n",
            "Step: 24,  Pearson: tensor([0.9107])\n",
            "Step: 25,  Pearson: tensor([0.9437])\n",
            "Step: 26,  Pearson: tensor([0.9062])\n",
            "Step: 27,  Pearson: tensor([0.9606])\n",
            "Step: 28,  Pearson: tensor([0.9583])\n",
            "Step: 29,  Pearson: tensor([0.8794])\n",
            "Step: 30,  Pearson: tensor([0.9130])\n",
            "Step: 31,  Pearson: tensor([0.9683])\n",
            "Step: 32,  Pearson: tensor([0.8998])\n",
            "Step: 33,  Pearson: tensor([0.8317])\n",
            "Step: 34,  Pearson: tensor([0.9560])\n",
            "Step: 35,  Pearson: tensor([0.9489])\n",
            "Step: 36,  Pearson: tensor([0.8616])\n",
            "Step: 37,  Pearson: tensor([0.8969])\n",
            "Step: 38,  Pearson: tensor([0.8982])\n",
            "Step: 39,  Pearson: tensor([0.9270])\n",
            "Step: 40,  Pearson: tensor([0.9480])\n",
            "Step: 41,  Pearson: tensor([0.9547])\n",
            "Step: 42,  Pearson: tensor([0.8937])\n",
            "Step: 43,  Pearson: tensor([0.8756])\n",
            "Step: 44,  Pearson: tensor([0.9850])\n",
            "Step: 45,  Pearson: tensor([0.9372])\n",
            "Step: 46,  Pearson: tensor([0.8832])\n",
            "Step: 47,  Pearson: tensor([0.9643])\n",
            "Step: 48,  Pearson: tensor([0.8534])\n",
            "Step: 49,  Pearson: tensor([0.9173])\n",
            "Step: 50,  Pearson: tensor([0.9485])\n",
            "Step: 51,  Pearson: tensor([0.9630])\n",
            "Step: 52,  Pearson: tensor([0.9316])\n",
            "Step: 53,  Pearson: tensor([0.9008])\n",
            "Step: 54,  Pearson: tensor([0.9359])\n",
            "Step: 55,  Pearson: tensor([0.9259])\n",
            "Step: 56,  Pearson: tensor([0.9033])\n",
            "Step: 57,  Pearson: tensor([0.9382])\n",
            "Step: 58,  Pearson: tensor([0.9700])\n",
            "Step: 59,  Pearson: tensor([0.9660])\n",
            "Step: 60,  Pearson: tensor([0.9660])\n",
            "Step: 61,  Pearson: tensor([0.9507])\n",
            "Step: 62,  Pearson: tensor([0.8280])\n",
            "Step: 63,  Pearson: tensor([0.9495])\n",
            "Step: 64,  Pearson: tensor([0.9290])\n",
            "Step: 65,  Pearson: tensor([0.9631])\n",
            "Step: 66,  Pearson: tensor([0.9165])\n",
            "Step: 67,  Pearson: tensor([0.9769])\n",
            "Step: 68,  Pearson: tensor([0.9602])\n",
            "Step: 69,  Pearson: tensor([0.9446])\n",
            "Step: 70,  Pearson: tensor([0.9259])\n",
            "Step: 71,  Pearson: tensor([0.9034])\n",
            "Step: 72,  Pearson: tensor([0.9788])\n",
            "Step: 73,  Pearson: tensor([0.9055])\n",
            "Step: 74,  Pearson: tensor([0.9019])\n",
            "Step: 75,  Pearson: tensor([0.9752])\n",
            "Step: 76,  Pearson: tensor([0.9523])\n",
            "Step: 77,  Pearson: tensor([0.9248])\n",
            "Step: 78,  Pearson: tensor([0.7198])\n",
            "Step: 79,  Pearson: tensor([0.9318])\n",
            "Step: 80,  Pearson: tensor([0.9229])\n",
            "Step: 81,  Pearson: tensor([0.9492])\n",
            "Step: 82,  Pearson: tensor([0.9314])\n",
            "Step: 83,  Pearson: tensor([0.9024])\n",
            "Step: 84,  Pearson: tensor([0.9635])\n",
            "Step: 85,  Pearson: tensor([0.8947])\n",
            "Step: 86,  Pearson: tensor([0.8690])\n",
            "Step: 87,  Pearson: tensor([0.9735])\n",
            "Step: 88,  Pearson: tensor([0.9090])\n",
            "Step: 89,  Pearson: tensor([0.8254])\n",
            "Step: 90,  Pearson: tensor([0.9425])\n",
            "Step: 91,  Pearson: tensor([0.9465])\n",
            "Step: 92,  Pearson: tensor([0.9069])\n",
            "Step: 93,  Pearson: tensor([0.9820])\n",
            "Step: 94,  Pearson: tensor([0.8959])\n",
            "Step: 95,  Pearson: tensor([0.9284])\n",
            "Step: 96,  Pearson: tensor([0.9177])\n",
            "Step: 97,  Pearson: tensor([0.9507])\n",
            "Step: 98,  Pearson: tensor([0.8860])\n",
            "Step: 99,  Pearson: tensor([0.9399])\n",
            "Step: 100,  Pearson: tensor([0.9723])\n",
            "Step: 101,  Pearson: tensor([0.9396])\n",
            "Step: 102,  Pearson: tensor([0.9401])\n",
            "Step: 103,  Pearson: tensor([0.9290])\n",
            "Step: 104,  Pearson: tensor([0.8654])\n",
            "Step: 105,  Pearson: tensor([0.9189])\n",
            "Step: 106,  Pearson: tensor([0.9199])\n",
            "Step: 107,  Pearson: tensor([0.9402])\n",
            "Step: 108,  Pearson: tensor([0.9601])\n",
            "total_valid_loss :  0.5046940899770195 total_f1_score :  0.9081803005008348 total_pearsonr : tensor([0.9219])\n",
            "Epoch 1 Valid Loss : 0.5046940899770195 Valid Pearsonr : tensor([0.9219]) ValidF1 : 0.9081803005008348\n",
            "*****Epoch 1 Valid Finish*****\n",
            "\n",
            "****** STARTING TO TRAIN EPOCH #2 ******\n",
            "Step : 11, train Loss : 0.0061\n",
            "Step : 21, train Loss : 0.0070\n",
            "Step : 31, train Loss : 0.0091\n",
            "Step : 41, train Loss : 0.0049\n",
            "Step : 51, train Loss : 0.0071\n",
            "Step : 61, train Loss : 0.0088\n",
            "Step : 71, train Loss : 0.0078\n",
            "Step : 81, train Loss : 0.0057\n",
            "Step : 91, train Loss : 0.0072\n",
            "Step : 101, train Loss : 0.0088\n",
            "Step : 111, train Loss : 0.0056\n",
            "Step : 121, train Loss : 0.0080\n",
            "Step : 131, train Loss : 0.0055\n",
            "Step : 141, train Loss : 0.0064\n",
            "Step : 151, train Loss : 0.0096\n",
            "Step : 161, train Loss : 0.0076\n",
            "Step : 171, train Loss : 0.0044\n",
            "Step : 181, train Loss : 0.0080\n",
            "Step : 191, train Loss : 0.0068\n",
            "Step : 201, train Loss : 0.0079\n",
            "Step : 211, train Loss : 0.0043\n",
            "Step : 221, train Loss : 0.0099\n",
            "Step : 231, train Loss : 0.0049\n",
            "Step : 241, train Loss : 0.0054\n",
            "Step : 251, train Loss : 0.0064\n",
            "Step : 261, train Loss : 0.0065\n",
            "Step : 271, train Loss : 0.0084\n",
            "Step : 281, train Loss : 0.0073\n",
            "Step : 291, train Loss : 0.0086\n",
            "Step : 301, train Loss : 0.0051\n",
            "Step : 311, train Loss : 0.0066\n",
            "Step : 321, train Loss : 0.0070\n",
            "Step : 331, train Loss : 0.0070\n",
            "Step : 341, train Loss : 0.0062\n",
            "Step : 351, train Loss : 0.0060\n",
            "Step : 361, train Loss : 0.0060\n",
            "Step : 371, train Loss : 0.0071\n",
            "Step : 381, train Loss : 0.0050\n",
            "Step : 391, train Loss : 0.0053\n",
            "Step : 401, train Loss : 0.0074\n",
            "Step : 411, train Loss : 0.0068\n",
            "Step : 421, train Loss : 0.0064\n",
            "Step : 431, train Loss : 0.0059\n",
            "Step : 441, train Loss : 0.0064\n",
            "Step : 451, train Loss : 0.0074\n",
            "Step : 461, train Loss : 0.0064\n",
            "Step : 471, train Loss : 0.0051\n",
            "Step : 481, train Loss : 0.0079\n",
            "Step : 491, train Loss : 0.0109\n",
            "Step : 501, train Loss : 0.0070\n",
            "Step : 511, train Loss : 0.0061\n",
            "Step : 521, train Loss : 0.0066\n",
            "Step : 531, train Loss : 0.0076\n",
            "Step : 541, train Loss : 0.0053\n",
            "Step : 551, train Loss : 0.0050\n",
            "Step : 561, train Loss : 0.0082\n",
            "Step : 571, train Loss : 0.0064\n",
            "Step : 581, train Loss : 0.0058\n",
            "Step : 591, train Loss : 0.0075\n",
            "Step : 601, train Loss : 0.0070\n",
            "Step : 611, train Loss : 0.0055\n",
            "Step : 621, train Loss : 0.0076\n",
            "Step : 631, train Loss : 0.0056\n",
            "Step : 641, train Loss : 0.0056\n",
            "Step : 651, train Loss : 0.0077\n",
            "Step : 661, train Loss : 0.0059\n",
            "Step : 671, train Loss : 0.0057\n",
            "Step : 681, train Loss : 0.0074\n",
            "Step : 691, train Loss : 0.0076\n",
            "Step : 701, train Loss : 0.0072\n",
            "Step : 711, train Loss : 0.0089\n",
            "Step : 721, train Loss : 0.0077\n",
            "Step : 731, train Loss : 0.0084\n",
            "Step : 741, train Loss : 0.0069\n",
            "Step : 751, train Loss : 0.0072\n",
            "Step : 761, train Loss : 0.0079\n",
            "Step : 771, train Loss : 0.0055\n",
            "Step : 781, train Loss : 0.0050\n",
            "Step : 791, train Loss : 0.0093\n",
            "Step : 801, train Loss : 0.0068\n",
            "Step : 811, train Loss : 0.0074\n",
            "Step : 821, train Loss : 0.0057\n",
            "Step : 831, train Loss : 0.0056\n",
            "Step : 841, train Loss : 0.0073\n",
            "Step : 851, train Loss : 0.0057\n",
            "Step : 861, train Loss : 0.0078\n",
            "Step : 871, train Loss : 0.0067\n",
            "Step : 881, train Loss : 0.0060\n",
            "Step : 891, train Loss : 0.0069\n",
            "Step : 901, train Loss : 0.0088\n",
            "Step : 911, train Loss : 0.0054\n",
            "Step : 921, train Loss : 0.0057\n",
            "Step : 931, train Loss : 0.0058\n",
            "Step : 941, train Loss : 0.0057\n",
            "Step : 951, train Loss : 0.0077\n",
            "Step : 961, train Loss : 0.0068\n",
            "Step : 971, train Loss : 0.0079\n",
            "Step : 981, train Loss : 0.0068\n",
            "Step : 991, train Loss : 0.0074\n",
            "Step : 1001, train Loss : 0.0067\n",
            "Step : 1011, train Loss : 0.0057\n",
            "Step : 1021, train Loss : 0.0060\n",
            "Step : 1031, train Loss : 0.0085\n",
            "Step : 1041, train Loss : 0.0090\n",
            "Step : 1051, train Loss : 0.0057\n",
            "Step : 1061, train Loss : 0.0053\n",
            "Step : 1071, train Loss : 0.0087\n",
            "Step : 1081, train Loss : 0.0058\n",
            "Step : 1091, train Loss : 0.0055\n",
            "Step : 1101, train Loss : 0.0064\n",
            "Step : 1111, train Loss : 0.0056\n",
            "Step : 1121, train Loss : 0.0068\n",
            "Step : 1131, train Loss : 0.0057\n",
            "Step : 1141, train Loss : 0.0055\n",
            "Step : 1151, train Loss : 0.0075\n",
            "Step : 1161, train Loss : 0.0099\n",
            "Step : 1171, train Loss : 0.0072\n",
            "Step : 1181, train Loss : 0.0059\n",
            "Step : 1191, train Loss : 0.0069\n",
            "Step : 1201, train Loss : 0.0110\n",
            "Step : 1211, train Loss : 0.0115\n",
            "Step : 1221, train Loss : 0.0078\n",
            "Step : 1231, train Loss : 0.0053\n",
            "Step : 1241, train Loss : 0.0066\n",
            "Step : 1251, train Loss : 0.0062\n",
            "Step : 1261, train Loss : 0.0049\n",
            "Step : 1271, train Loss : 0.0065\n",
            "Step : 1281, train Loss : 0.0088\n",
            "Step : 1291, train Loss : 0.0081\n",
            "Step : 1301, train Loss : 0.0080\n",
            "Step : 1311, train Loss : 0.0067\n",
            "Step : 1321, train Loss : 0.0043\n",
            "Step : 1331, train Loss : 0.0047\n",
            "Step : 1341, train Loss : 0.0089\n",
            "Step : 1351, train Loss : 0.0069\n",
            "Step : 1361, train Loss : 0.0056\n",
            "Step : 1371, train Loss : 0.0065\n",
            "Step : 1381, train Loss : 0.0060\n",
            "Step : 1391, train Loss : 0.0061\n",
            "Step : 1401, train Loss : 0.0075\n",
            "Step : 1411, train Loss : 0.0070\n",
            "Step : 1421, train Loss : 0.0061\n",
            "Step : 1431, train Loss : 0.0070\n",
            "Step : 1441, train Loss : 0.0060\n",
            "Step : 1451, train Loss : 0.0064\n",
            "Step : 1461, train Loss : 0.0053\n",
            "Step : 1471, train Loss : 0.0064\n",
            "Step : 1481, train Loss : 0.0089\n",
            "Step : 1491, train Loss : 0.0057\n",
            "Step : 1501, train Loss : 0.0071\n",
            "Step : 1511, train Loss : 0.0063\n",
            "Step : 1521, train Loss : 0.0059\n",
            "Step : 1531, train Loss : 0.0070\n",
            "Step : 1541, train Loss : 0.0080\n",
            "Step : 1551, train Loss : 0.0096\n",
            "Step : 1561, train Loss : 0.0067\n",
            "Step : 1571, train Loss : 0.0077\n",
            "Step : 1581, train Loss : 0.0077\n",
            "Step : 1591, train Loss : 0.0086\n",
            "Step : 1601, train Loss : 0.0069\n",
            "Step : 1611, train Loss : 0.0069\n",
            "Step : 1621, train Loss : 0.0077\n",
            "Step : 1631, train Loss : 0.0066\n",
            "Step : 1641, train Loss : 0.0068\n",
            "Step : 1651, train Loss : 0.0047\n",
            "Step : 1661, train Loss : 0.0090\n",
            "Step : 1671, train Loss : 0.0062\n",
            "Step : 1681, train Loss : 0.0070\n",
            "Step : 1691, train Loss : 0.0048\n",
            "Step : 1701, train Loss : 0.0091\n",
            "Step : 1711, train Loss : 0.0050\n",
            "Step : 1721, train Loss : 0.0070\n",
            "Step : 1731, train Loss : 0.0098\n",
            "Step : 1741, train Loss : 0.0073\n",
            "Step : 1751, train Loss : 0.0084\n",
            "Step : 1761, train Loss : 0.0061\n",
            "Step : 1771, train Loss : 0.0055\n",
            "Step : 1781, train Loss : 0.0067\n",
            "Step : 1791, train Loss : 0.0061\n",
            "Step : 1801, train Loss : 0.0053\n",
            "Step : 1811, train Loss : 0.0067\n",
            "Step : 1821, train Loss : 0.0083\n",
            "Step : 1831, train Loss : 0.0089\n",
            "Step : 1841, train Loss : 0.0083\n",
            "Step : 1851, train Loss : 0.0061\n",
            "Step : 1861, train Loss : 0.0063\n",
            "Step : 1871, train Loss : 0.0081\n",
            "Step : 1881, train Loss : 0.0081\n",
            "Step : 1891, train Loss : 0.0082\n",
            "Step : 1901, train Loss : 0.0077\n",
            "Step : 1911, train Loss : 0.0065\n",
            "Step : 1921, train Loss : 0.0084\n",
            "Step : 1931, train Loss : 0.0053\n",
            "Step : 1941, train Loss : 0.0086\n",
            "Step : 1951, train Loss : 0.0058\n",
            "Epoch 2 Total Mean Loss : 0.0069\n",
            "*****Epoch 2 Train Finish*****\n",
            "\n",
            "SAVING EPOCH 2 ...\n",
            "*****Epoch 2 Valid Start*****\n",
            "Step: 0,  Pearson: tensor([0.8781])\n",
            "Step: 1,  Pearson: tensor([0.9230])\n",
            "Step: 2,  Pearson: tensor([0.9530])\n",
            "Step: 3,  Pearson: tensor([0.9183])\n",
            "Step: 4,  Pearson: tensor([0.9177])\n",
            "Step: 5,  Pearson: tensor([0.9851])\n",
            "Step: 6,  Pearson: tensor([0.9118])\n",
            "Step: 7,  Pearson: tensor([0.9323])\n",
            "Step: 8,  Pearson: tensor([0.9667])\n",
            "Step: 9,  Pearson: tensor([0.9479])\n",
            "Step: 10,  Pearson: tensor([0.8063])\n",
            "Step: 11,  Pearson: tensor([0.9125])\n",
            "Step: 12,  Pearson: tensor([0.8887])\n",
            "Step: 13,  Pearson: tensor([0.9373])\n",
            "Step: 14,  Pearson: tensor([0.9531])\n",
            "Step: 15,  Pearson: tensor([0.9591])\n",
            "Step: 16,  Pearson: tensor([0.9511])\n",
            "Step: 17,  Pearson: tensor([0.9530])\n",
            "Step: 18,  Pearson: tensor([0.7715])\n",
            "Step: 19,  Pearson: tensor([0.9543])\n",
            "Step: 20,  Pearson: tensor([0.9099])\n",
            "Step: 21,  Pearson: tensor([0.9439])\n",
            "Step: 22,  Pearson: tensor([0.9434])\n",
            "Step: 23,  Pearson: tensor([0.7638])\n",
            "Step: 24,  Pearson: tensor([0.9177])\n",
            "Step: 25,  Pearson: tensor([0.9540])\n",
            "Step: 26,  Pearson: tensor([0.9077])\n",
            "Step: 27,  Pearson: tensor([0.9688])\n",
            "Step: 28,  Pearson: tensor([0.9466])\n",
            "Step: 29,  Pearson: tensor([0.8706])\n",
            "Step: 30,  Pearson: tensor([0.9082])\n",
            "Step: 31,  Pearson: tensor([0.9706])\n",
            "Step: 32,  Pearson: tensor([0.9084])\n",
            "Step: 33,  Pearson: tensor([0.8946])\n",
            "Step: 34,  Pearson: tensor([0.9567])\n",
            "Step: 35,  Pearson: tensor([0.9567])\n",
            "Step: 36,  Pearson: tensor([0.8875])\n",
            "Step: 37,  Pearson: tensor([0.9095])\n",
            "Step: 38,  Pearson: tensor([0.9039])\n",
            "Step: 39,  Pearson: tensor([0.9361])\n",
            "Step: 40,  Pearson: tensor([0.9674])\n",
            "Step: 41,  Pearson: tensor([0.9451])\n",
            "Step: 42,  Pearson: tensor([0.8860])\n",
            "Step: 43,  Pearson: tensor([0.8933])\n",
            "Step: 44,  Pearson: tensor([0.9837])\n",
            "Step: 45,  Pearson: tensor([0.9306])\n",
            "Step: 46,  Pearson: tensor([0.9024])\n",
            "Step: 47,  Pearson: tensor([0.9593])\n",
            "Step: 48,  Pearson: tensor([0.8617])\n",
            "Step: 49,  Pearson: tensor([0.9026])\n",
            "Step: 50,  Pearson: tensor([0.9418])\n",
            "Step: 51,  Pearson: tensor([0.9592])\n",
            "Step: 52,  Pearson: tensor([0.9282])\n",
            "Step: 53,  Pearson: tensor([0.9067])\n",
            "Step: 54,  Pearson: tensor([0.9274])\n",
            "Step: 55,  Pearson: tensor([0.9317])\n",
            "Step: 56,  Pearson: tensor([0.9126])\n",
            "Step: 57,  Pearson: tensor([0.9250])\n",
            "Step: 58,  Pearson: tensor([0.9708])\n",
            "Step: 59,  Pearson: tensor([0.9617])\n",
            "Step: 60,  Pearson: tensor([0.9654])\n",
            "Step: 61,  Pearson: tensor([0.9631])\n",
            "Step: 62,  Pearson: tensor([0.8608])\n",
            "Step: 63,  Pearson: tensor([0.9585])\n",
            "Step: 64,  Pearson: tensor([0.9249])\n",
            "Step: 65,  Pearson: tensor([0.9658])\n",
            "Step: 66,  Pearson: tensor([0.9098])\n",
            "Step: 67,  Pearson: tensor([0.9781])\n",
            "Step: 68,  Pearson: tensor([0.9605])\n",
            "Step: 69,  Pearson: tensor([0.9413])\n",
            "Step: 70,  Pearson: tensor([0.9228])\n",
            "Step: 71,  Pearson: tensor([0.9055])\n",
            "Step: 72,  Pearson: tensor([0.9785])\n",
            "Step: 73,  Pearson: tensor([0.9110])\n",
            "Step: 74,  Pearson: tensor([0.9067])\n",
            "Step: 75,  Pearson: tensor([0.9711])\n",
            "Step: 76,  Pearson: tensor([0.9523])\n",
            "Step: 77,  Pearson: tensor([0.9266])\n",
            "Step: 78,  Pearson: tensor([0.7544])\n",
            "Step: 79,  Pearson: tensor([0.9496])\n",
            "Step: 80,  Pearson: tensor([0.9384])\n",
            "Step: 81,  Pearson: tensor([0.9500])\n",
            "Step: 82,  Pearson: tensor([0.9238])\n",
            "Step: 83,  Pearson: tensor([0.9259])\n",
            "Step: 84,  Pearson: tensor([0.9710])\n",
            "Step: 85,  Pearson: tensor([0.9027])\n",
            "Step: 86,  Pearson: tensor([0.8799])\n",
            "Step: 87,  Pearson: tensor([0.9739])\n",
            "Step: 88,  Pearson: tensor([0.8943])\n",
            "Step: 89,  Pearson: tensor([0.8464])\n",
            "Step: 90,  Pearson: tensor([0.9442])\n",
            "Step: 91,  Pearson: tensor([0.9497])\n",
            "Step: 92,  Pearson: tensor([0.9186])\n",
            "Step: 93,  Pearson: tensor([0.9768])\n",
            "Step: 94,  Pearson: tensor([0.9072])\n",
            "Step: 95,  Pearson: tensor([0.9283])\n",
            "Step: 96,  Pearson: tensor([0.9185])\n",
            "Step: 97,  Pearson: tensor([0.9507])\n",
            "Step: 98,  Pearson: tensor([0.8838])\n",
            "Step: 99,  Pearson: tensor([0.9471])\n",
            "Step: 100,  Pearson: tensor([0.9661])\n",
            "Step: 101,  Pearson: tensor([0.9447])\n",
            "Step: 102,  Pearson: tensor([0.9263])\n",
            "Step: 103,  Pearson: tensor([0.9324])\n",
            "Step: 104,  Pearson: tensor([0.8621])\n",
            "Step: 105,  Pearson: tensor([0.9285])\n",
            "Step: 106,  Pearson: tensor([0.9085])\n",
            "Step: 107,  Pearson: tensor([0.9184])\n",
            "Step: 108,  Pearson: tensor([0.9712])\n",
            "total_valid_loss :  0.49172540883952326 total_f1_score :  0.9141274238227147 total_pearsonr : tensor([0.9253])\n",
            "Epoch 2 Valid Loss : 0.49172540883952326 Valid Pearsonr : tensor([0.9253]) ValidF1 : 0.9141274238227147\n",
            "*****Epoch 2 Valid Finish*****\n",
            "\n",
            "****** STARTING TO TRAIN EPOCH #3 ******\n",
            "Step : 11, train Loss : 0.0058\n",
            "Step : 21, train Loss : 0.0071\n",
            "Step : 31, train Loss : 0.0058\n",
            "Step : 41, train Loss : 0.0037\n",
            "Step : 51, train Loss : 0.0045\n",
            "Step : 61, train Loss : 0.0045\n",
            "Step : 71, train Loss : 0.0066\n",
            "Step : 81, train Loss : 0.0045\n",
            "Step : 91, train Loss : 0.0048\n",
            "Step : 101, train Loss : 0.0038\n",
            "Step : 111, train Loss : 0.0048\n",
            "Step : 121, train Loss : 0.0033\n",
            "Step : 131, train Loss : 0.0051\n",
            "Step : 141, train Loss : 0.0055\n",
            "Step : 151, train Loss : 0.0054\n",
            "Step : 161, train Loss : 0.0047\n",
            "Step : 171, train Loss : 0.0043\n",
            "Step : 181, train Loss : 0.0062\n",
            "Step : 191, train Loss : 0.0052\n",
            "Step : 201, train Loss : 0.0039\n",
            "Step : 211, train Loss : 0.0044\n",
            "Step : 221, train Loss : 0.0057\n",
            "Step : 231, train Loss : 0.0060\n",
            "Step : 241, train Loss : 0.0040\n",
            "Step : 251, train Loss : 0.0053\n",
            "Step : 261, train Loss : 0.0042\n",
            "Step : 271, train Loss : 0.0088\n",
            "Step : 281, train Loss : 0.0045\n",
            "Step : 291, train Loss : 0.0036\n",
            "Step : 301, train Loss : 0.0036\n",
            "Step : 311, train Loss : 0.0053\n",
            "Step : 321, train Loss : 0.0045\n",
            "Step : 331, train Loss : 0.0056\n",
            "Step : 341, train Loss : 0.0056\n",
            "Step : 351, train Loss : 0.0044\n",
            "Step : 361, train Loss : 0.0044\n",
            "Step : 371, train Loss : 0.0057\n",
            "Step : 381, train Loss : 0.0047\n",
            "Step : 391, train Loss : 0.0039\n",
            "Step : 401, train Loss : 0.0058\n",
            "Step : 411, train Loss : 0.0042\n",
            "Step : 421, train Loss : 0.0056\n",
            "Step : 431, train Loss : 0.0046\n",
            "Step : 441, train Loss : 0.0035\n",
            "Step : 451, train Loss : 0.0064\n",
            "Step : 461, train Loss : 0.0035\n",
            "Step : 471, train Loss : 0.0044\n",
            "Step : 481, train Loss : 0.0041\n",
            "Step : 491, train Loss : 0.0041\n",
            "Step : 501, train Loss : 0.0043\n",
            "Step : 511, train Loss : 0.0043\n",
            "Step : 521, train Loss : 0.0039\n",
            "Step : 531, train Loss : 0.0046\n",
            "Step : 541, train Loss : 0.0041\n",
            "Step : 551, train Loss : 0.0044\n",
            "Step : 561, train Loss : 0.0038\n",
            "Step : 571, train Loss : 0.0060\n",
            "Step : 581, train Loss : 0.0055\n",
            "Step : 591, train Loss : 0.0041\n",
            "Step : 601, train Loss : 0.0051\n",
            "Step : 611, train Loss : 0.0053\n",
            "Step : 621, train Loss : 0.0055\n",
            "Step : 631, train Loss : 0.0035\n",
            "Step : 641, train Loss : 0.0044\n",
            "Step : 651, train Loss : 0.0042\n",
            "Step : 661, train Loss : 0.0043\n",
            "Step : 671, train Loss : 0.0058\n",
            "Step : 681, train Loss : 0.0037\n",
            "Step : 691, train Loss : 0.0043\n",
            "Step : 701, train Loss : 0.0045\n",
            "Step : 711, train Loss : 0.0067\n",
            "Step : 721, train Loss : 0.0047\n",
            "Step : 731, train Loss : 0.0051\n",
            "Step : 741, train Loss : 0.0055\n",
            "Step : 751, train Loss : 0.0063\n",
            "Step : 761, train Loss : 0.0041\n",
            "Step : 771, train Loss : 0.0073\n",
            "Step : 781, train Loss : 0.0051\n",
            "Step : 791, train Loss : 0.0041\n",
            "Step : 801, train Loss : 0.0058\n",
            "Step : 811, train Loss : 0.0029\n",
            "Step : 821, train Loss : 0.0040\n",
            "Step : 831, train Loss : 0.0040\n",
            "Step : 841, train Loss : 0.0049\n",
            "Step : 851, train Loss : 0.0055\n",
            "Step : 861, train Loss : 0.0066\n",
            "Step : 871, train Loss : 0.0035\n",
            "Step : 881, train Loss : 0.0055\n",
            "Step : 891, train Loss : 0.0049\n",
            "Step : 901, train Loss : 0.0042\n",
            "Step : 911, train Loss : 0.0063\n",
            "Step : 921, train Loss : 0.0066\n",
            "Step : 931, train Loss : 0.0046\n",
            "Step : 941, train Loss : 0.0058\n",
            "Step : 951, train Loss : 0.0029\n",
            "Step : 961, train Loss : 0.0062\n",
            "Step : 971, train Loss : 0.0053\n",
            "Step : 981, train Loss : 0.0042\n",
            "Step : 991, train Loss : 0.0048\n",
            "Step : 1001, train Loss : 0.0037\n",
            "Step : 1011, train Loss : 0.0038\n",
            "Step : 1021, train Loss : 0.0040\n",
            "Step : 1031, train Loss : 0.0059\n",
            "Step : 1041, train Loss : 0.0048\n",
            "Step : 1051, train Loss : 0.0038\n",
            "Step : 1061, train Loss : 0.0053\n",
            "Step : 1071, train Loss : 0.0086\n",
            "Step : 1081, train Loss : 0.0036\n",
            "Step : 1091, train Loss : 0.0042\n",
            "Step : 1101, train Loss : 0.0044\n",
            "Step : 1111, train Loss : 0.0043\n",
            "Step : 1121, train Loss : 0.0028\n",
            "Step : 1131, train Loss : 0.0044\n",
            "Step : 1141, train Loss : 0.0048\n",
            "Step : 1151, train Loss : 0.0051\n",
            "Step : 1161, train Loss : 0.0061\n",
            "Step : 1171, train Loss : 0.0051\n",
            "Step : 1181, train Loss : 0.0035\n",
            "Step : 1191, train Loss : 0.0056\n",
            "Step : 1201, train Loss : 0.0060\n",
            "Step : 1211, train Loss : 0.0041\n",
            "Step : 1221, train Loss : 0.0046\n",
            "Step : 1231, train Loss : 0.0054\n",
            "Step : 1241, train Loss : 0.0066\n",
            "Step : 1251, train Loss : 0.0057\n",
            "Step : 1261, train Loss : 0.0053\n",
            "Step : 1271, train Loss : 0.0048\n",
            "Step : 1281, train Loss : 0.0046\n",
            "Step : 1291, train Loss : 0.0055\n",
            "Step : 1301, train Loss : 0.0033\n",
            "Step : 1311, train Loss : 0.0042\n",
            "Step : 1321, train Loss : 0.0050\n",
            "Step : 1331, train Loss : 0.0056\n",
            "Step : 1341, train Loss : 0.0050\n",
            "Step : 1351, train Loss : 0.0040\n",
            "Step : 1361, train Loss : 0.0041\n",
            "Step : 1371, train Loss : 0.0044\n",
            "Step : 1381, train Loss : 0.0037\n",
            "Step : 1391, train Loss : 0.0038\n",
            "Step : 1401, train Loss : 0.0051\n",
            "Step : 1411, train Loss : 0.0062\n",
            "Step : 1421, train Loss : 0.0055\n",
            "Step : 1431, train Loss : 0.0032\n",
            "Step : 1441, train Loss : 0.0053\n",
            "Step : 1451, train Loss : 0.0049\n",
            "Step : 1461, train Loss : 0.0045\n",
            "Step : 1471, train Loss : 0.0050\n",
            "Step : 1481, train Loss : 0.0078\n",
            "Step : 1491, train Loss : 0.0042\n",
            "Step : 1501, train Loss : 0.0056\n",
            "Step : 1511, train Loss : 0.0041\n",
            "Step : 1521, train Loss : 0.0051\n",
            "Step : 1531, train Loss : 0.0048\n",
            "Step : 1541, train Loss : 0.0043\n",
            "Step : 1551, train Loss : 0.0047\n",
            "Step : 1561, train Loss : 0.0043\n",
            "Step : 1571, train Loss : 0.0033\n",
            "Step : 1581, train Loss : 0.0059\n",
            "Step : 1591, train Loss : 0.0053\n",
            "Step : 1601, train Loss : 0.0043\n",
            "Step : 1611, train Loss : 0.0048\n",
            "Step : 1621, train Loss : 0.0041\n",
            "Step : 1631, train Loss : 0.0041\n",
            "Step : 1641, train Loss : 0.0059\n",
            "Step : 1651, train Loss : 0.0037\n",
            "Step : 1661, train Loss : 0.0037\n",
            "Step : 1671, train Loss : 0.0048\n",
            "Step : 1681, train Loss : 0.0064\n",
            "Step : 1691, train Loss : 0.0058\n",
            "Step : 1701, train Loss : 0.0052\n",
            "Step : 1711, train Loss : 0.0046\n",
            "Step : 1721, train Loss : 0.0043\n",
            "Step : 1731, train Loss : 0.0049\n",
            "Step : 1741, train Loss : 0.0049\n",
            "Step : 1751, train Loss : 0.0050\n",
            "Step : 1761, train Loss : 0.0049\n",
            "Step : 1771, train Loss : 0.0032\n",
            "Step : 1781, train Loss : 0.0036\n",
            "Step : 1791, train Loss : 0.0058\n",
            "Step : 1801, train Loss : 0.0067\n",
            "Step : 1811, train Loss : 0.0030\n",
            "Step : 1821, train Loss : 0.0047\n",
            "Step : 1831, train Loss : 0.0056\n",
            "Step : 1841, train Loss : 0.0051\n",
            "Step : 1851, train Loss : 0.0044\n",
            "Step : 1861, train Loss : 0.0059\n",
            "Step : 1871, train Loss : 0.0033\n",
            "Step : 1881, train Loss : 0.0046\n",
            "Step : 1891, train Loss : 0.0040\n",
            "Step : 1901, train Loss : 0.0044\n",
            "Step : 1911, train Loss : 0.0042\n",
            "Step : 1921, train Loss : 0.0055\n",
            "Step : 1931, train Loss : 0.0038\n",
            "Step : 1941, train Loss : 0.0036\n",
            "Step : 1951, train Loss : 0.0056\n",
            "Epoch 3 Total Mean Loss : 0.0048\n",
            "*****Epoch 3 Train Finish*****\n",
            "\n",
            "SAVING EPOCH 3 ...\n",
            "*****Epoch 3 Valid Start*****\n",
            "Step: 0,  Pearson: tensor([0.8883])\n",
            "Step: 1,  Pearson: tensor([0.9167])\n",
            "Step: 2,  Pearson: tensor([0.9585])\n",
            "Step: 3,  Pearson: tensor([0.9185])\n",
            "Step: 4,  Pearson: tensor([0.9172])\n",
            "Step: 5,  Pearson: tensor([0.9837])\n",
            "Step: 6,  Pearson: tensor([0.9069])\n",
            "Step: 7,  Pearson: tensor([0.9273])\n",
            "Step: 8,  Pearson: tensor([0.9656])\n",
            "Step: 9,  Pearson: tensor([0.9522])\n",
            "Step: 10,  Pearson: tensor([0.8090])\n",
            "Step: 11,  Pearson: tensor([0.9069])\n",
            "Step: 12,  Pearson: tensor([0.8968])\n",
            "Step: 13,  Pearson: tensor([0.9466])\n",
            "Step: 14,  Pearson: tensor([0.9578])\n",
            "Step: 15,  Pearson: tensor([0.9632])\n",
            "Step: 16,  Pearson: tensor([0.9575])\n",
            "Step: 17,  Pearson: tensor([0.9494])\n",
            "Step: 18,  Pearson: tensor([0.7661])\n",
            "Step: 19,  Pearson: tensor([0.9561])\n",
            "Step: 20,  Pearson: tensor([0.9106])\n",
            "Step: 21,  Pearson: tensor([0.9480])\n",
            "Step: 22,  Pearson: tensor([0.9371])\n",
            "Step: 23,  Pearson: tensor([0.7551])\n",
            "Step: 24,  Pearson: tensor([0.9083])\n",
            "Step: 25,  Pearson: tensor([0.9535])\n",
            "Step: 26,  Pearson: tensor([0.9157])\n",
            "Step: 27,  Pearson: tensor([0.9695])\n",
            "Step: 28,  Pearson: tensor([0.9522])\n",
            "Step: 29,  Pearson: tensor([0.8695])\n",
            "Step: 30,  Pearson: tensor([0.9148])\n",
            "Step: 31,  Pearson: tensor([0.9725])\n",
            "Step: 32,  Pearson: tensor([0.9022])\n",
            "Step: 33,  Pearson: tensor([0.8800])\n",
            "Step: 34,  Pearson: tensor([0.9536])\n",
            "Step: 35,  Pearson: tensor([0.9534])\n",
            "Step: 36,  Pearson: tensor([0.8946])\n",
            "Step: 37,  Pearson: tensor([0.9037])\n",
            "Step: 38,  Pearson: tensor([0.9025])\n",
            "Step: 39,  Pearson: tensor([0.9419])\n",
            "Step: 40,  Pearson: tensor([0.9676])\n",
            "Step: 41,  Pearson: tensor([0.9492])\n",
            "Step: 42,  Pearson: tensor([0.8889])\n",
            "Step: 43,  Pearson: tensor([0.8859])\n",
            "Step: 44,  Pearson: tensor([0.9853])\n",
            "Step: 45,  Pearson: tensor([0.9267])\n",
            "Step: 46,  Pearson: tensor([0.8945])\n",
            "Step: 47,  Pearson: tensor([0.9508])\n",
            "Step: 48,  Pearson: tensor([0.8685])\n",
            "Step: 49,  Pearson: tensor([0.9037])\n",
            "Step: 50,  Pearson: tensor([0.9453])\n",
            "Step: 51,  Pearson: tensor([0.9590])\n",
            "Step: 52,  Pearson: tensor([0.9306])\n",
            "Step: 53,  Pearson: tensor([0.8954])\n",
            "Step: 54,  Pearson: tensor([0.9263])\n",
            "Step: 55,  Pearson: tensor([0.9109])\n",
            "Step: 56,  Pearson: tensor([0.9153])\n",
            "Step: 57,  Pearson: tensor([0.9365])\n",
            "Step: 58,  Pearson: tensor([0.9730])\n",
            "Step: 59,  Pearson: tensor([0.9597])\n",
            "Step: 60,  Pearson: tensor([0.9714])\n",
            "Step: 61,  Pearson: tensor([0.9647])\n",
            "Step: 62,  Pearson: tensor([0.8474])\n",
            "Step: 63,  Pearson: tensor([0.9613])\n",
            "Step: 64,  Pearson: tensor([0.9233])\n",
            "Step: 65,  Pearson: tensor([0.9646])\n",
            "Step: 66,  Pearson: tensor([0.9116])\n",
            "Step: 67,  Pearson: tensor([0.9779])\n",
            "Step: 68,  Pearson: tensor([0.9625])\n",
            "Step: 69,  Pearson: tensor([0.9429])\n",
            "Step: 70,  Pearson: tensor([0.9153])\n",
            "Step: 71,  Pearson: tensor([0.9085])\n",
            "Step: 72,  Pearson: tensor([0.9782])\n",
            "Step: 73,  Pearson: tensor([0.9140])\n",
            "Step: 74,  Pearson: tensor([0.9114])\n",
            "Step: 75,  Pearson: tensor([0.9745])\n",
            "Step: 76,  Pearson: tensor([0.9489])\n",
            "Step: 77,  Pearson: tensor([0.9277])\n",
            "Step: 78,  Pearson: tensor([0.7277])\n",
            "Step: 79,  Pearson: tensor([0.9537])\n",
            "Step: 80,  Pearson: tensor([0.9395])\n",
            "Step: 81,  Pearson: tensor([0.9511])\n",
            "Step: 82,  Pearson: tensor([0.9332])\n",
            "Step: 83,  Pearson: tensor([0.9093])\n",
            "Step: 84,  Pearson: tensor([0.9675])\n",
            "Step: 85,  Pearson: tensor([0.9092])\n",
            "Step: 86,  Pearson: tensor([0.8811])\n",
            "Step: 87,  Pearson: tensor([0.9763])\n",
            "Step: 88,  Pearson: tensor([0.8934])\n",
            "Step: 89,  Pearson: tensor([0.8456])\n",
            "Step: 90,  Pearson: tensor([0.9358])\n",
            "Step: 91,  Pearson: tensor([0.9486])\n",
            "Step: 92,  Pearson: tensor([0.9164])\n",
            "Step: 93,  Pearson: tensor([0.9816])\n",
            "Step: 94,  Pearson: tensor([0.9217])\n",
            "Step: 95,  Pearson: tensor([0.9296])\n",
            "Step: 96,  Pearson: tensor([0.9247])\n",
            "Step: 97,  Pearson: tensor([0.9525])\n",
            "Step: 98,  Pearson: tensor([0.8905])\n",
            "Step: 99,  Pearson: tensor([0.9513])\n",
            "Step: 100,  Pearson: tensor([0.9708])\n",
            "Step: 101,  Pearson: tensor([0.9538])\n",
            "Step: 102,  Pearson: tensor([0.9359])\n",
            "Step: 103,  Pearson: tensor([0.9288])\n",
            "Step: 104,  Pearson: tensor([0.8702])\n",
            "Step: 105,  Pearson: tensor([0.9313])\n",
            "Step: 106,  Pearson: tensor([0.9066])\n",
            "Step: 107,  Pearson: tensor([0.9215])\n",
            "Step: 108,  Pearson: tensor([0.9738])\n",
            "total_valid_loss :  0.48250897980611257 total_f1_score :  0.91019955654102 total_pearsonr : tensor([0.9257])\n",
            "Epoch 3 Valid Loss : 0.48250897980611257 Valid Pearsonr : tensor([0.9257]) ValidF1 : 0.91019955654102\n",
            "*****Epoch 3 Valid Finish*****\n",
            "\n",
            "** Train Completed! **\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fcf6c4ebd1114ee9916cb552eeb0f304"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▆█</td></tr><tr><td>total_f1_score </td><td>▁▄█▅</td></tr><tr><td>total_pearsonr</td><td>▁▄██</td></tr><tr><td>total_train_loss</td><td>█▃▂▁</td></tr><tr><td>total_train_lr</td><td>█▆▃▁</td></tr><tr><td>total_valid_loss</td><td>█▄▂▁</td></tr><tr><td>train_loss</td><td>█▄▂▂▂▂▂▂▂▂▁▂▁▁▂▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_lr</td><td>▂▃▅▇███▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>4</td></tr><tr><td>total_f1_score </td><td>0.9102</td></tr><tr><td>total_pearsonr</td><td>0.92573</td></tr><tr><td>total_train_loss</td><td>0.00482</td></tr><tr><td>total_train_lr</td><td>0.0</td></tr><tr><td>total_valid_loss</td><td>0.48251</td></tr><tr><td>train_loss</td><td>0.00561</td></tr><tr><td>train_lr</td><td>0.0</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">fragrant-sweep-3</strong>: <a href=\"https://wandb.ai/kdb/sts_v2/runs/hq2qnjn2\" target=\"_blank\">https://wandb.ai/kdb/sts_v2/runs/hq2qnjn2</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20220529_165737-hq2qnjn2/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: n6ddh39r with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \teps: 1e-08\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_batch_size: 8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tvalid_batch_size: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twarm_up_ratio: 0.1\n",
            "Some weights of the model checkpoint at klue/roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.17"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/NLP/wandb/run-20220529_183159-n6ddh39r</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/kdb/sts_v2/runs/n6ddh39r\" target=\"_blank\">fluent-sweep-4</a></strong> to <a href=\"https://wandb.ai/kdb/sts_v2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/kdb/sts_v2/sweeps/sy73kc4x\" target=\"_blank\">https://wandb.ai/kdb/sts_v2/sweeps/sy73kc4x</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "****** STARTING TO TRAIN EPOCH #0 ******\n",
            "Step : 11, train Loss : 0.2024\n",
            "Step : 21, train Loss : 0.2338\n",
            "Step : 31, train Loss : 0.2025\n",
            "Step : 41, train Loss : 0.1643\n",
            "Step : 51, train Loss : 0.1387\n",
            "Step : 61, train Loss : 0.0733\n",
            "Step : 71, train Loss : 0.0675\n",
            "Step : 81, train Loss : 0.0560\n",
            "Step : 91, train Loss : 0.0505\n",
            "Step : 101, train Loss : 0.0579\n",
            "Step : 111, train Loss : 0.0450\n",
            "Step : 121, train Loss : 0.0414\n",
            "Step : 131, train Loss : 0.0322\n",
            "Step : 141, train Loss : 0.0306\n",
            "Step : 151, train Loss : 0.0295\n",
            "Step : 161, train Loss : 0.0308\n",
            "Step : 171, train Loss : 0.0303\n",
            "Step : 181, train Loss : 0.0355\n",
            "Step : 191, train Loss : 0.0278\n",
            "Step : 201, train Loss : 0.0291\n",
            "Step : 211, train Loss : 0.0258\n",
            "Step : 221, train Loss : 0.0266\n",
            "Step : 231, train Loss : 0.0343\n",
            "Step : 241, train Loss : 0.0233\n",
            "Step : 251, train Loss : 0.0303\n",
            "Step : 261, train Loss : 0.0265\n",
            "Step : 271, train Loss : 0.0225\n",
            "Step : 281, train Loss : 0.0356\n",
            "Step : 291, train Loss : 0.0271\n",
            "Step : 301, train Loss : 0.0249\n",
            "Step : 311, train Loss : 0.0201\n",
            "Step : 321, train Loss : 0.0259\n",
            "Step : 331, train Loss : 0.0281\n",
            "Step : 341, train Loss : 0.0318\n",
            "Step : 351, train Loss : 0.0222\n",
            "Step : 361, train Loss : 0.0233\n",
            "Step : 371, train Loss : 0.0292\n",
            "Step : 381, train Loss : 0.0324\n",
            "Step : 391, train Loss : 0.0212\n",
            "Step : 401, train Loss : 0.0321\n",
            "Step : 411, train Loss : 0.0276\n",
            "Step : 421, train Loss : 0.0271\n",
            "Step : 431, train Loss : 0.0267\n",
            "Step : 441, train Loss : 0.0235\n",
            "Step : 451, train Loss : 0.0195\n",
            "Step : 461, train Loss : 0.0228\n",
            "Step : 471, train Loss : 0.0246\n",
            "Step : 481, train Loss : 0.0291\n",
            "Step : 491, train Loss : 0.0235\n",
            "Step : 501, train Loss : 0.0277\n",
            "Step : 511, train Loss : 0.0250\n",
            "Step : 521, train Loss : 0.0253\n",
            "Step : 531, train Loss : 0.0213\n",
            "Step : 541, train Loss : 0.0199\n",
            "Step : 551, train Loss : 0.0322\n",
            "Step : 561, train Loss : 0.0302\n",
            "Step : 571, train Loss : 0.0292\n",
            "Step : 581, train Loss : 0.0141\n",
            "Step : 591, train Loss : 0.0342\n",
            "Step : 601, train Loss : 0.0378\n",
            "Step : 611, train Loss : 0.0250\n",
            "Step : 621, train Loss : 0.0295\n",
            "Step : 631, train Loss : 0.0319\n",
            "Step : 641, train Loss : 0.0235\n",
            "Step : 651, train Loss : 0.0302\n",
            "Step : 661, train Loss : 0.0208\n",
            "Step : 671, train Loss : 0.0263\n",
            "Step : 681, train Loss : 0.0327\n",
            "Step : 691, train Loss : 0.0266\n",
            "Step : 701, train Loss : 0.0326\n",
            "Step : 711, train Loss : 0.0203\n",
            "Step : 721, train Loss : 0.0244\n",
            "Step : 731, train Loss : 0.0268\n",
            "Step : 741, train Loss : 0.0279\n",
            "Step : 751, train Loss : 0.0358\n",
            "Step : 761, train Loss : 0.0262\n",
            "Step : 771, train Loss : 0.0257\n",
            "Step : 781, train Loss : 0.0341\n",
            "Step : 791, train Loss : 0.0263\n",
            "Step : 801, train Loss : 0.0261\n",
            "Step : 811, train Loss : 0.0383\n",
            "Step : 821, train Loss : 0.0214\n",
            "Step : 831, train Loss : 0.0349\n",
            "Step : 841, train Loss : 0.0231\n",
            "Step : 851, train Loss : 0.0179\n",
            "Step : 861, train Loss : 0.0242\n",
            "Step : 871, train Loss : 0.0295\n",
            "Step : 881, train Loss : 0.0327\n",
            "Step : 891, train Loss : 0.0250\n",
            "Step : 901, train Loss : 0.0276\n",
            "Step : 911, train Loss : 0.0194\n",
            "Step : 921, train Loss : 0.0313\n",
            "Step : 931, train Loss : 0.0393\n",
            "Step : 941, train Loss : 0.0256\n",
            "Step : 951, train Loss : 0.0192\n",
            "Step : 961, train Loss : 0.0325\n",
            "Step : 971, train Loss : 0.0299\n",
            "Step : 981, train Loss : 0.0267\n",
            "Step : 991, train Loss : 0.0418\n",
            "Step : 1001, train Loss : 0.0218\n",
            "Step : 1011, train Loss : 0.0320\n",
            "Step : 1021, train Loss : 0.0242\n",
            "Step : 1031, train Loss : 0.0280\n",
            "Step : 1041, train Loss : 0.0221\n",
            "Step : 1051, train Loss : 0.0314\n",
            "Step : 1061, train Loss : 0.0292\n",
            "Step : 1071, train Loss : 0.0295\n",
            "Step : 1081, train Loss : 0.0266\n",
            "Step : 1091, train Loss : 0.0296\n",
            "Step : 1101, train Loss : 0.0258\n",
            "Step : 1111, train Loss : 0.0312\n",
            "Step : 1121, train Loss : 0.0319\n",
            "Step : 1131, train Loss : 0.0320\n",
            "Step : 1141, train Loss : 0.0222\n",
            "Step : 1151, train Loss : 0.0397\n",
            "Step : 1161, train Loss : 0.0388\n",
            "Step : 1171, train Loss : 0.0293\n",
            "Step : 1181, train Loss : 0.0278\n",
            "Step : 1191, train Loss : 0.0259\n",
            "Step : 1201, train Loss : 0.0307\n",
            "Step : 1211, train Loss : 0.0278\n",
            "Step : 1221, train Loss : 0.0311\n",
            "Step : 1231, train Loss : 0.0264\n",
            "Step : 1241, train Loss : 0.0218\n",
            "Step : 1251, train Loss : 0.0264\n",
            "Step : 1261, train Loss : 0.0188\n",
            "Step : 1271, train Loss : 0.0251\n",
            "Step : 1281, train Loss : 0.0225\n",
            "Step : 1291, train Loss : 0.0320\n",
            "Step : 1301, train Loss : 0.0273\n",
            "Step : 1311, train Loss : 0.0256\n",
            "Step : 1321, train Loss : 0.0345\n",
            "Step : 1331, train Loss : 0.0230\n",
            "Step : 1341, train Loss : 0.0302\n",
            "Step : 1351, train Loss : 0.0321\n",
            "Step : 1361, train Loss : 0.0326\n",
            "Step : 1371, train Loss : 0.0288\n",
            "Step : 1381, train Loss : 0.0265\n",
            "Step : 1391, train Loss : 0.0260\n",
            "Step : 1401, train Loss : 0.0261\n",
            "Step : 1411, train Loss : 0.0265\n",
            "Step : 1421, train Loss : 0.0237\n",
            "Step : 1431, train Loss : 0.0275\n",
            "Step : 1441, train Loss : 0.0233\n",
            "Step : 1451, train Loss : 0.0246\n",
            "Step : 1461, train Loss : 0.0309\n",
            "Step : 1471, train Loss : 0.0287\n",
            "Step : 1481, train Loss : 0.0248\n",
            "Step : 1491, train Loss : 0.0267\n",
            "Step : 1501, train Loss : 0.0187\n",
            "Step : 1511, train Loss : 0.0211\n",
            "Step : 1521, train Loss : 0.0201\n",
            "Step : 1531, train Loss : 0.0175\n",
            "Step : 1541, train Loss : 0.0272\n",
            "Step : 1551, train Loss : 0.0273\n",
            "Step : 1561, train Loss : 0.0309\n",
            "Step : 1571, train Loss : 0.0279\n",
            "Step : 1581, train Loss : 0.0204\n",
            "Step : 1591, train Loss : 0.0327\n",
            "Step : 1601, train Loss : 0.0272\n",
            "Step : 1611, train Loss : 0.0277\n",
            "Step : 1621, train Loss : 0.0288\n",
            "Step : 1631, train Loss : 0.0258\n",
            "Step : 1641, train Loss : 0.0244\n",
            "Step : 1651, train Loss : 0.0243\n",
            "Step : 1661, train Loss : 0.0238\n",
            "Step : 1671, train Loss : 0.0297\n",
            "Step : 1681, train Loss : 0.0271\n",
            "Step : 1691, train Loss : 0.0221\n",
            "Step : 1701, train Loss : 0.0203\n",
            "Step : 1711, train Loss : 0.0238\n",
            "Step : 1721, train Loss : 0.0294\n",
            "Step : 1731, train Loss : 0.0347\n",
            "Step : 1741, train Loss : 0.0240\n",
            "Step : 1751, train Loss : 0.0221\n",
            "Step : 1761, train Loss : 0.0251\n",
            "Step : 1771, train Loss : 0.0294\n",
            "Step : 1781, train Loss : 0.0167\n",
            "Step : 1791, train Loss : 0.0283\n",
            "Step : 1801, train Loss : 0.0318\n",
            "Step : 1811, train Loss : 0.0221\n",
            "Step : 1821, train Loss : 0.0193\n",
            "Step : 1831, train Loss : 0.0271\n",
            "Step : 1841, train Loss : 0.0233\n",
            "Step : 1851, train Loss : 0.0238\n",
            "Step : 1861, train Loss : 0.0268\n",
            "Step : 1871, train Loss : 0.0236\n",
            "Step : 1881, train Loss : 0.0167\n",
            "Step : 1891, train Loss : 0.0225\n",
            "Step : 1901, train Loss : 0.0273\n",
            "Step : 1911, train Loss : 0.0314\n",
            "Step : 1921, train Loss : 0.0181\n",
            "Step : 1931, train Loss : 0.0212\n",
            "Step : 1941, train Loss : 0.0232\n",
            "Step : 1951, train Loss : 0.0247\n",
            "Epoch 0 Total Mean Loss : 0.0322\n",
            "*****Epoch 0 Train Finish*****\n",
            "\n",
            "SAVING EPOCH 0 ...\n",
            "*****Epoch 0 Valid Start*****\n",
            "Step: 0,  Pearson: tensor([0.8904])\n",
            "Step: 1,  Pearson: tensor([0.9320])\n",
            "Step: 2,  Pearson: tensor([0.9751])\n",
            "Step: 3,  Pearson: tensor([0.8874])\n",
            "Step: 4,  Pearson: tensor([0.8643])\n",
            "Step: 5,  Pearson: tensor([0.9632])\n",
            "Step: 6,  Pearson: tensor([0.8318])\n",
            "Step: 7,  Pearson: tensor([0.8609])\n",
            "Step: 8,  Pearson: tensor([0.9395])\n",
            "Step: 9,  Pearson: tensor([0.9412])\n",
            "Step: 10,  Pearson: tensor([0.8100])\n",
            "Step: 11,  Pearson: tensor([0.8843])\n",
            "Step: 12,  Pearson: tensor([0.9077])\n",
            "Step: 13,  Pearson: tensor([0.9018])\n",
            "Step: 14,  Pearson: tensor([0.9497])\n",
            "Step: 15,  Pearson: tensor([0.9624])\n",
            "Step: 16,  Pearson: tensor([0.9264])\n",
            "Step: 17,  Pearson: tensor([0.9644])\n",
            "Step: 18,  Pearson: tensor([0.8093])\n",
            "Step: 19,  Pearson: tensor([0.9141])\n",
            "Step: 20,  Pearson: tensor([0.8749])\n",
            "Step: 21,  Pearson: tensor([0.8615])\n",
            "Step: 22,  Pearson: tensor([0.8976])\n",
            "Step: 23,  Pearson: tensor([0.7714])\n",
            "Step: 24,  Pearson: tensor([0.7925])\n",
            "Step: 25,  Pearson: tensor([0.9346])\n",
            "Step: 26,  Pearson: tensor([0.8623])\n",
            "Step: 27,  Pearson: tensor([0.9433])\n",
            "Step: 28,  Pearson: tensor([0.9087])\n",
            "Step: 29,  Pearson: tensor([0.7781])\n",
            "Step: 30,  Pearson: tensor([0.8392])\n",
            "Step: 31,  Pearson: tensor([0.9335])\n",
            "Step: 32,  Pearson: tensor([0.9293])\n",
            "Step: 33,  Pearson: tensor([0.7848])\n",
            "Step: 34,  Pearson: tensor([0.9599])\n",
            "Step: 35,  Pearson: tensor([0.9137])\n",
            "Step: 36,  Pearson: tensor([0.9097])\n",
            "Step: 37,  Pearson: tensor([0.8732])\n",
            "Step: 38,  Pearson: tensor([0.8819])\n",
            "Step: 39,  Pearson: tensor([0.9365])\n",
            "Step: 40,  Pearson: tensor([0.8844])\n",
            "Step: 41,  Pearson: tensor([0.9296])\n",
            "Step: 42,  Pearson: tensor([0.8446])\n",
            "Step: 43,  Pearson: tensor([0.8609])\n",
            "Step: 44,  Pearson: tensor([0.9579])\n",
            "Step: 45,  Pearson: tensor([0.9337])\n",
            "Step: 46,  Pearson: tensor([0.8092])\n",
            "Step: 47,  Pearson: tensor([0.9614])\n",
            "Step: 48,  Pearson: tensor([0.8720])\n",
            "Step: 49,  Pearson: tensor([0.9003])\n",
            "Step: 50,  Pearson: tensor([0.9373])\n",
            "Step: 51,  Pearson: tensor([0.9243])\n",
            "Step: 52,  Pearson: tensor([0.9140])\n",
            "Step: 53,  Pearson: tensor([0.9034])\n",
            "Step: 54,  Pearson: tensor([0.9423])\n",
            "Step: 55,  Pearson: tensor([0.9579])\n",
            "Step: 56,  Pearson: tensor([0.7882])\n",
            "Step: 57,  Pearson: tensor([0.9622])\n",
            "Step: 58,  Pearson: tensor([0.9267])\n",
            "Step: 59,  Pearson: tensor([0.9381])\n",
            "Step: 60,  Pearson: tensor([0.9485])\n",
            "Step: 61,  Pearson: tensor([0.9205])\n",
            "Step: 62,  Pearson: tensor([0.7707])\n",
            "Step: 63,  Pearson: tensor([0.9000])\n",
            "Step: 64,  Pearson: tensor([0.9331])\n",
            "Step: 65,  Pearson: tensor([0.9389])\n",
            "Step: 66,  Pearson: tensor([0.8809])\n",
            "Step: 67,  Pearson: tensor([0.9064])\n",
            "Step: 68,  Pearson: tensor([0.9359])\n",
            "Step: 69,  Pearson: tensor([0.8665])\n",
            "Step: 70,  Pearson: tensor([0.9322])\n",
            "Step: 71,  Pearson: tensor([0.9041])\n",
            "Step: 72,  Pearson: tensor([0.9774])\n",
            "Step: 73,  Pearson: tensor([0.8474])\n",
            "Step: 74,  Pearson: tensor([0.8651])\n",
            "Step: 75,  Pearson: tensor([0.9562])\n",
            "Step: 76,  Pearson: tensor([0.9340])\n",
            "Step: 77,  Pearson: tensor([0.9137])\n",
            "Step: 78,  Pearson: tensor([0.6776])\n",
            "Step: 79,  Pearson: tensor([0.9280])\n",
            "Step: 80,  Pearson: tensor([0.9111])\n",
            "Step: 81,  Pearson: tensor([0.9457])\n",
            "Step: 82,  Pearson: tensor([0.9174])\n",
            "Step: 83,  Pearson: tensor([0.8857])\n",
            "Step: 84,  Pearson: tensor([0.9854])\n",
            "Step: 85,  Pearson: tensor([0.8685])\n",
            "Step: 86,  Pearson: tensor([0.7938])\n",
            "Step: 87,  Pearson: tensor([0.9523])\n",
            "Step: 88,  Pearson: tensor([0.8580])\n",
            "Step: 89,  Pearson: tensor([0.7996])\n",
            "Step: 90,  Pearson: tensor([0.9078])\n",
            "Step: 91,  Pearson: tensor([0.9305])\n",
            "Step: 92,  Pearson: tensor([0.8844])\n",
            "Step: 93,  Pearson: tensor([0.9540])\n",
            "Step: 94,  Pearson: tensor([0.9043])\n",
            "Step: 95,  Pearson: tensor([0.8816])\n",
            "Step: 96,  Pearson: tensor([0.8126])\n",
            "Step: 97,  Pearson: tensor([0.9211])\n",
            "Step: 98,  Pearson: tensor([0.8286])\n",
            "Step: 99,  Pearson: tensor([0.9320])\n",
            "Step: 100,  Pearson: tensor([0.9407])\n",
            "Step: 101,  Pearson: tensor([0.9314])\n",
            "Step: 102,  Pearson: tensor([0.9047])\n",
            "Step: 103,  Pearson: tensor([0.9363])\n",
            "Step: 104,  Pearson: tensor([0.8789])\n",
            "Step: 105,  Pearson: tensor([0.8385])\n",
            "Step: 106,  Pearson: tensor([0.9302])\n",
            "Step: 107,  Pearson: tensor([0.8970])\n",
            "Step: 108,  Pearson: tensor([0.9454])\n",
            "total_valid_loss :  0.6384038141151087 total_f1_score :  0.8937360178970917 total_pearsonr : tensor([0.8998])\n",
            "Epoch 0 Valid Loss : 0.6384038141151087 Valid Pearsonr : tensor([0.8998]) ValidF1 : 0.8937360178970917\n",
            "*****Epoch 0 Valid Finish*****\n",
            "\n",
            "****** STARTING TO TRAIN EPOCH #1 ******\n",
            "Step : 11, train Loss : 0.0116\n",
            "Step : 21, train Loss : 0.0132\n",
            "Step : 31, train Loss : 0.0225\n",
            "Step : 41, train Loss : 0.0131\n",
            "Step : 51, train Loss : 0.0201\n",
            "Step : 61, train Loss : 0.0119\n",
            "Step : 71, train Loss : 0.0193\n",
            "Step : 81, train Loss : 0.0177\n",
            "Step : 91, train Loss : 0.0219\n",
            "Step : 101, train Loss : 0.0190\n",
            "Step : 111, train Loss : 0.0114\n",
            "Step : 121, train Loss : 0.0150\n",
            "Step : 131, train Loss : 0.0198\n",
            "Step : 141, train Loss : 0.0187\n",
            "Step : 151, train Loss : 0.0184\n",
            "Step : 161, train Loss : 0.0108\n",
            "Step : 171, train Loss : 0.0195\n",
            "Step : 181, train Loss : 0.0158\n",
            "Step : 191, train Loss : 0.0117\n",
            "Step : 201, train Loss : 0.0114\n",
            "Step : 211, train Loss : 0.0150\n",
            "Step : 221, train Loss : 0.0192\n",
            "Step : 231, train Loss : 0.0150\n",
            "Step : 241, train Loss : 0.0137\n",
            "Step : 251, train Loss : 0.0131\n",
            "Step : 261, train Loss : 0.0155\n",
            "Step : 271, train Loss : 0.0136\n",
            "Step : 281, train Loss : 0.0157\n",
            "Step : 291, train Loss : 0.0100\n",
            "Step : 301, train Loss : 0.0197\n",
            "Step : 311, train Loss : 0.0148\n",
            "Step : 321, train Loss : 0.0217\n",
            "Step : 331, train Loss : 0.0135\n",
            "Step : 341, train Loss : 0.0187\n",
            "Step : 351, train Loss : 0.0167\n",
            "Step : 361, train Loss : 0.0160\n",
            "Step : 371, train Loss : 0.0185\n",
            "Step : 381, train Loss : 0.0160\n",
            "Step : 391, train Loss : 0.0170\n",
            "Step : 401, train Loss : 0.0180\n",
            "Step : 411, train Loss : 0.0090\n",
            "Step : 421, train Loss : 0.0157\n",
            "Step : 431, train Loss : 0.0144\n",
            "Step : 441, train Loss : 0.0168\n",
            "Step : 451, train Loss : 0.0118\n",
            "Step : 461, train Loss : 0.0140\n",
            "Step : 471, train Loss : 0.0158\n",
            "Step : 481, train Loss : 0.0242\n",
            "Step : 491, train Loss : 0.0152\n",
            "Step : 501, train Loss : 0.0149\n",
            "Step : 511, train Loss : 0.0150\n",
            "Step : 521, train Loss : 0.0192\n",
            "Step : 531, train Loss : 0.0193\n",
            "Step : 541, train Loss : 0.0177\n",
            "Step : 551, train Loss : 0.0175\n",
            "Step : 561, train Loss : 0.0167\n",
            "Step : 571, train Loss : 0.0174\n",
            "Step : 581, train Loss : 0.0157\n",
            "Step : 591, train Loss : 0.0181\n",
            "Step : 601, train Loss : 0.0191\n",
            "Step : 611, train Loss : 0.0150\n",
            "Step : 621, train Loss : 0.0153\n",
            "Step : 631, train Loss : 0.0224\n",
            "Step : 641, train Loss : 0.0137\n",
            "Step : 651, train Loss : 0.0175\n",
            "Step : 661, train Loss : 0.0139\n",
            "Step : 671, train Loss : 0.0165\n",
            "Step : 681, train Loss : 0.0149\n",
            "Step : 691, train Loss : 0.0136\n",
            "Step : 701, train Loss : 0.0172\n",
            "Step : 711, train Loss : 0.0176\n",
            "Step : 721, train Loss : 0.0183\n",
            "Step : 731, train Loss : 0.0203\n",
            "Step : 741, train Loss : 0.0153\n",
            "Step : 751, train Loss : 0.0211\n",
            "Step : 761, train Loss : 0.0119\n",
            "Step : 771, train Loss : 0.0185\n",
            "Step : 781, train Loss : 0.0135\n",
            "Step : 791, train Loss : 0.0142\n",
            "Step : 801, train Loss : 0.0163\n",
            "Step : 811, train Loss : 0.0110\n",
            "Step : 821, train Loss : 0.0158\n",
            "Step : 831, train Loss : 0.0173\n",
            "Step : 841, train Loss : 0.0155\n",
            "Step : 851, train Loss : 0.0135\n",
            "Step : 861, train Loss : 0.0222\n",
            "Step : 871, train Loss : 0.0185\n",
            "Step : 881, train Loss : 0.0231\n",
            "Step : 891, train Loss : 0.0180\n",
            "Step : 901, train Loss : 0.0145\n",
            "Step : 911, train Loss : 0.0147\n",
            "Step : 921, train Loss : 0.0152\n",
            "Step : 931, train Loss : 0.0201\n",
            "Step : 941, train Loss : 0.0109\n",
            "Step : 951, train Loss : 0.0230\n",
            "Step : 961, train Loss : 0.0144\n",
            "Step : 971, train Loss : 0.0167\n",
            "Step : 981, train Loss : 0.0197\n",
            "Step : 991, train Loss : 0.0183\n",
            "Step : 1001, train Loss : 0.0155\n",
            "Step : 1011, train Loss : 0.0141\n",
            "Step : 1021, train Loss : 0.0174\n",
            "Step : 1031, train Loss : 0.0140\n",
            "Step : 1041, train Loss : 0.0076\n",
            "Step : 1051, train Loss : 0.0147\n",
            "Step : 1061, train Loss : 0.0093\n",
            "Step : 1071, train Loss : 0.0173\n",
            "Step : 1081, train Loss : 0.0106\n",
            "Step : 1091, train Loss : 0.0147\n",
            "Step : 1101, train Loss : 0.0203\n",
            "Step : 1111, train Loss : 0.0119\n",
            "Step : 1121, train Loss : 0.0158\n",
            "Step : 1131, train Loss : 0.0155\n",
            "Step : 1141, train Loss : 0.0122\n",
            "Step : 1151, train Loss : 0.0189\n",
            "Step : 1161, train Loss : 0.0122\n",
            "Step : 1171, train Loss : 0.0130\n",
            "Step : 1181, train Loss : 0.0185\n",
            "Step : 1191, train Loss : 0.0158\n",
            "Step : 1201, train Loss : 0.0134\n",
            "Step : 1211, train Loss : 0.0142\n",
            "Step : 1221, train Loss : 0.0128\n",
            "Step : 1231, train Loss : 0.0218\n",
            "Step : 1241, train Loss : 0.0171\n",
            "Step : 1251, train Loss : 0.0212\n",
            "Step : 1261, train Loss : 0.0143\n",
            "Step : 1271, train Loss : 0.0144\n",
            "Step : 1281, train Loss : 0.0178\n",
            "Step : 1291, train Loss : 0.0164\n",
            "Step : 1301, train Loss : 0.0084\n",
            "Step : 1311, train Loss : 0.0124\n",
            "Step : 1321, train Loss : 0.0110\n",
            "Step : 1331, train Loss : 0.0179\n",
            "Step : 1341, train Loss : 0.0171\n",
            "Step : 1351, train Loss : 0.0144\n",
            "Step : 1361, train Loss : 0.0135\n",
            "Step : 1371, train Loss : 0.0154\n",
            "Step : 1381, train Loss : 0.0153\n",
            "Step : 1391, train Loss : 0.0120\n",
            "Step : 1401, train Loss : 0.0167\n",
            "Step : 1411, train Loss : 0.0145\n",
            "Step : 1421, train Loss : 0.0139\n",
            "Step : 1431, train Loss : 0.0164\n",
            "Step : 1441, train Loss : 0.0111\n",
            "Step : 1451, train Loss : 0.0146\n",
            "Step : 1461, train Loss : 0.0191\n",
            "Step : 1471, train Loss : 0.0165\n",
            "Step : 1481, train Loss : 0.0163\n",
            "Step : 1491, train Loss : 0.0181\n",
            "Step : 1501, train Loss : 0.0151\n",
            "Step : 1511, train Loss : 0.0150\n",
            "Step : 1521, train Loss : 0.0173\n",
            "Step : 1531, train Loss : 0.0115\n",
            "Step : 1541, train Loss : 0.0121\n",
            "Step : 1551, train Loss : 0.0106\n",
            "Step : 1561, train Loss : 0.0215\n",
            "Step : 1571, train Loss : 0.0166\n",
            "Step : 1581, train Loss : 0.0118\n",
            "Step : 1591, train Loss : 0.0139\n",
            "Step : 1601, train Loss : 0.0134\n",
            "Step : 1611, train Loss : 0.0143\n",
            "Step : 1621, train Loss : 0.0161\n",
            "Step : 1631, train Loss : 0.0187\n",
            "Step : 1641, train Loss : 0.0143\n",
            "Step : 1651, train Loss : 0.0137\n",
            "Step : 1661, train Loss : 0.0202\n",
            "Step : 1671, train Loss : 0.0111\n",
            "Step : 1681, train Loss : 0.0116\n",
            "Step : 1691, train Loss : 0.0234\n",
            "Step : 1701, train Loss : 0.0158\n",
            "Step : 1711, train Loss : 0.0164\n",
            "Step : 1721, train Loss : 0.0134\n",
            "Step : 1731, train Loss : 0.0148\n",
            "Step : 1741, train Loss : 0.0144\n",
            "Step : 1751, train Loss : 0.0096\n",
            "Step : 1761, train Loss : 0.0197\n",
            "Step : 1771, train Loss : 0.0114\n",
            "Step : 1781, train Loss : 0.0135\n",
            "Step : 1791, train Loss : 0.0172\n",
            "Step : 1801, train Loss : 0.0144\n",
            "Step : 1811, train Loss : 0.0124\n",
            "Step : 1821, train Loss : 0.0114\n",
            "Step : 1831, train Loss : 0.0151\n",
            "Step : 1841, train Loss : 0.0158\n",
            "Step : 1851, train Loss : 0.0151\n",
            "Step : 1861, train Loss : 0.0181\n",
            "Step : 1871, train Loss : 0.0161\n",
            "Step : 1881, train Loss : 0.0132\n",
            "Step : 1891, train Loss : 0.0162\n",
            "Step : 1901, train Loss : 0.0148\n",
            "Step : 1911, train Loss : 0.0171\n",
            "Step : 1921, train Loss : 0.0159\n",
            "Step : 1931, train Loss : 0.0114\n",
            "Step : 1941, train Loss : 0.0144\n",
            "Step : 1951, train Loss : 0.0170\n",
            "Epoch 1 Total Mean Loss : 0.0157\n",
            "*****Epoch 1 Train Finish*****\n",
            "\n",
            "SAVING EPOCH 1 ...\n",
            "*****Epoch 1 Valid Start*****\n",
            "Step: 0,  Pearson: tensor([0.8963])\n",
            "Step: 1,  Pearson: tensor([0.9551])\n",
            "Step: 2,  Pearson: tensor([0.9545])\n",
            "Step: 3,  Pearson: tensor([0.8045])\n",
            "Step: 4,  Pearson: tensor([0.8737])\n",
            "Step: 5,  Pearson: tensor([0.9433])\n",
            "Step: 6,  Pearson: tensor([0.8904])\n",
            "Step: 7,  Pearson: tensor([0.9113])\n",
            "Step: 8,  Pearson: tensor([0.9340])\n",
            "Step: 9,  Pearson: tensor([0.9181])\n",
            "Step: 10,  Pearson: tensor([0.8479])\n",
            "Step: 11,  Pearson: tensor([0.9246])\n",
            "Step: 12,  Pearson: tensor([0.8756])\n",
            "Step: 13,  Pearson: tensor([0.9241])\n",
            "Step: 14,  Pearson: tensor([0.9283])\n",
            "Step: 15,  Pearson: tensor([0.9460])\n",
            "Step: 16,  Pearson: tensor([0.9438])\n",
            "Step: 17,  Pearson: tensor([0.9501])\n",
            "Step: 18,  Pearson: tensor([0.7053])\n",
            "Step: 19,  Pearson: tensor([0.9082])\n",
            "Step: 20,  Pearson: tensor([0.8513])\n",
            "Step: 21,  Pearson: tensor([0.8955])\n",
            "Step: 22,  Pearson: tensor([0.9143])\n",
            "Step: 23,  Pearson: tensor([0.7855])\n",
            "Step: 24,  Pearson: tensor([0.7869])\n",
            "Step: 25,  Pearson: tensor([0.9565])\n",
            "Step: 26,  Pearson: tensor([0.9023])\n",
            "Step: 27,  Pearson: tensor([0.9607])\n",
            "Step: 28,  Pearson: tensor([0.9613])\n",
            "Step: 29,  Pearson: tensor([0.8660])\n",
            "Step: 30,  Pearson: tensor([0.8697])\n",
            "Step: 31,  Pearson: tensor([0.9457])\n",
            "Step: 32,  Pearson: tensor([0.9326])\n",
            "Step: 33,  Pearson: tensor([0.9380])\n",
            "Step: 34,  Pearson: tensor([0.9662])\n",
            "Step: 35,  Pearson: tensor([0.8966])\n",
            "Step: 36,  Pearson: tensor([0.8386])\n",
            "Step: 37,  Pearson: tensor([0.8481])\n",
            "Step: 38,  Pearson: tensor([0.8905])\n",
            "Step: 39,  Pearson: tensor([0.9469])\n",
            "Step: 40,  Pearson: tensor([0.9082])\n",
            "Step: 41,  Pearson: tensor([0.9521])\n",
            "Step: 42,  Pearson: tensor([0.8774])\n",
            "Step: 43,  Pearson: tensor([0.8953])\n",
            "Step: 44,  Pearson: tensor([0.9704])\n",
            "Step: 45,  Pearson: tensor([0.9239])\n",
            "Step: 46,  Pearson: tensor([0.9269])\n",
            "Step: 47,  Pearson: tensor([0.8959])\n",
            "Step: 48,  Pearson: tensor([0.9368])\n",
            "Step: 49,  Pearson: tensor([0.9003])\n",
            "Step: 50,  Pearson: tensor([0.9128])\n",
            "Step: 51,  Pearson: tensor([0.9479])\n",
            "Step: 52,  Pearson: tensor([0.9381])\n",
            "Step: 53,  Pearson: tensor([0.9432])\n",
            "Step: 54,  Pearson: tensor([0.9321])\n",
            "Step: 55,  Pearson: tensor([0.9429])\n",
            "Step: 56,  Pearson: tensor([0.8933])\n",
            "Step: 57,  Pearson: tensor([0.9297])\n",
            "Step: 58,  Pearson: tensor([0.9119])\n",
            "Step: 59,  Pearson: tensor([0.9169])\n",
            "Step: 60,  Pearson: tensor([0.9618])\n",
            "Step: 61,  Pearson: tensor([0.9186])\n",
            "Step: 62,  Pearson: tensor([0.8070])\n",
            "Step: 63,  Pearson: tensor([0.9699])\n",
            "Step: 64,  Pearson: tensor([0.9210])\n",
            "Step: 65,  Pearson: tensor([0.9533])\n",
            "Step: 66,  Pearson: tensor([0.9038])\n",
            "Step: 67,  Pearson: tensor([0.9561])\n",
            "Step: 68,  Pearson: tensor([0.9463])\n",
            "Step: 69,  Pearson: tensor([0.9024])\n",
            "Step: 70,  Pearson: tensor([0.8946])\n",
            "Step: 71,  Pearson: tensor([0.9372])\n",
            "Step: 72,  Pearson: tensor([0.9479])\n",
            "Step: 73,  Pearson: tensor([0.8748])\n",
            "Step: 74,  Pearson: tensor([0.9193])\n",
            "Step: 75,  Pearson: tensor([0.9807])\n",
            "Step: 76,  Pearson: tensor([0.9366])\n",
            "Step: 77,  Pearson: tensor([0.9346])\n",
            "Step: 78,  Pearson: tensor([0.7148])\n",
            "Step: 79,  Pearson: tensor([0.9560])\n",
            "Step: 80,  Pearson: tensor([0.9388])\n",
            "Step: 81,  Pearson: tensor([0.9350])\n",
            "Step: 82,  Pearson: tensor([0.9045])\n",
            "Step: 83,  Pearson: tensor([0.8633])\n",
            "Step: 84,  Pearson: tensor([0.9779])\n",
            "Step: 85,  Pearson: tensor([0.8845])\n",
            "Step: 86,  Pearson: tensor([0.8751])\n",
            "Step: 87,  Pearson: tensor([0.9593])\n",
            "Step: 88,  Pearson: tensor([0.8827])\n",
            "Step: 89,  Pearson: tensor([0.8223])\n",
            "Step: 90,  Pearson: tensor([0.9372])\n",
            "Step: 91,  Pearson: tensor([0.9600])\n",
            "Step: 92,  Pearson: tensor([0.8839])\n",
            "Step: 93,  Pearson: tensor([0.9676])\n",
            "Step: 94,  Pearson: tensor([0.8974])\n",
            "Step: 95,  Pearson: tensor([0.9157])\n",
            "Step: 96,  Pearson: tensor([0.8899])\n",
            "Step: 97,  Pearson: tensor([0.9077])\n",
            "Step: 98,  Pearson: tensor([0.8599])\n",
            "Step: 99,  Pearson: tensor([0.9540])\n",
            "Step: 100,  Pearson: tensor([0.9526])\n",
            "Step: 101,  Pearson: tensor([0.8910])\n",
            "Step: 102,  Pearson: tensor([0.8877])\n",
            "Step: 103,  Pearson: tensor([0.9762])\n",
            "Step: 104,  Pearson: tensor([0.8343])\n",
            "Step: 105,  Pearson: tensor([0.8728])\n",
            "Step: 106,  Pearson: tensor([0.8559])\n",
            "Step: 107,  Pearson: tensor([0.8696])\n",
            "Step: 108,  Pearson: tensor([0.9744])\n",
            "total_valid_loss :  0.5847896131080225 total_f1_score :  0.9025069637883009 total_pearsonr : tensor([0.9093])\n",
            "Epoch 1 Valid Loss : 0.5847896131080225 Valid Pearsonr : tensor([0.9093]) ValidF1 : 0.9025069637883009\n",
            "*****Epoch 1 Valid Finish*****\n",
            "\n",
            "****** STARTING TO TRAIN EPOCH #2 ******\n",
            "Step : 11, train Loss : 0.0094\n",
            "Step : 21, train Loss : 0.0070\n",
            "Step : 31, train Loss : 0.0111\n",
            "Step : 41, train Loss : 0.0087\n",
            "Step : 51, train Loss : 0.0112\n",
            "Step : 61, train Loss : 0.0098\n",
            "Step : 71, train Loss : 0.0071\n",
            "Step : 81, train Loss : 0.0086\n",
            "Step : 91, train Loss : 0.0069\n",
            "Step : 101, train Loss : 0.0094\n",
            "Step : 111, train Loss : 0.0065\n",
            "Step : 121, train Loss : 0.0100\n",
            "Step : 131, train Loss : 0.0095\n",
            "Step : 141, train Loss : 0.0097\n",
            "Step : 151, train Loss : 0.0072\n",
            "Step : 161, train Loss : 0.0087\n",
            "Step : 171, train Loss : 0.0120\n",
            "Step : 181, train Loss : 0.0094\n",
            "Step : 191, train Loss : 0.0110\n",
            "Step : 201, train Loss : 0.0117\n",
            "Step : 211, train Loss : 0.0079\n",
            "Step : 221, train Loss : 0.0116\n",
            "Step : 231, train Loss : 0.0087\n",
            "Step : 241, train Loss : 0.0095\n",
            "Step : 251, train Loss : 0.0088\n",
            "Step : 261, train Loss : 0.0090\n",
            "Step : 271, train Loss : 0.0095\n",
            "Step : 281, train Loss : 0.0061\n",
            "Step : 291, train Loss : 0.0101\n",
            "Step : 301, train Loss : 0.0068\n",
            "Step : 311, train Loss : 0.0089\n",
            "Step : 321, train Loss : 0.0066\n",
            "Step : 331, train Loss : 0.0090\n",
            "Step : 341, train Loss : 0.0079\n",
            "Step : 351, train Loss : 0.0099\n",
            "Step : 361, train Loss : 0.0123\n",
            "Step : 371, train Loss : 0.0113\n",
            "Step : 381, train Loss : 0.0105\n",
            "Step : 391, train Loss : 0.0084\n",
            "Step : 401, train Loss : 0.0074\n",
            "Step : 411, train Loss : 0.0116\n",
            "Step : 421, train Loss : 0.0079\n",
            "Step : 431, train Loss : 0.0082\n",
            "Step : 441, train Loss : 0.0082\n",
            "Step : 451, train Loss : 0.0081\n",
            "Step : 461, train Loss : 0.0068\n",
            "Step : 471, train Loss : 0.0060\n",
            "Step : 481, train Loss : 0.0089\n",
            "Step : 491, train Loss : 0.0071\n",
            "Step : 501, train Loss : 0.0071\n",
            "Step : 511, train Loss : 0.0082\n",
            "Step : 521, train Loss : 0.0083\n",
            "Step : 531, train Loss : 0.0069\n",
            "Step : 541, train Loss : 0.0082\n",
            "Step : 551, train Loss : 0.0082\n",
            "Step : 561, train Loss : 0.0074\n",
            "Step : 571, train Loss : 0.0084\n",
            "Step : 581, train Loss : 0.0062\n",
            "Step : 591, train Loss : 0.0067\n",
            "Step : 601, train Loss : 0.0078\n",
            "Step : 611, train Loss : 0.0081\n",
            "Step : 621, train Loss : 0.0073\n",
            "Step : 631, train Loss : 0.0049\n",
            "Step : 641, train Loss : 0.0076\n",
            "Step : 651, train Loss : 0.0079\n",
            "Step : 661, train Loss : 0.0072\n",
            "Step : 671, train Loss : 0.0070\n",
            "Step : 681, train Loss : 0.0059\n",
            "Step : 691, train Loss : 0.0083\n",
            "Step : 701, train Loss : 0.0093\n",
            "Step : 711, train Loss : 0.0108\n",
            "Step : 721, train Loss : 0.0077\n",
            "Step : 731, train Loss : 0.0061\n",
            "Step : 741, train Loss : 0.0070\n",
            "Step : 751, train Loss : 0.0085\n",
            "Step : 761, train Loss : 0.0071\n",
            "Step : 771, train Loss : 0.0077\n",
            "Step : 781, train Loss : 0.0074\n",
            "Step : 791, train Loss : 0.0087\n",
            "Step : 801, train Loss : 0.0080\n",
            "Step : 811, train Loss : 0.0090\n",
            "Step : 821, train Loss : 0.0072\n",
            "Step : 831, train Loss : 0.0071\n",
            "Step : 841, train Loss : 0.0085\n",
            "Step : 851, train Loss : 0.0126\n",
            "Step : 861, train Loss : 0.0076\n",
            "Step : 871, train Loss : 0.0099\n",
            "Step : 881, train Loss : 0.0094\n",
            "Step : 891, train Loss : 0.0079\n",
            "Step : 901, train Loss : 0.0072\n",
            "Step : 911, train Loss : 0.0101\n",
            "Step : 921, train Loss : 0.0101\n",
            "Step : 931, train Loss : 0.0069\n",
            "Step : 941, train Loss : 0.0054\n",
            "Step : 951, train Loss : 0.0086\n",
            "Step : 961, train Loss : 0.0100\n",
            "Step : 971, train Loss : 0.0105\n",
            "Step : 981, train Loss : 0.0083\n",
            "Step : 991, train Loss : 0.0080\n",
            "Step : 1001, train Loss : 0.0093\n",
            "Step : 1011, train Loss : 0.0051\n",
            "Step : 1021, train Loss : 0.0087\n",
            "Step : 1031, train Loss : 0.0078\n",
            "Step : 1041, train Loss : 0.0085\n",
            "Step : 1051, train Loss : 0.0105\n",
            "Step : 1061, train Loss : 0.0095\n",
            "Step : 1071, train Loss : 0.0064\n",
            "Step : 1081, train Loss : 0.0084\n",
            "Step : 1091, train Loss : 0.0067\n",
            "Step : 1101, train Loss : 0.0091\n",
            "Step : 1111, train Loss : 0.0060\n",
            "Step : 1121, train Loss : 0.0056\n",
            "Step : 1131, train Loss : 0.0095\n",
            "Step : 1141, train Loss : 0.0055\n",
            "Step : 1151, train Loss : 0.0080\n",
            "Step : 1161, train Loss : 0.0090\n",
            "Step : 1171, train Loss : 0.0067\n",
            "Step : 1181, train Loss : 0.0079\n",
            "Step : 1191, train Loss : 0.0063\n",
            "Step : 1201, train Loss : 0.0094\n",
            "Step : 1211, train Loss : 0.0076\n",
            "Step : 1221, train Loss : 0.0074\n",
            "Step : 1231, train Loss : 0.0073\n",
            "Step : 1241, train Loss : 0.0064\n",
            "Step : 1251, train Loss : 0.0072\n",
            "Step : 1261, train Loss : 0.0090\n",
            "Step : 1271, train Loss : 0.0052\n",
            "Step : 1281, train Loss : 0.0089\n",
            "Step : 1291, train Loss : 0.0062\n",
            "Step : 1301, train Loss : 0.0142\n",
            "Step : 1311, train Loss : 0.0075\n",
            "Step : 1321, train Loss : 0.0050\n",
            "Step : 1331, train Loss : 0.0066\n",
            "Step : 1341, train Loss : 0.0085\n",
            "Step : 1351, train Loss : 0.0088\n",
            "Step : 1361, train Loss : 0.0073\n",
            "Step : 1371, train Loss : 0.0119\n",
            "Step : 1381, train Loss : 0.0056\n",
            "Step : 1391, train Loss : 0.0100\n",
            "Step : 1401, train Loss : 0.0062\n",
            "Step : 1411, train Loss : 0.0090\n",
            "Step : 1421, train Loss : 0.0070\n",
            "Step : 1431, train Loss : 0.0070\n",
            "Step : 1441, train Loss : 0.0070\n",
            "Step : 1451, train Loss : 0.0091\n",
            "Step : 1461, train Loss : 0.0075\n",
            "Step : 1471, train Loss : 0.0078\n",
            "Step : 1481, train Loss : 0.0076\n",
            "Step : 1491, train Loss : 0.0081\n",
            "Step : 1501, train Loss : 0.0070\n",
            "Step : 1511, train Loss : 0.0069\n",
            "Step : 1521, train Loss : 0.0094\n",
            "Step : 1531, train Loss : 0.0074\n",
            "Step : 1541, train Loss : 0.0053\n",
            "Step : 1551, train Loss : 0.0085\n",
            "Step : 1561, train Loss : 0.0081\n",
            "Step : 1571, train Loss : 0.0059\n",
            "Step : 1581, train Loss : 0.0087\n",
            "Step : 1591, train Loss : 0.0091\n",
            "Step : 1601, train Loss : 0.0082\n",
            "Step : 1611, train Loss : 0.0096\n",
            "Step : 1621, train Loss : 0.0069\n",
            "Step : 1631, train Loss : 0.0064\n",
            "Step : 1641, train Loss : 0.0075\n",
            "Step : 1651, train Loss : 0.0077\n",
            "Step : 1661, train Loss : 0.0091\n",
            "Step : 1671, train Loss : 0.0073\n",
            "Step : 1681, train Loss : 0.0090\n",
            "Step : 1691, train Loss : 0.0088\n",
            "Step : 1701, train Loss : 0.0077\n",
            "Step : 1711, train Loss : 0.0088\n",
            "Step : 1721, train Loss : 0.0089\n",
            "Step : 1731, train Loss : 0.0092\n",
            "Step : 1741, train Loss : 0.0073\n",
            "Step : 1751, train Loss : 0.0107\n",
            "Step : 1761, train Loss : 0.0087\n",
            "Step : 1771, train Loss : 0.0058\n",
            "Step : 1781, train Loss : 0.0068\n",
            "Step : 1791, train Loss : 0.0070\n",
            "Step : 1801, train Loss : 0.0090\n",
            "Step : 1811, train Loss : 0.0077\n",
            "Step : 1821, train Loss : 0.0087\n",
            "Step : 1831, train Loss : 0.0105\n",
            "Step : 1841, train Loss : 0.0062\n",
            "Step : 1851, train Loss : 0.0071\n",
            "Step : 1861, train Loss : 0.0064\n",
            "Step : 1871, train Loss : 0.0074\n",
            "Step : 1881, train Loss : 0.0073\n",
            "Step : 1891, train Loss : 0.0048\n",
            "Step : 1901, train Loss : 0.0106\n",
            "Step : 1911, train Loss : 0.0076\n",
            "Step : 1921, train Loss : 0.0117\n",
            "Step : 1931, train Loss : 0.0087\n",
            "Step : 1941, train Loss : 0.0060\n",
            "Step : 1951, train Loss : 0.0088\n",
            "Epoch 2 Total Mean Loss : 0.0082\n",
            "*****Epoch 2 Train Finish*****\n",
            "\n",
            "SAVING EPOCH 2 ...\n",
            "*****Epoch 2 Valid Start*****\n",
            "Step: 0,  Pearson: tensor([0.8564])\n",
            "Step: 1,  Pearson: tensor([0.9559])\n",
            "Step: 2,  Pearson: tensor([0.9564])\n",
            "Step: 3,  Pearson: tensor([0.9112])\n",
            "Step: 4,  Pearson: tensor([0.8419])\n",
            "Step: 5,  Pearson: tensor([0.9174])\n",
            "Step: 6,  Pearson: tensor([0.8920])\n",
            "Step: 7,  Pearson: tensor([0.9336])\n",
            "Step: 8,  Pearson: tensor([0.9506])\n",
            "Step: 9,  Pearson: tensor([0.9096])\n",
            "Step: 10,  Pearson: tensor([0.8700])\n",
            "Step: 11,  Pearson: tensor([0.9126])\n",
            "Step: 12,  Pearson: tensor([0.9003])\n",
            "Step: 13,  Pearson: tensor([0.8863])\n",
            "Step: 14,  Pearson: tensor([0.9483])\n",
            "Step: 15,  Pearson: tensor([0.9586])\n",
            "Step: 16,  Pearson: tensor([0.9570])\n",
            "Step: 17,  Pearson: tensor([0.9497])\n",
            "Step: 18,  Pearson: tensor([0.7385])\n",
            "Step: 19,  Pearson: tensor([0.9534])\n",
            "Step: 20,  Pearson: tensor([0.8883])\n",
            "Step: 21,  Pearson: tensor([0.8654])\n",
            "Step: 22,  Pearson: tensor([0.9403])\n",
            "Step: 23,  Pearson: tensor([0.7991])\n",
            "Step: 24,  Pearson: tensor([0.8511])\n",
            "Step: 25,  Pearson: tensor([0.9262])\n",
            "Step: 26,  Pearson: tensor([0.8819])\n",
            "Step: 27,  Pearson: tensor([0.9397])\n",
            "Step: 28,  Pearson: tensor([0.9632])\n",
            "Step: 29,  Pearson: tensor([0.8353])\n",
            "Step: 30,  Pearson: tensor([0.8730])\n",
            "Step: 31,  Pearson: tensor([0.9582])\n",
            "Step: 32,  Pearson: tensor([0.9142])\n",
            "Step: 33,  Pearson: tensor([0.8483])\n",
            "Step: 34,  Pearson: tensor([0.9686])\n",
            "Step: 35,  Pearson: tensor([0.8812])\n",
            "Step: 36,  Pearson: tensor([0.8915])\n",
            "Step: 37,  Pearson: tensor([0.8470])\n",
            "Step: 38,  Pearson: tensor([0.8921])\n",
            "Step: 39,  Pearson: tensor([0.9388])\n",
            "Step: 40,  Pearson: tensor([0.9451])\n",
            "Step: 41,  Pearson: tensor([0.9542])\n",
            "Step: 42,  Pearson: tensor([0.9007])\n",
            "Step: 43,  Pearson: tensor([0.8966])\n",
            "Step: 44,  Pearson: tensor([0.9711])\n",
            "Step: 45,  Pearson: tensor([0.9385])\n",
            "Step: 46,  Pearson: tensor([0.9110])\n",
            "Step: 47,  Pearson: tensor([0.9491])\n",
            "Step: 48,  Pearson: tensor([0.9157])\n",
            "Step: 49,  Pearson: tensor([0.8849])\n",
            "Step: 50,  Pearson: tensor([0.9456])\n",
            "Step: 51,  Pearson: tensor([0.9523])\n",
            "Step: 52,  Pearson: tensor([0.9126])\n",
            "Step: 53,  Pearson: tensor([0.9306])\n",
            "Step: 54,  Pearson: tensor([0.9613])\n",
            "Step: 55,  Pearson: tensor([0.9434])\n",
            "Step: 56,  Pearson: tensor([0.8919])\n",
            "Step: 57,  Pearson: tensor([0.9297])\n",
            "Step: 58,  Pearson: tensor([0.9330])\n",
            "Step: 59,  Pearson: tensor([0.9468])\n",
            "Step: 60,  Pearson: tensor([0.9787])\n",
            "Step: 61,  Pearson: tensor([0.9313])\n",
            "Step: 62,  Pearson: tensor([0.8017])\n",
            "Step: 63,  Pearson: tensor([0.9611])\n",
            "Step: 64,  Pearson: tensor([0.9435])\n",
            "Step: 65,  Pearson: tensor([0.9598])\n",
            "Step: 66,  Pearson: tensor([0.9309])\n",
            "Step: 67,  Pearson: tensor([0.9549])\n",
            "Step: 68,  Pearson: tensor([0.9629])\n",
            "Step: 69,  Pearson: tensor([0.8687])\n",
            "Step: 70,  Pearson: tensor([0.9077])\n",
            "Step: 71,  Pearson: tensor([0.9504])\n",
            "Step: 72,  Pearson: tensor([0.9777])\n",
            "Step: 73,  Pearson: tensor([0.8970])\n",
            "Step: 74,  Pearson: tensor([0.8964])\n",
            "Step: 75,  Pearson: tensor([0.9791])\n",
            "Step: 76,  Pearson: tensor([0.9371])\n",
            "Step: 77,  Pearson: tensor([0.9428])\n",
            "Step: 78,  Pearson: tensor([0.6759])\n",
            "Step: 79,  Pearson: tensor([0.9277])\n",
            "Step: 80,  Pearson: tensor([0.9252])\n",
            "Step: 81,  Pearson: tensor([0.9445])\n",
            "Step: 82,  Pearson: tensor([0.9270])\n",
            "Step: 83,  Pearson: tensor([0.9243])\n",
            "Step: 84,  Pearson: tensor([0.9722])\n",
            "Step: 85,  Pearson: tensor([0.8894])\n",
            "Step: 86,  Pearson: tensor([0.8797])\n",
            "Step: 87,  Pearson: tensor([0.9682])\n",
            "Step: 88,  Pearson: tensor([0.8909])\n",
            "Step: 89,  Pearson: tensor([0.8204])\n",
            "Step: 90,  Pearson: tensor([0.9537])\n",
            "Step: 91,  Pearson: tensor([0.9612])\n",
            "Step: 92,  Pearson: tensor([0.9372])\n",
            "Step: 93,  Pearson: tensor([0.9830])\n",
            "Step: 94,  Pearson: tensor([0.9250])\n",
            "Step: 95,  Pearson: tensor([0.9246])\n",
            "Step: 96,  Pearson: tensor([0.8737])\n",
            "Step: 97,  Pearson: tensor([0.9332])\n",
            "Step: 98,  Pearson: tensor([0.8970])\n",
            "Step: 99,  Pearson: tensor([0.9538])\n",
            "Step: 100,  Pearson: tensor([0.9417])\n",
            "Step: 101,  Pearson: tensor([0.9287])\n",
            "Step: 102,  Pearson: tensor([0.9274])\n",
            "Step: 103,  Pearson: tensor([0.9507])\n",
            "Step: 104,  Pearson: tensor([0.8459])\n",
            "Step: 105,  Pearson: tensor([0.8965])\n",
            "Step: 106,  Pearson: tensor([0.8979])\n",
            "Step: 107,  Pearson: tensor([0.8990])\n",
            "Step: 108,  Pearson: tensor([0.9712])\n",
            "total_valid_loss :  0.5464722262609989 total_f1_score :  0.9013927576601671 total_pearsonr : tensor([0.9171])\n",
            "Epoch 2 Valid Loss : 0.5464722262609989 Valid Pearsonr : tensor([0.9171]) ValidF1 : 0.9013927576601671\n",
            "*****Epoch 2 Valid Finish*****\n",
            "\n",
            "****** STARTING TO TRAIN EPOCH #3 ******\n",
            "Step : 11, train Loss : 0.0042\n",
            "Step : 21, train Loss : 0.0052\n",
            "Step : 31, train Loss : 0.0046\n",
            "Step : 41, train Loss : 0.0044\n",
            "Step : 51, train Loss : 0.0058\n",
            "Step : 61, train Loss : 0.0045\n",
            "Step : 71, train Loss : 0.0043\n",
            "Step : 81, train Loss : 0.0043\n",
            "Step : 91, train Loss : 0.0042\n",
            "Step : 101, train Loss : 0.0050\n",
            "Step : 111, train Loss : 0.0050\n",
            "Step : 121, train Loss : 0.0047\n",
            "Step : 131, train Loss : 0.0051\n",
            "Step : 141, train Loss : 0.0048\n",
            "Step : 151, train Loss : 0.0038\n",
            "Step : 161, train Loss : 0.0045\n",
            "Step : 171, train Loss : 0.0050\n",
            "Step : 181, train Loss : 0.0054\n",
            "Step : 191, train Loss : 0.0055\n",
            "Step : 201, train Loss : 0.0050\n",
            "Step : 211, train Loss : 0.0054\n",
            "Step : 221, train Loss : 0.0046\n",
            "Step : 231, train Loss : 0.0048\n",
            "Step : 241, train Loss : 0.0034\n",
            "Step : 251, train Loss : 0.0038\n",
            "Step : 261, train Loss : 0.0036\n",
            "Step : 271, train Loss : 0.0046\n",
            "Step : 281, train Loss : 0.0053\n",
            "Step : 291, train Loss : 0.0041\n",
            "Step : 301, train Loss : 0.0047\n",
            "Step : 311, train Loss : 0.0050\n",
            "Step : 321, train Loss : 0.0059\n",
            "Step : 331, train Loss : 0.0044\n",
            "Step : 341, train Loss : 0.0034\n",
            "Step : 351, train Loss : 0.0045\n",
            "Step : 361, train Loss : 0.0044\n",
            "Step : 371, train Loss : 0.0037\n",
            "Step : 381, train Loss : 0.0072\n",
            "Step : 391, train Loss : 0.0043\n",
            "Step : 401, train Loss : 0.0039\n",
            "Step : 411, train Loss : 0.0045\n",
            "Step : 421, train Loss : 0.0035\n",
            "Step : 431, train Loss : 0.0037\n",
            "Step : 441, train Loss : 0.0051\n",
            "Step : 451, train Loss : 0.0048\n",
            "Step : 461, train Loss : 0.0032\n",
            "Step : 471, train Loss : 0.0050\n",
            "Step : 481, train Loss : 0.0069\n",
            "Step : 491, train Loss : 0.0043\n",
            "Step : 501, train Loss : 0.0036\n",
            "Step : 511, train Loss : 0.0067\n",
            "Step : 521, train Loss : 0.0038\n",
            "Step : 531, train Loss : 0.0064\n",
            "Step : 541, train Loss : 0.0057\n",
            "Step : 551, train Loss : 0.0048\n",
            "Step : 561, train Loss : 0.0060\n",
            "Step : 571, train Loss : 0.0031\n",
            "Step : 581, train Loss : 0.0047\n",
            "Step : 591, train Loss : 0.0039\n",
            "Step : 601, train Loss : 0.0039\n",
            "Step : 611, train Loss : 0.0044\n",
            "Step : 621, train Loss : 0.0042\n",
            "Step : 631, train Loss : 0.0046\n",
            "Step : 641, train Loss : 0.0043\n",
            "Step : 651, train Loss : 0.0045\n",
            "Step : 661, train Loss : 0.0052\n",
            "Step : 671, train Loss : 0.0037\n",
            "Step : 681, train Loss : 0.0060\n",
            "Step : 691, train Loss : 0.0038\n",
            "Step : 701, train Loss : 0.0044\n",
            "Step : 711, train Loss : 0.0069\n",
            "Step : 721, train Loss : 0.0032\n",
            "Step : 731, train Loss : 0.0046\n",
            "Step : 741, train Loss : 0.0050\n",
            "Step : 751, train Loss : 0.0042\n",
            "Step : 761, train Loss : 0.0035\n",
            "Step : 771, train Loss : 0.0056\n",
            "Step : 781, train Loss : 0.0047\n",
            "Step : 791, train Loss : 0.0051\n",
            "Step : 801, train Loss : 0.0028\n",
            "Step : 811, train Loss : 0.0050\n",
            "Step : 821, train Loss : 0.0053\n",
            "Step : 831, train Loss : 0.0041\n",
            "Step : 841, train Loss : 0.0041\n",
            "Step : 851, train Loss : 0.0039\n",
            "Step : 861, train Loss : 0.0065\n",
            "Step : 871, train Loss : 0.0038\n",
            "Step : 881, train Loss : 0.0055\n",
            "Step : 891, train Loss : 0.0046\n",
            "Step : 901, train Loss : 0.0048\n",
            "Step : 911, train Loss : 0.0051\n",
            "Step : 921, train Loss : 0.0044\n",
            "Step : 931, train Loss : 0.0037\n",
            "Step : 941, train Loss : 0.0069\n",
            "Step : 951, train Loss : 0.0061\n",
            "Step : 961, train Loss : 0.0058\n",
            "Step : 971, train Loss : 0.0039\n",
            "Step : 981, train Loss : 0.0054\n",
            "Step : 991, train Loss : 0.0046\n",
            "Step : 1001, train Loss : 0.0040\n",
            "Step : 1011, train Loss : 0.0046\n",
            "Step : 1021, train Loss : 0.0044\n",
            "Step : 1031, train Loss : 0.0041\n",
            "Step : 1041, train Loss : 0.0045\n",
            "Step : 1051, train Loss : 0.0038\n",
            "Step : 1061, train Loss : 0.0043\n",
            "Step : 1071, train Loss : 0.0040\n",
            "Step : 1081, train Loss : 0.0034\n",
            "Step : 1091, train Loss : 0.0058\n",
            "Step : 1101, train Loss : 0.0038\n",
            "Step : 1111, train Loss : 0.0045\n",
            "Step : 1121, train Loss : 0.0040\n",
            "Step : 1131, train Loss : 0.0043\n",
            "Step : 1141, train Loss : 0.0035\n",
            "Step : 1151, train Loss : 0.0036\n",
            "Step : 1161, train Loss : 0.0053\n",
            "Step : 1171, train Loss : 0.0043\n",
            "Step : 1181, train Loss : 0.0039\n",
            "Step : 1191, train Loss : 0.0058\n",
            "Step : 1201, train Loss : 0.0034\n",
            "Step : 1211, train Loss : 0.0038\n",
            "Step : 1221, train Loss : 0.0053\n",
            "Step : 1231, train Loss : 0.0061\n",
            "Step : 1241, train Loss : 0.0042\n",
            "Step : 1251, train Loss : 0.0042\n",
            "Step : 1261, train Loss : 0.0054\n",
            "Step : 1271, train Loss : 0.0060\n",
            "Step : 1281, train Loss : 0.0047\n",
            "Step : 1291, train Loss : 0.0043\n",
            "Step : 1301, train Loss : 0.0043\n",
            "Step : 1311, train Loss : 0.0055\n",
            "Step : 1321, train Loss : 0.0055\n",
            "Step : 1331, train Loss : 0.0044\n",
            "Step : 1341, train Loss : 0.0041\n",
            "Step : 1351, train Loss : 0.0045\n",
            "Step : 1361, train Loss : 0.0050\n",
            "Step : 1371, train Loss : 0.0048\n",
            "Step : 1381, train Loss : 0.0056\n",
            "Step : 1391, train Loss : 0.0039\n",
            "Step : 1401, train Loss : 0.0044\n",
            "Step : 1411, train Loss : 0.0056\n",
            "Step : 1421, train Loss : 0.0056\n",
            "Step : 1431, train Loss : 0.0043\n",
            "Step : 1441, train Loss : 0.0047\n",
            "Step : 1451, train Loss : 0.0052\n",
            "Step : 1461, train Loss : 0.0047\n",
            "Step : 1471, train Loss : 0.0057\n",
            "Step : 1481, train Loss : 0.0030\n",
            "Step : 1491, train Loss : 0.0030\n",
            "Step : 1501, train Loss : 0.0048\n",
            "Step : 1511, train Loss : 0.0048\n",
            "Step : 1521, train Loss : 0.0054\n",
            "Step : 1531, train Loss : 0.0038\n",
            "Step : 1541, train Loss : 0.0039\n",
            "Step : 1551, train Loss : 0.0054\n",
            "Step : 1561, train Loss : 0.0043\n",
            "Step : 1571, train Loss : 0.0040\n",
            "Step : 1581, train Loss : 0.0042\n",
            "Step : 1591, train Loss : 0.0039\n",
            "Step : 1601, train Loss : 0.0059\n",
            "Step : 1611, train Loss : 0.0048\n",
            "Step : 1621, train Loss : 0.0033\n",
            "Step : 1631, train Loss : 0.0045\n",
            "Step : 1641, train Loss : 0.0037\n",
            "Step : 1651, train Loss : 0.0049\n",
            "Step : 1661, train Loss : 0.0048\n",
            "Step : 1671, train Loss : 0.0030\n",
            "Step : 1681, train Loss : 0.0030\n",
            "Step : 1691, train Loss : 0.0024\n",
            "Step : 1701, train Loss : 0.0039\n",
            "Step : 1711, train Loss : 0.0035\n",
            "Step : 1721, train Loss : 0.0075\n",
            "Step : 1731, train Loss : 0.0045\n",
            "Step : 1741, train Loss : 0.0051\n",
            "Step : 1751, train Loss : 0.0036\n",
            "Step : 1761, train Loss : 0.0037\n",
            "Step : 1771, train Loss : 0.0036\n",
            "Step : 1781, train Loss : 0.0046\n",
            "Step : 1791, train Loss : 0.0050\n",
            "Step : 1801, train Loss : 0.0039\n",
            "Step : 1811, train Loss : 0.0056\n",
            "Step : 1821, train Loss : 0.0043\n",
            "Step : 1831, train Loss : 0.0040\n",
            "Step : 1841, train Loss : 0.0037\n",
            "Step : 1851, train Loss : 0.0046\n",
            "Step : 1861, train Loss : 0.0052\n",
            "Step : 1871, train Loss : 0.0037\n",
            "Step : 1881, train Loss : 0.0055\n",
            "Step : 1891, train Loss : 0.0051\n",
            "Step : 1901, train Loss : 0.0047\n",
            "Step : 1911, train Loss : 0.0037\n",
            "Step : 1921, train Loss : 0.0040\n",
            "Step : 1931, train Loss : 0.0045\n",
            "Step : 1941, train Loss : 0.0063\n",
            "Step : 1951, train Loss : 0.0059\n",
            "Epoch 3 Total Mean Loss : 0.0046\n",
            "*****Epoch 3 Train Finish*****\n",
            "\n",
            "SAVING EPOCH 3 ...\n",
            "*****Epoch 3 Valid Start*****\n",
            "Step: 0,  Pearson: tensor([0.8788])\n",
            "Step: 1,  Pearson: tensor([0.9453])\n",
            "Step: 2,  Pearson: tensor([0.9592])\n",
            "Step: 3,  Pearson: tensor([0.8971])\n",
            "Step: 4,  Pearson: tensor([0.8535])\n",
            "Step: 5,  Pearson: tensor([0.9371])\n",
            "Step: 6,  Pearson: tensor([0.8982])\n",
            "Step: 7,  Pearson: tensor([0.9433])\n",
            "Step: 8,  Pearson: tensor([0.9244])\n",
            "Step: 9,  Pearson: tensor([0.9254])\n",
            "Step: 10,  Pearson: tensor([0.8618])\n",
            "Step: 11,  Pearson: tensor([0.9107])\n",
            "Step: 12,  Pearson: tensor([0.8759])\n",
            "Step: 13,  Pearson: tensor([0.8970])\n",
            "Step: 14,  Pearson: tensor([0.9437])\n",
            "Step: 15,  Pearson: tensor([0.9597])\n",
            "Step: 16,  Pearson: tensor([0.9640])\n",
            "Step: 17,  Pearson: tensor([0.9594])\n",
            "Step: 18,  Pearson: tensor([0.7395])\n",
            "Step: 19,  Pearson: tensor([0.9307])\n",
            "Step: 20,  Pearson: tensor([0.9117])\n",
            "Step: 21,  Pearson: tensor([0.8919])\n",
            "Step: 22,  Pearson: tensor([0.9402])\n",
            "Step: 23,  Pearson: tensor([0.7963])\n",
            "Step: 24,  Pearson: tensor([0.8564])\n",
            "Step: 25,  Pearson: tensor([0.9427])\n",
            "Step: 26,  Pearson: tensor([0.8880])\n",
            "Step: 27,  Pearson: tensor([0.9426])\n",
            "Step: 28,  Pearson: tensor([0.9621])\n",
            "Step: 29,  Pearson: tensor([0.8535])\n",
            "Step: 30,  Pearson: tensor([0.8722])\n",
            "Step: 31,  Pearson: tensor([0.9544])\n",
            "Step: 32,  Pearson: tensor([0.9182])\n",
            "Step: 33,  Pearson: tensor([0.8862])\n",
            "Step: 34,  Pearson: tensor([0.9652])\n",
            "Step: 35,  Pearson: tensor([0.8924])\n",
            "Step: 36,  Pearson: tensor([0.9041])\n",
            "Step: 37,  Pearson: tensor([0.8715])\n",
            "Step: 38,  Pearson: tensor([0.8893])\n",
            "Step: 39,  Pearson: tensor([0.9421])\n",
            "Step: 40,  Pearson: tensor([0.9453])\n",
            "Step: 41,  Pearson: tensor([0.9459])\n",
            "Step: 42,  Pearson: tensor([0.8970])\n",
            "Step: 43,  Pearson: tensor([0.9000])\n",
            "Step: 44,  Pearson: tensor([0.9799])\n",
            "Step: 45,  Pearson: tensor([0.9354])\n",
            "Step: 46,  Pearson: tensor([0.9210])\n",
            "Step: 47,  Pearson: tensor([0.9542])\n",
            "Step: 48,  Pearson: tensor([0.9108])\n",
            "Step: 49,  Pearson: tensor([0.9056])\n",
            "Step: 50,  Pearson: tensor([0.9297])\n",
            "Step: 51,  Pearson: tensor([0.9577])\n",
            "Step: 52,  Pearson: tensor([0.9212])\n",
            "Step: 53,  Pearson: tensor([0.9227])\n",
            "Step: 54,  Pearson: tensor([0.9605])\n",
            "Step: 55,  Pearson: tensor([0.9402])\n",
            "Step: 56,  Pearson: tensor([0.8939])\n",
            "Step: 57,  Pearson: tensor([0.9459])\n",
            "Step: 58,  Pearson: tensor([0.9342])\n",
            "Step: 59,  Pearson: tensor([0.9187])\n",
            "Step: 60,  Pearson: tensor([0.9828])\n",
            "Step: 61,  Pearson: tensor([0.9203])\n",
            "Step: 62,  Pearson: tensor([0.8328])\n",
            "Step: 63,  Pearson: tensor([0.9694])\n",
            "Step: 64,  Pearson: tensor([0.9415])\n",
            "Step: 65,  Pearson: tensor([0.9582])\n",
            "Step: 66,  Pearson: tensor([0.9091])\n",
            "Step: 67,  Pearson: tensor([0.9585])\n",
            "Step: 68,  Pearson: tensor([0.9625])\n",
            "Step: 69,  Pearson: tensor([0.8910])\n",
            "Step: 70,  Pearson: tensor([0.9207])\n",
            "Step: 71,  Pearson: tensor([0.9365])\n",
            "Step: 72,  Pearson: tensor([0.9732])\n",
            "Step: 73,  Pearson: tensor([0.9002])\n",
            "Step: 74,  Pearson: tensor([0.8893])\n",
            "Step: 75,  Pearson: tensor([0.9821])\n",
            "Step: 76,  Pearson: tensor([0.9573])\n",
            "Step: 77,  Pearson: tensor([0.9471])\n",
            "Step: 78,  Pearson: tensor([0.6848])\n",
            "Step: 79,  Pearson: tensor([0.9362])\n",
            "Step: 80,  Pearson: tensor([0.9378])\n",
            "Step: 81,  Pearson: tensor([0.9459])\n",
            "Step: 82,  Pearson: tensor([0.9193])\n",
            "Step: 83,  Pearson: tensor([0.9121])\n",
            "Step: 84,  Pearson: tensor([0.9749])\n",
            "Step: 85,  Pearson: tensor([0.8951])\n",
            "Step: 86,  Pearson: tensor([0.8826])\n",
            "Step: 87,  Pearson: tensor([0.9780])\n",
            "Step: 88,  Pearson: tensor([0.8960])\n",
            "Step: 89,  Pearson: tensor([0.7992])\n",
            "Step: 90,  Pearson: tensor([0.9522])\n",
            "Step: 91,  Pearson: tensor([0.9657])\n",
            "Step: 92,  Pearson: tensor([0.9178])\n",
            "Step: 93,  Pearson: tensor([0.9839])\n",
            "Step: 94,  Pearson: tensor([0.9312])\n",
            "Step: 95,  Pearson: tensor([0.9142])\n",
            "Step: 96,  Pearson: tensor([0.8865])\n",
            "Step: 97,  Pearson: tensor([0.9261])\n",
            "Step: 98,  Pearson: tensor([0.8899])\n",
            "Step: 99,  Pearson: tensor([0.9609])\n",
            "Step: 100,  Pearson: tensor([0.9448])\n",
            "Step: 101,  Pearson: tensor([0.9363])\n",
            "Step: 102,  Pearson: tensor([0.9146])\n",
            "Step: 103,  Pearson: tensor([0.9525])\n",
            "Step: 104,  Pearson: tensor([0.8469])\n",
            "Step: 105,  Pearson: tensor([0.8783])\n",
            "Step: 106,  Pearson: tensor([0.9223])\n",
            "Step: 107,  Pearson: tensor([0.8973])\n",
            "Step: 108,  Pearson: tensor([0.9742])\n",
            "total_valid_loss :  0.5391688895061475 total_f1_score :  0.8986636971046771 total_pearsonr : tensor([0.9188])\n",
            "Epoch 3 Valid Loss : 0.5391688895061475 Valid Pearsonr : tensor([0.9188]) ValidF1 : 0.8986636971046771\n",
            "*****Epoch 3 Valid Finish*****\n",
            "\n",
            "** Train Completed! **\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9dc39149f29e4e549a3115d7f3590cae"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▆█</td></tr><tr><td>total_f1_score </td><td>▁█▇▅</td></tr><tr><td>total_pearsonr</td><td>▁▄▇█</td></tr><tr><td>total_train_loss</td><td>█▄▂▁</td></tr><tr><td>total_train_lr</td><td>█▆▃▁</td></tr><tr><td>total_valid_loss</td><td>█▄▂▁</td></tr><tr><td>train_loss</td><td>█▄▅▄▄▄▅▄▄▄▃▂▂▂▂▂▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▂▁▁▁▁▁▁</td></tr><tr><td>train_lr</td><td>▂▃▅▇███▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>4</td></tr><tr><td>total_f1_score </td><td>0.89866</td></tr><tr><td>total_pearsonr</td><td>0.91885</td></tr><tr><td>total_train_loss</td><td>0.00459</td></tr><tr><td>total_train_lr</td><td>0.0</td></tr><tr><td>total_valid_loss</td><td>0.53917</td></tr><tr><td>train_loss</td><td>0.00585</td></tr><tr><td>train_lr</td><td>0.0</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">fluent-sweep-4</strong>: <a href=\"https://wandb.ai/kdb/sts_v2/runs/n6ddh39r\" target=\"_blank\">https://wandb.ai/kdb/sts_v2/runs/n6ddh39r</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20220529_183159-n6ddh39r/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: f2xjgp2n with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \teps: 1e-08\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 3e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_batch_size: 8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tvalid_batch_size: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twarm_up_ratio: 0.1\n",
            "Some weights of the model checkpoint at klue/roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.17"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/NLP/wandb/run-20220529_200355-f2xjgp2n</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/kdb/sts_v2/runs/f2xjgp2n\" target=\"_blank\">vocal-sweep-5</a></strong> to <a href=\"https://wandb.ai/kdb/sts_v2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/kdb/sts_v2/sweeps/sy73kc4x\" target=\"_blank\">https://wandb.ai/kdb/sts_v2/sweeps/sy73kc4x</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "****** STARTING TO TRAIN EPOCH #0 ******\n",
            "Step : 11, train Loss : 0.1763\n",
            "Step : 21, train Loss : 0.1691\n",
            "Step : 31, train Loss : 0.1963\n",
            "Step : 41, train Loss : 0.1863\n",
            "Step : 51, train Loss : 0.1754\n",
            "Step : 61, train Loss : 0.1333\n",
            "Step : 71, train Loss : 0.1065\n",
            "Step : 81, train Loss : 0.0870\n",
            "Step : 91, train Loss : 0.0627\n",
            "Step : 101, train Loss : 0.0535\n",
            "Step : 111, train Loss : 0.0541\n",
            "Step : 121, train Loss : 0.0430\n",
            "Step : 131, train Loss : 0.0406\n",
            "Step : 141, train Loss : 0.0541\n",
            "Step : 151, train Loss : 0.0435\n",
            "Step : 161, train Loss : 0.0331\n",
            "Step : 171, train Loss : 0.0294\n",
            "Step : 181, train Loss : 0.0349\n",
            "Step : 191, train Loss : 0.0415\n",
            "Step : 201, train Loss : 0.0361\n",
            "Step : 211, train Loss : 0.0316\n",
            "Step : 221, train Loss : 0.0297\n",
            "Step : 231, train Loss : 0.0228\n",
            "Step : 241, train Loss : 0.0328\n",
            "Step : 251, train Loss : 0.0363\n",
            "Step : 261, train Loss : 0.0248\n",
            "Step : 271, train Loss : 0.0280\n",
            "Step : 281, train Loss : 0.0206\n",
            "Step : 291, train Loss : 0.0375\n",
            "Step : 301, train Loss : 0.0268\n",
            "Step : 311, train Loss : 0.0181\n",
            "Step : 321, train Loss : 0.0138\n",
            "Step : 331, train Loss : 0.0265\n",
            "Step : 341, train Loss : 0.0274\n",
            "Step : 351, train Loss : 0.0291\n",
            "Step : 361, train Loss : 0.0305\n",
            "Step : 371, train Loss : 0.0214\n",
            "Step : 381, train Loss : 0.0383\n",
            "Step : 391, train Loss : 0.0183\n",
            "Step : 401, train Loss : 0.0212\n",
            "Step : 411, train Loss : 0.0365\n",
            "Step : 421, train Loss : 0.0213\n",
            "Step : 431, train Loss : 0.0258\n",
            "Step : 441, train Loss : 0.0272\n",
            "Step : 451, train Loss : 0.0275\n",
            "Step : 461, train Loss : 0.0267\n",
            "Step : 471, train Loss : 0.0244\n",
            "Step : 481, train Loss : 0.0329\n",
            "Step : 491, train Loss : 0.0194\n",
            "Step : 501, train Loss : 0.0227\n",
            "Step : 511, train Loss : 0.0250\n",
            "Step : 521, train Loss : 0.0357\n",
            "Step : 531, train Loss : 0.0219\n",
            "Step : 541, train Loss : 0.0193\n",
            "Step : 551, train Loss : 0.0179\n",
            "Step : 561, train Loss : 0.0260\n",
            "Step : 571, train Loss : 0.0202\n",
            "Step : 581, train Loss : 0.0194\n",
            "Step : 591, train Loss : 0.0204\n",
            "Step : 601, train Loss : 0.0195\n",
            "Step : 611, train Loss : 0.0237\n",
            "Step : 621, train Loss : 0.0228\n",
            "Step : 631, train Loss : 0.0260\n",
            "Step : 641, train Loss : 0.0204\n",
            "Step : 651, train Loss : 0.0179\n",
            "Step : 661, train Loss : 0.0226\n",
            "Step : 671, train Loss : 0.0299\n",
            "Step : 681, train Loss : 0.0221\n",
            "Step : 691, train Loss : 0.0182\n",
            "Step : 701, train Loss : 0.0291\n",
            "Step : 711, train Loss : 0.0241\n",
            "Step : 721, train Loss : 0.0202\n",
            "Step : 731, train Loss : 0.0242\n",
            "Step : 741, train Loss : 0.0267\n",
            "Step : 751, train Loss : 0.0189\n",
            "Step : 761, train Loss : 0.0284\n",
            "Step : 771, train Loss : 0.0160\n",
            "Step : 781, train Loss : 0.0222\n",
            "Step : 791, train Loss : 0.0271\n",
            "Step : 801, train Loss : 0.0305\n",
            "Step : 811, train Loss : 0.0257\n",
            "Step : 821, train Loss : 0.0345\n",
            "Step : 831, train Loss : 0.0236\n",
            "Step : 841, train Loss : 0.0204\n",
            "Step : 851, train Loss : 0.0185\n",
            "Step : 861, train Loss : 0.0278\n",
            "Step : 871, train Loss : 0.0321\n",
            "Step : 881, train Loss : 0.0340\n",
            "Step : 891, train Loss : 0.0245\n",
            "Step : 901, train Loss : 0.0227\n",
            "Step : 911, train Loss : 0.0336\n",
            "Step : 921, train Loss : 0.0225\n",
            "Step : 931, train Loss : 0.0251\n",
            "Step : 941, train Loss : 0.0256\n",
            "Step : 951, train Loss : 0.0255\n",
            "Step : 961, train Loss : 0.0275\n",
            "Step : 971, train Loss : 0.0271\n",
            "Step : 981, train Loss : 0.0197\n",
            "Step : 991, train Loss : 0.0224\n",
            "Step : 1001, train Loss : 0.0198\n",
            "Step : 1011, train Loss : 0.0270\n",
            "Step : 1021, train Loss : 0.0153\n",
            "Step : 1031, train Loss : 0.0245\n",
            "Step : 1041, train Loss : 0.0192\n",
            "Step : 1051, train Loss : 0.0213\n",
            "Step : 1061, train Loss : 0.0219\n",
            "Step : 1071, train Loss : 0.0230\n",
            "Step : 1081, train Loss : 0.0275\n",
            "Step : 1091, train Loss : 0.0316\n",
            "Step : 1101, train Loss : 0.0270\n",
            "Step : 1111, train Loss : 0.0298\n",
            "Step : 1121, train Loss : 0.0217\n",
            "Step : 1131, train Loss : 0.0257\n",
            "Step : 1141, train Loss : 0.0240\n",
            "Step : 1151, train Loss : 0.0184\n",
            "Step : 1161, train Loss : 0.0205\n",
            "Step : 1171, train Loss : 0.0261\n",
            "Step : 1181, train Loss : 0.0244\n",
            "Step : 1191, train Loss : 0.0202\n",
            "Step : 1201, train Loss : 0.0172\n",
            "Step : 1211, train Loss : 0.0195\n",
            "Step : 1221, train Loss : 0.0367\n",
            "Step : 1231, train Loss : 0.0163\n",
            "Step : 1241, train Loss : 0.0195\n",
            "Step : 1251, train Loss : 0.0208\n",
            "Step : 1261, train Loss : 0.0234\n",
            "Step : 1271, train Loss : 0.0146\n",
            "Step : 1281, train Loss : 0.0257\n",
            "Step : 1291, train Loss : 0.0196\n",
            "Step : 1301, train Loss : 0.0182\n",
            "Step : 1311, train Loss : 0.0207\n",
            "Step : 1321, train Loss : 0.0146\n",
            "Step : 1331, train Loss : 0.0183\n",
            "Step : 1341, train Loss : 0.0254\n",
            "Step : 1351, train Loss : 0.0254\n",
            "Step : 1361, train Loss : 0.0247\n",
            "Step : 1371, train Loss : 0.0201\n",
            "Step : 1381, train Loss : 0.0188\n",
            "Step : 1391, train Loss : 0.0279\n",
            "Step : 1401, train Loss : 0.0235\n",
            "Step : 1411, train Loss : 0.0271\n",
            "Step : 1421, train Loss : 0.0174\n",
            "Step : 1431, train Loss : 0.0200\n",
            "Step : 1441, train Loss : 0.0220\n",
            "Step : 1451, train Loss : 0.0263\n",
            "Step : 1461, train Loss : 0.0162\n",
            "Step : 1471, train Loss : 0.0207\n",
            "Step : 1481, train Loss : 0.0233\n",
            "Step : 1491, train Loss : 0.0150\n",
            "Step : 1501, train Loss : 0.0262\n",
            "Step : 1511, train Loss : 0.0239\n",
            "Step : 1521, train Loss : 0.0185\n",
            "Step : 1531, train Loss : 0.0179\n",
            "Step : 1541, train Loss : 0.0257\n",
            "Step : 1551, train Loss : 0.0214\n",
            "Step : 1561, train Loss : 0.0304\n",
            "Step : 1571, train Loss : 0.0314\n",
            "Step : 1581, train Loss : 0.0217\n",
            "Step : 1591, train Loss : 0.0273\n",
            "Step : 1601, train Loss : 0.0211\n",
            "Step : 1611, train Loss : 0.0202\n",
            "Step : 1621, train Loss : 0.0209\n",
            "Step : 1631, train Loss : 0.0216\n",
            "Step : 1641, train Loss : 0.0249\n",
            "Step : 1651, train Loss : 0.0226\n",
            "Step : 1661, train Loss : 0.0176\n",
            "Step : 1671, train Loss : 0.0208\n",
            "Step : 1681, train Loss : 0.0211\n",
            "Step : 1691, train Loss : 0.0162\n",
            "Step : 1701, train Loss : 0.0180\n",
            "Step : 1711, train Loss : 0.0212\n",
            "Step : 1721, train Loss : 0.0171\n",
            "Step : 1731, train Loss : 0.0178\n",
            "Step : 1741, train Loss : 0.0155\n",
            "Step : 1751, train Loss : 0.0196\n",
            "Step : 1761, train Loss : 0.0217\n",
            "Step : 1771, train Loss : 0.0227\n",
            "Step : 1781, train Loss : 0.0205\n",
            "Step : 1791, train Loss : 0.0149\n",
            "Step : 1801, train Loss : 0.0209\n",
            "Step : 1811, train Loss : 0.0239\n",
            "Step : 1821, train Loss : 0.0140\n",
            "Step : 1831, train Loss : 0.0255\n",
            "Step : 1841, train Loss : 0.0344\n",
            "Step : 1851, train Loss : 0.0195\n",
            "Step : 1861, train Loss : 0.0226\n",
            "Step : 1871, train Loss : 0.0197\n",
            "Step : 1881, train Loss : 0.0175\n",
            "Step : 1891, train Loss : 0.0170\n",
            "Step : 1901, train Loss : 0.0326\n",
            "Step : 1911, train Loss : 0.0312\n",
            "Step : 1921, train Loss : 0.0239\n",
            "Step : 1931, train Loss : 0.0170\n",
            "Step : 1941, train Loss : 0.0197\n",
            "Step : 1951, train Loss : 0.0233\n",
            "Epoch 0 Total Mean Loss : 0.0301\n",
            "*****Epoch 0 Train Finish*****\n",
            "\n",
            "SAVING EPOCH 0 ...\n",
            "*****Epoch 0 Valid Start*****\n",
            "Step: 0,  Pearson: tensor([0.8714])\n",
            "Step: 1,  Pearson: tensor([0.8739])\n",
            "Step: 2,  Pearson: tensor([0.9435])\n",
            "Step: 3,  Pearson: tensor([0.8321])\n",
            "Step: 4,  Pearson: tensor([0.9028])\n",
            "Step: 5,  Pearson: tensor([0.9662])\n",
            "Step: 6,  Pearson: tensor([0.9060])\n",
            "Step: 7,  Pearson: tensor([0.8823])\n",
            "Step: 8,  Pearson: tensor([0.9402])\n",
            "Step: 9,  Pearson: tensor([0.9371])\n",
            "Step: 10,  Pearson: tensor([0.8073])\n",
            "Step: 11,  Pearson: tensor([0.9146])\n",
            "Step: 12,  Pearson: tensor([0.8629])\n",
            "Step: 13,  Pearson: tensor([0.9100])\n",
            "Step: 14,  Pearson: tensor([0.9545])\n",
            "Step: 15,  Pearson: tensor([0.9239])\n",
            "Step: 16,  Pearson: tensor([0.9636])\n",
            "Step: 17,  Pearson: tensor([0.9690])\n",
            "Step: 18,  Pearson: tensor([0.8149])\n",
            "Step: 19,  Pearson: tensor([0.9384])\n",
            "Step: 20,  Pearson: tensor([0.9382])\n",
            "Step: 21,  Pearson: tensor([0.9256])\n",
            "Step: 22,  Pearson: tensor([0.8994])\n",
            "Step: 23,  Pearson: tensor([0.6813])\n",
            "Step: 24,  Pearson: tensor([0.8761])\n",
            "Step: 25,  Pearson: tensor([0.9432])\n",
            "Step: 26,  Pearson: tensor([0.9399])\n",
            "Step: 27,  Pearson: tensor([0.9214])\n",
            "Step: 28,  Pearson: tensor([0.9192])\n",
            "Step: 29,  Pearson: tensor([0.8628])\n",
            "Step: 30,  Pearson: tensor([0.9228])\n",
            "Step: 31,  Pearson: tensor([0.9680])\n",
            "Step: 32,  Pearson: tensor([0.8735])\n",
            "Step: 33,  Pearson: tensor([0.8187])\n",
            "Step: 34,  Pearson: tensor([0.9555])\n",
            "Step: 35,  Pearson: tensor([0.9170])\n",
            "Step: 36,  Pearson: tensor([0.8323])\n",
            "Step: 37,  Pearson: tensor([0.8821])\n",
            "Step: 38,  Pearson: tensor([0.8656])\n",
            "Step: 39,  Pearson: tensor([0.9383])\n",
            "Step: 40,  Pearson: tensor([0.9426])\n",
            "Step: 41,  Pearson: tensor([0.9277])\n",
            "Step: 42,  Pearson: tensor([0.8291])\n",
            "Step: 43,  Pearson: tensor([0.8878])\n",
            "Step: 44,  Pearson: tensor([0.9808])\n",
            "Step: 45,  Pearson: tensor([0.8781])\n",
            "Step: 46,  Pearson: tensor([0.8871])\n",
            "Step: 47,  Pearson: tensor([0.9625])\n",
            "Step: 48,  Pearson: tensor([0.8745])\n",
            "Step: 49,  Pearson: tensor([0.9003])\n",
            "Step: 50,  Pearson: tensor([0.9390])\n",
            "Step: 51,  Pearson: tensor([0.9309])\n",
            "Step: 52,  Pearson: tensor([0.9126])\n",
            "Step: 53,  Pearson: tensor([0.8817])\n",
            "Step: 54,  Pearson: tensor([0.9449])\n",
            "Step: 55,  Pearson: tensor([0.9308])\n",
            "Step: 56,  Pearson: tensor([0.8188])\n",
            "Step: 57,  Pearson: tensor([0.9754])\n",
            "Step: 58,  Pearson: tensor([0.9415])\n",
            "Step: 59,  Pearson: tensor([0.9371])\n",
            "Step: 60,  Pearson: tensor([0.9476])\n",
            "Step: 61,  Pearson: tensor([0.9341])\n",
            "Step: 62,  Pearson: tensor([0.7467])\n",
            "Step: 63,  Pearson: tensor([0.9361])\n",
            "Step: 64,  Pearson: tensor([0.9182])\n",
            "Step: 65,  Pearson: tensor([0.9426])\n",
            "Step: 66,  Pearson: tensor([0.9320])\n",
            "Step: 67,  Pearson: tensor([0.9442])\n",
            "Step: 68,  Pearson: tensor([0.9352])\n",
            "Step: 69,  Pearson: tensor([0.9038])\n",
            "Step: 70,  Pearson: tensor([0.8463])\n",
            "Step: 71,  Pearson: tensor([0.9037])\n",
            "Step: 72,  Pearson: tensor([0.9406])\n",
            "Step: 73,  Pearson: tensor([0.9097])\n",
            "Step: 74,  Pearson: tensor([0.8572])\n",
            "Step: 75,  Pearson: tensor([0.9642])\n",
            "Step: 76,  Pearson: tensor([0.9280])\n",
            "Step: 77,  Pearson: tensor([0.9340])\n",
            "Step: 78,  Pearson: tensor([0.7351])\n",
            "Step: 79,  Pearson: tensor([0.8947])\n",
            "Step: 80,  Pearson: tensor([0.9172])\n",
            "Step: 81,  Pearson: tensor([0.9353])\n",
            "Step: 82,  Pearson: tensor([0.9048])\n",
            "Step: 83,  Pearson: tensor([0.8728])\n",
            "Step: 84,  Pearson: tensor([0.9759])\n",
            "Step: 85,  Pearson: tensor([0.8959])\n",
            "Step: 86,  Pearson: tensor([0.8513])\n",
            "Step: 87,  Pearson: tensor([0.9750])\n",
            "Step: 88,  Pearson: tensor([0.8762])\n",
            "Step: 89,  Pearson: tensor([0.8018])\n",
            "Step: 90,  Pearson: tensor([0.9338])\n",
            "Step: 91,  Pearson: tensor([0.9246])\n",
            "Step: 92,  Pearson: tensor([0.9025])\n",
            "Step: 93,  Pearson: tensor([0.9680])\n",
            "Step: 94,  Pearson: tensor([0.9351])\n",
            "Step: 95,  Pearson: tensor([0.8527])\n",
            "Step: 96,  Pearson: tensor([0.9062])\n",
            "Step: 97,  Pearson: tensor([0.9351])\n",
            "Step: 98,  Pearson: tensor([0.8466])\n",
            "Step: 99,  Pearson: tensor([0.9328])\n",
            "Step: 100,  Pearson: tensor([0.9266])\n",
            "Step: 101,  Pearson: tensor([0.8632])\n",
            "Step: 102,  Pearson: tensor([0.8630])\n",
            "Step: 103,  Pearson: tensor([0.9012])\n",
            "Step: 104,  Pearson: tensor([0.8786])\n",
            "Step: 105,  Pearson: tensor([0.8662])\n",
            "Step: 106,  Pearson: tensor([0.9363])\n",
            "Step: 107,  Pearson: tensor([0.9060])\n",
            "Step: 108,  Pearson: tensor([0.9735])\n",
            "total_valid_loss :  0.6131756173087917 total_f1_score :  0.8950892857142858 total_pearsonr : tensor([0.9056])\n",
            "Epoch 0 Valid Loss : 0.6131756173087917 Valid Pearsonr : tensor([0.9056]) ValidF1 : 0.8950892857142858\n",
            "*****Epoch 0 Valid Finish*****\n",
            "\n",
            "****** STARTING TO TRAIN EPOCH #1 ******\n",
            "Step : 11, train Loss : 0.0097\n",
            "Step : 21, train Loss : 0.0206\n",
            "Step : 31, train Loss : 0.0136\n",
            "Step : 41, train Loss : 0.0217\n",
            "Step : 51, train Loss : 0.0127\n",
            "Step : 61, train Loss : 0.0144\n",
            "Step : 71, train Loss : 0.0126\n",
            "Step : 81, train Loss : 0.0114\n",
            "Step : 91, train Loss : 0.0134\n",
            "Step : 101, train Loss : 0.0122\n",
            "Step : 111, train Loss : 0.0142\n",
            "Step : 121, train Loss : 0.0157\n",
            "Step : 131, train Loss : 0.0119\n",
            "Step : 141, train Loss : 0.0103\n",
            "Step : 151, train Loss : 0.0139\n",
            "Step : 161, train Loss : 0.0132\n",
            "Step : 171, train Loss : 0.0127\n",
            "Step : 181, train Loss : 0.0151\n",
            "Step : 191, train Loss : 0.0113\n",
            "Step : 201, train Loss : 0.0099\n",
            "Step : 211, train Loss : 0.0141\n",
            "Step : 221, train Loss : 0.0107\n",
            "Step : 231, train Loss : 0.0110\n",
            "Step : 241, train Loss : 0.0127\n",
            "Step : 251, train Loss : 0.0157\n",
            "Step : 261, train Loss : 0.0097\n",
            "Step : 271, train Loss : 0.0085\n",
            "Step : 281, train Loss : 0.0087\n",
            "Step : 291, train Loss : 0.0127\n",
            "Step : 301, train Loss : 0.0113\n",
            "Step : 311, train Loss : 0.0129\n",
            "Step : 321, train Loss : 0.0085\n",
            "Step : 331, train Loss : 0.0131\n",
            "Step : 341, train Loss : 0.0089\n",
            "Step : 351, train Loss : 0.0104\n",
            "Step : 361, train Loss : 0.0098\n",
            "Step : 371, train Loss : 0.0108\n",
            "Step : 381, train Loss : 0.0132\n",
            "Step : 391, train Loss : 0.0173\n",
            "Step : 401, train Loss : 0.0139\n",
            "Step : 411, train Loss : 0.0128\n",
            "Step : 421, train Loss : 0.0145\n",
            "Step : 431, train Loss : 0.0145\n",
            "Step : 441, train Loss : 0.0115\n",
            "Step : 451, train Loss : 0.0142\n",
            "Step : 461, train Loss : 0.0113\n",
            "Step : 471, train Loss : 0.0152\n",
            "Step : 481, train Loss : 0.0139\n",
            "Step : 491, train Loss : 0.0099\n",
            "Step : 501, train Loss : 0.0086\n",
            "Step : 511, train Loss : 0.0117\n",
            "Step : 521, train Loss : 0.0101\n",
            "Step : 531, train Loss : 0.0120\n",
            "Step : 541, train Loss : 0.0105\n",
            "Step : 551, train Loss : 0.0150\n",
            "Step : 561, train Loss : 0.0142\n",
            "Step : 571, train Loss : 0.0122\n",
            "Step : 581, train Loss : 0.0111\n",
            "Step : 591, train Loss : 0.0118\n",
            "Step : 601, train Loss : 0.0129\n",
            "Step : 611, train Loss : 0.0117\n",
            "Step : 621, train Loss : 0.0117\n",
            "Step : 631, train Loss : 0.0151\n",
            "Step : 641, train Loss : 0.0087\n",
            "Step : 651, train Loss : 0.0116\n",
            "Step : 661, train Loss : 0.0178\n",
            "Step : 671, train Loss : 0.0139\n",
            "Step : 681, train Loss : 0.0101\n",
            "Step : 691, train Loss : 0.0170\n",
            "Step : 701, train Loss : 0.0102\n",
            "Step : 711, train Loss : 0.0076\n",
            "Step : 721, train Loss : 0.0116\n",
            "Step : 731, train Loss : 0.0119\n",
            "Step : 741, train Loss : 0.0090\n",
            "Step : 751, train Loss : 0.0118\n",
            "Step : 761, train Loss : 0.0104\n",
            "Step : 771, train Loss : 0.0102\n",
            "Step : 781, train Loss : 0.0148\n",
            "Step : 791, train Loss : 0.0129\n",
            "Step : 801, train Loss : 0.0108\n",
            "Step : 811, train Loss : 0.0173\n",
            "Step : 821, train Loss : 0.0142\n",
            "Step : 831, train Loss : 0.0086\n",
            "Step : 841, train Loss : 0.0120\n",
            "Step : 851, train Loss : 0.0104\n",
            "Step : 861, train Loss : 0.0149\n",
            "Step : 871, train Loss : 0.0101\n",
            "Step : 881, train Loss : 0.0111\n",
            "Step : 891, train Loss : 0.0175\n",
            "Step : 901, train Loss : 0.0170\n",
            "Step : 911, train Loss : 0.0159\n",
            "Step : 921, train Loss : 0.0121\n",
            "Step : 931, train Loss : 0.0157\n",
            "Step : 941, train Loss : 0.0127\n",
            "Step : 951, train Loss : 0.0105\n",
            "Step : 961, train Loss : 0.0096\n",
            "Step : 971, train Loss : 0.0166\n",
            "Step : 981, train Loss : 0.0106\n",
            "Step : 991, train Loss : 0.0137\n",
            "Step : 1001, train Loss : 0.0151\n",
            "Step : 1011, train Loss : 0.0114\n",
            "Step : 1021, train Loss : 0.0092\n",
            "Step : 1031, train Loss : 0.0130\n",
            "Step : 1041, train Loss : 0.0137\n",
            "Step : 1051, train Loss : 0.0169\n",
            "Step : 1061, train Loss : 0.0148\n",
            "Step : 1071, train Loss : 0.0075\n",
            "Step : 1081, train Loss : 0.0113\n",
            "Step : 1091, train Loss : 0.0121\n",
            "Step : 1101, train Loss : 0.0103\n",
            "Step : 1111, train Loss : 0.0150\n",
            "Step : 1121, train Loss : 0.0150\n",
            "Step : 1131, train Loss : 0.0147\n",
            "Step : 1141, train Loss : 0.0107\n",
            "Step : 1151, train Loss : 0.0155\n",
            "Step : 1161, train Loss : 0.0134\n",
            "Step : 1171, train Loss : 0.0129\n",
            "Step : 1181, train Loss : 0.0095\n",
            "Step : 1191, train Loss : 0.0122\n",
            "Step : 1201, train Loss : 0.0112\n",
            "Step : 1211, train Loss : 0.0077\n",
            "Step : 1221, train Loss : 0.0103\n",
            "Step : 1231, train Loss : 0.0121\n",
            "Step : 1241, train Loss : 0.0128\n",
            "Step : 1251, train Loss : 0.0140\n",
            "Step : 1261, train Loss : 0.0094\n",
            "Step : 1271, train Loss : 0.0172\n",
            "Step : 1281, train Loss : 0.0092\n",
            "Step : 1291, train Loss : 0.0097\n",
            "Step : 1301, train Loss : 0.0139\n",
            "Step : 1311, train Loss : 0.0110\n",
            "Step : 1321, train Loss : 0.0146\n",
            "Step : 1331, train Loss : 0.0138\n",
            "Step : 1341, train Loss : 0.0210\n",
            "Step : 1351, train Loss : 0.0123\n",
            "Step : 1361, train Loss : 0.0116\n",
            "Step : 1371, train Loss : 0.0106\n",
            "Step : 1381, train Loss : 0.0107\n",
            "Step : 1391, train Loss : 0.0118\n",
            "Step : 1401, train Loss : 0.0145\n",
            "Step : 1411, train Loss : 0.0133\n",
            "Step : 1421, train Loss : 0.0108\n",
            "Step : 1431, train Loss : 0.0171\n",
            "Step : 1441, train Loss : 0.0084\n",
            "Step : 1451, train Loss : 0.0108\n",
            "Step : 1461, train Loss : 0.0139\n",
            "Step : 1471, train Loss : 0.0179\n",
            "Step : 1481, train Loss : 0.0139\n",
            "Step : 1491, train Loss : 0.0125\n",
            "Step : 1501, train Loss : 0.0114\n",
            "Step : 1511, train Loss : 0.0155\n",
            "Step : 1521, train Loss : 0.0138\n",
            "Step : 1531, train Loss : 0.0111\n",
            "Step : 1541, train Loss : 0.0111\n",
            "Step : 1551, train Loss : 0.0165\n",
            "Step : 1561, train Loss : 0.0127\n",
            "Step : 1571, train Loss : 0.0081\n",
            "Step : 1581, train Loss : 0.0125\n",
            "Step : 1591, train Loss : 0.0141\n",
            "Step : 1601, train Loss : 0.0116\n",
            "Step : 1611, train Loss : 0.0091\n",
            "Step : 1621, train Loss : 0.0089\n",
            "Step : 1631, train Loss : 0.0156\n",
            "Step : 1641, train Loss : 0.0095\n",
            "Step : 1651, train Loss : 0.0083\n",
            "Step : 1661, train Loss : 0.0149\n",
            "Step : 1671, train Loss : 0.0118\n",
            "Step : 1681, train Loss : 0.0109\n",
            "Step : 1691, train Loss : 0.0094\n",
            "Step : 1701, train Loss : 0.0140\n",
            "Step : 1711, train Loss : 0.0103\n",
            "Step : 1721, train Loss : 0.0129\n",
            "Step : 1731, train Loss : 0.0085\n",
            "Step : 1741, train Loss : 0.0087\n",
            "Step : 1751, train Loss : 0.0097\n",
            "Step : 1761, train Loss : 0.0100\n",
            "Step : 1771, train Loss : 0.0086\n",
            "Step : 1781, train Loss : 0.0149\n",
            "Step : 1791, train Loss : 0.0104\n",
            "Step : 1801, train Loss : 0.0075\n",
            "Step : 1811, train Loss : 0.0098\n",
            "Step : 1821, train Loss : 0.0148\n",
            "Step : 1831, train Loss : 0.0125\n",
            "Step : 1841, train Loss : 0.0113\n",
            "Step : 1851, train Loss : 0.0099\n",
            "Step : 1861, train Loss : 0.0157\n",
            "Step : 1871, train Loss : 0.0163\n",
            "Step : 1881, train Loss : 0.0165\n",
            "Step : 1891, train Loss : 0.0116\n",
            "Step : 1901, train Loss : 0.0169\n",
            "Step : 1911, train Loss : 0.0076\n",
            "Step : 1921, train Loss : 0.0092\n",
            "Step : 1931, train Loss : 0.0098\n",
            "Step : 1941, train Loss : 0.0085\n",
            "Step : 1951, train Loss : 0.0091\n",
            "Epoch 1 Total Mean Loss : 0.0123\n",
            "*****Epoch 1 Train Finish*****\n",
            "\n",
            "SAVING EPOCH 1 ...\n",
            "*****Epoch 1 Valid Start*****\n",
            "Step: 0,  Pearson: tensor([0.8837])\n",
            "Step: 1,  Pearson: tensor([0.9451])\n",
            "Step: 2,  Pearson: tensor([0.9524])\n",
            "Step: 3,  Pearson: tensor([0.8989])\n",
            "Step: 4,  Pearson: tensor([0.9017])\n",
            "Step: 5,  Pearson: tensor([0.9750])\n",
            "Step: 6,  Pearson: tensor([0.8581])\n",
            "Step: 7,  Pearson: tensor([0.8947])\n",
            "Step: 8,  Pearson: tensor([0.9407])\n",
            "Step: 9,  Pearson: tensor([0.9279])\n",
            "Step: 10,  Pearson: tensor([0.7949])\n",
            "Step: 11,  Pearson: tensor([0.9448])\n",
            "Step: 12,  Pearson: tensor([0.8325])\n",
            "Step: 13,  Pearson: tensor([0.8743])\n",
            "Step: 14,  Pearson: tensor([0.9525])\n",
            "Step: 15,  Pearson: tensor([0.9455])\n",
            "Step: 16,  Pearson: tensor([0.9612])\n",
            "Step: 17,  Pearson: tensor([0.9582])\n",
            "Step: 18,  Pearson: tensor([0.7662])\n",
            "Step: 19,  Pearson: tensor([0.9440])\n",
            "Step: 20,  Pearson: tensor([0.9098])\n",
            "Step: 21,  Pearson: tensor([0.9214])\n",
            "Step: 22,  Pearson: tensor([0.9298])\n",
            "Step: 23,  Pearson: tensor([0.7660])\n",
            "Step: 24,  Pearson: tensor([0.8678])\n",
            "Step: 25,  Pearson: tensor([0.9401])\n",
            "Step: 26,  Pearson: tensor([0.9092])\n",
            "Step: 27,  Pearson: tensor([0.9418])\n",
            "Step: 28,  Pearson: tensor([0.9452])\n",
            "Step: 29,  Pearson: tensor([0.8660])\n",
            "Step: 30,  Pearson: tensor([0.9242])\n",
            "Step: 31,  Pearson: tensor([0.9654])\n",
            "Step: 32,  Pearson: tensor([0.9090])\n",
            "Step: 33,  Pearson: tensor([0.8707])\n",
            "Step: 34,  Pearson: tensor([0.9460])\n",
            "Step: 35,  Pearson: tensor([0.9601])\n",
            "Step: 36,  Pearson: tensor([0.8350])\n",
            "Step: 37,  Pearson: tensor([0.8872])\n",
            "Step: 38,  Pearson: tensor([0.8846])\n",
            "Step: 39,  Pearson: tensor([0.9213])\n",
            "Step: 40,  Pearson: tensor([0.9398])\n",
            "Step: 41,  Pearson: tensor([0.9550])\n",
            "Step: 42,  Pearson: tensor([0.8750])\n",
            "Step: 43,  Pearson: tensor([0.8765])\n",
            "Step: 44,  Pearson: tensor([0.9689])\n",
            "Step: 45,  Pearson: tensor([0.9033])\n",
            "Step: 46,  Pearson: tensor([0.9058])\n",
            "Step: 47,  Pearson: tensor([0.9726])\n",
            "Step: 48,  Pearson: tensor([0.8584])\n",
            "Step: 49,  Pearson: tensor([0.8934])\n",
            "Step: 50,  Pearson: tensor([0.9244])\n",
            "Step: 51,  Pearson: tensor([0.9405])\n",
            "Step: 52,  Pearson: tensor([0.9368])\n",
            "Step: 53,  Pearson: tensor([0.8742])\n",
            "Step: 54,  Pearson: tensor([0.9606])\n",
            "Step: 55,  Pearson: tensor([0.9408])\n",
            "Step: 56,  Pearson: tensor([0.9040])\n",
            "Step: 57,  Pearson: tensor([0.9613])\n",
            "Step: 58,  Pearson: tensor([0.9654])\n",
            "Step: 59,  Pearson: tensor([0.9454])\n",
            "Step: 60,  Pearson: tensor([0.9812])\n",
            "Step: 61,  Pearson: tensor([0.9048])\n",
            "Step: 62,  Pearson: tensor([0.8334])\n",
            "Step: 63,  Pearson: tensor([0.9505])\n",
            "Step: 64,  Pearson: tensor([0.9219])\n",
            "Step: 65,  Pearson: tensor([0.9463])\n",
            "Step: 66,  Pearson: tensor([0.9333])\n",
            "Step: 67,  Pearson: tensor([0.9817])\n",
            "Step: 68,  Pearson: tensor([0.9186])\n",
            "Step: 69,  Pearson: tensor([0.9189])\n",
            "Step: 70,  Pearson: tensor([0.8936])\n",
            "Step: 71,  Pearson: tensor([0.9035])\n",
            "Step: 72,  Pearson: tensor([0.9706])\n",
            "Step: 73,  Pearson: tensor([0.9005])\n",
            "Step: 74,  Pearson: tensor([0.8866])\n",
            "Step: 75,  Pearson: tensor([0.9766])\n",
            "Step: 76,  Pearson: tensor([0.9342])\n",
            "Step: 77,  Pearson: tensor([0.9234])\n",
            "Step: 78,  Pearson: tensor([0.6625])\n",
            "Step: 79,  Pearson: tensor([0.9039])\n",
            "Step: 80,  Pearson: tensor([0.8831])\n",
            "Step: 81,  Pearson: tensor([0.9201])\n",
            "Step: 82,  Pearson: tensor([0.9323])\n",
            "Step: 83,  Pearson: tensor([0.8672])\n",
            "Step: 84,  Pearson: tensor([0.9523])\n",
            "Step: 85,  Pearson: tensor([0.9144])\n",
            "Step: 86,  Pearson: tensor([0.8646])\n",
            "Step: 87,  Pearson: tensor([0.9719])\n",
            "Step: 88,  Pearson: tensor([0.8854])\n",
            "Step: 89,  Pearson: tensor([0.8726])\n",
            "Step: 90,  Pearson: tensor([0.9628])\n",
            "Step: 91,  Pearson: tensor([0.9539])\n",
            "Step: 92,  Pearson: tensor([0.8857])\n",
            "Step: 93,  Pearson: tensor([0.9811])\n",
            "Step: 94,  Pearson: tensor([0.9456])\n",
            "Step: 95,  Pearson: tensor([0.8912])\n",
            "Step: 96,  Pearson: tensor([0.8597])\n",
            "Step: 97,  Pearson: tensor([0.9431])\n",
            "Step: 98,  Pearson: tensor([0.8764])\n",
            "Step: 99,  Pearson: tensor([0.9531])\n",
            "Step: 100,  Pearson: tensor([0.9611])\n",
            "Step: 101,  Pearson: tensor([0.9408])\n",
            "Step: 102,  Pearson: tensor([0.8973])\n",
            "Step: 103,  Pearson: tensor([0.9380])\n",
            "Step: 104,  Pearson: tensor([0.8674])\n",
            "Step: 105,  Pearson: tensor([0.8784])\n",
            "Step: 106,  Pearson: tensor([0.8979])\n",
            "Step: 107,  Pearson: tensor([0.8872])\n",
            "Step: 108,  Pearson: tensor([0.9817])\n",
            "total_valid_loss :  0.6002415426827352 total_f1_score :  0.9127811300054854 total_pearsonr : tensor([0.9152])\n",
            "Epoch 1 Valid Loss : 0.6002415426827352 Valid Pearsonr : tensor([0.9152]) ValidF1 : 0.9127811300054854\n",
            "*****Epoch 1 Valid Finish*****\n",
            "\n",
            "****** STARTING TO TRAIN EPOCH #2 ******\n",
            "Step : 11, train Loss : 0.0085\n",
            "Step : 21, train Loss : 0.0090\n",
            "Step : 31, train Loss : 0.0040\n",
            "Step : 41, train Loss : 0.0072\n",
            "Step : 51, train Loss : 0.0063\n",
            "Step : 61, train Loss : 0.0076\n",
            "Step : 71, train Loss : 0.0049\n",
            "Step : 81, train Loss : 0.0057\n",
            "Step : 91, train Loss : 0.0064\n",
            "Step : 101, train Loss : 0.0055\n",
            "Step : 111, train Loss : 0.0056\n",
            "Step : 121, train Loss : 0.0065\n",
            "Step : 131, train Loss : 0.0097\n",
            "Step : 141, train Loss : 0.0063\n",
            "Step : 151, train Loss : 0.0063\n",
            "Step : 161, train Loss : 0.0065\n",
            "Step : 171, train Loss : 0.0068\n",
            "Step : 181, train Loss : 0.0051\n",
            "Step : 191, train Loss : 0.0079\n",
            "Step : 201, train Loss : 0.0067\n",
            "Step : 211, train Loss : 0.0049\n",
            "Step : 221, train Loss : 0.0041\n",
            "Step : 231, train Loss : 0.0056\n",
            "Step : 241, train Loss : 0.0081\n",
            "Step : 251, train Loss : 0.0074\n",
            "Step : 261, train Loss : 0.0063\n",
            "Step : 271, train Loss : 0.0076\n",
            "Step : 281, train Loss : 0.0074\n",
            "Step : 291, train Loss : 0.0055\n",
            "Step : 301, train Loss : 0.0060\n",
            "Step : 311, train Loss : 0.0067\n",
            "Step : 321, train Loss : 0.0051\n",
            "Step : 331, train Loss : 0.0061\n",
            "Step : 341, train Loss : 0.0068\n",
            "Step : 351, train Loss : 0.0067\n",
            "Step : 361, train Loss : 0.0054\n",
            "Step : 371, train Loss : 0.0077\n",
            "Step : 381, train Loss : 0.0070\n",
            "Step : 391, train Loss : 0.0064\n",
            "Step : 401, train Loss : 0.0088\n",
            "Step : 411, train Loss : 0.0063\n",
            "Step : 421, train Loss : 0.0058\n",
            "Step : 431, train Loss : 0.0086\n",
            "Step : 441, train Loss : 0.0056\n",
            "Step : 451, train Loss : 0.0087\n",
            "Step : 461, train Loss : 0.0071\n",
            "Step : 471, train Loss : 0.0041\n",
            "Step : 481, train Loss : 0.0058\n",
            "Step : 491, train Loss : 0.0046\n",
            "Step : 501, train Loss : 0.0053\n",
            "Step : 511, train Loss : 0.0065\n",
            "Step : 521, train Loss : 0.0076\n",
            "Step : 531, train Loss : 0.0073\n",
            "Step : 541, train Loss : 0.0071\n",
            "Step : 551, train Loss : 0.0060\n",
            "Step : 561, train Loss : 0.0074\n",
            "Step : 571, train Loss : 0.0043\n",
            "Step : 581, train Loss : 0.0055\n",
            "Step : 591, train Loss : 0.0071\n",
            "Step : 601, train Loss : 0.0065\n",
            "Step : 611, train Loss : 0.0052\n",
            "Step : 621, train Loss : 0.0054\n",
            "Step : 631, train Loss : 0.0060\n",
            "Step : 641, train Loss : 0.0068\n",
            "Step : 651, train Loss : 0.0063\n",
            "Step : 661, train Loss : 0.0086\n",
            "Step : 671, train Loss : 0.0059\n",
            "Step : 681, train Loss : 0.0074\n",
            "Step : 691, train Loss : 0.0044\n",
            "Step : 701, train Loss : 0.0060\n",
            "Step : 711, train Loss : 0.0057\n",
            "Step : 721, train Loss : 0.0066\n",
            "Step : 731, train Loss : 0.0050\n",
            "Step : 741, train Loss : 0.0070\n",
            "Step : 751, train Loss : 0.0077\n",
            "Step : 761, train Loss : 0.0068\n",
            "Step : 771, train Loss : 0.0061\n",
            "Step : 781, train Loss : 0.0064\n",
            "Step : 791, train Loss : 0.0060\n",
            "Step : 801, train Loss : 0.0066\n",
            "Step : 811, train Loss : 0.0042\n",
            "Step : 821, train Loss : 0.0067\n",
            "Step : 831, train Loss : 0.0064\n",
            "Step : 841, train Loss : 0.0064\n",
            "Step : 851, train Loss : 0.0053\n",
            "Step : 861, train Loss : 0.0051\n",
            "Step : 871, train Loss : 0.0052\n",
            "Step : 881, train Loss : 0.0054\n",
            "Step : 891, train Loss : 0.0056\n",
            "Step : 901, train Loss : 0.0036\n",
            "Step : 911, train Loss : 0.0069\n",
            "Step : 921, train Loss : 0.0075\n",
            "Step : 931, train Loss : 0.0065\n",
            "Step : 941, train Loss : 0.0062\n",
            "Step : 951, train Loss : 0.0051\n",
            "Step : 961, train Loss : 0.0070\n",
            "Step : 971, train Loss : 0.0049\n",
            "Step : 981, train Loss : 0.0064\n",
            "Step : 991, train Loss : 0.0051\n",
            "Step : 1001, train Loss : 0.0070\n",
            "Step : 1011, train Loss : 0.0051\n",
            "Step : 1021, train Loss : 0.0068\n",
            "Step : 1031, train Loss : 0.0058\n",
            "Step : 1041, train Loss : 0.0063\n",
            "Step : 1051, train Loss : 0.0063\n",
            "Step : 1061, train Loss : 0.0090\n",
            "Step : 1071, train Loss : 0.0081\n",
            "Step : 1081, train Loss : 0.0069\n",
            "Step : 1091, train Loss : 0.0061\n",
            "Step : 1101, train Loss : 0.0064\n",
            "Step : 1111, train Loss : 0.0069\n",
            "Step : 1121, train Loss : 0.0054\n",
            "Step : 1131, train Loss : 0.0061\n",
            "Step : 1141, train Loss : 0.0072\n",
            "Step : 1151, train Loss : 0.0055\n",
            "Step : 1161, train Loss : 0.0042\n",
            "Step : 1171, train Loss : 0.0051\n",
            "Step : 1181, train Loss : 0.0054\n",
            "Step : 1191, train Loss : 0.0071\n",
            "Step : 1201, train Loss : 0.0058\n",
            "Step : 1211, train Loss : 0.0048\n",
            "Step : 1221, train Loss : 0.0055\n",
            "Step : 1231, train Loss : 0.0056\n",
            "Step : 1241, train Loss : 0.0046\n",
            "Step : 1251, train Loss : 0.0069\n",
            "Step : 1261, train Loss : 0.0056\n",
            "Step : 1271, train Loss : 0.0064\n",
            "Step : 1281, train Loss : 0.0053\n",
            "Step : 1291, train Loss : 0.0051\n",
            "Step : 1301, train Loss : 0.0074\n",
            "Step : 1311, train Loss : 0.0074\n",
            "Step : 1321, train Loss : 0.0063\n",
            "Step : 1331, train Loss : 0.0064\n",
            "Step : 1341, train Loss : 0.0053\n",
            "Step : 1351, train Loss : 0.0058\n",
            "Step : 1361, train Loss : 0.0087\n",
            "Step : 1371, train Loss : 0.0076\n",
            "Step : 1381, train Loss : 0.0072\n",
            "Step : 1391, train Loss : 0.0072\n",
            "Step : 1401, train Loss : 0.0060\n",
            "Step : 1411, train Loss : 0.0061\n",
            "Step : 1421, train Loss : 0.0068\n",
            "Step : 1431, train Loss : 0.0050\n",
            "Step : 1441, train Loss : 0.0051\n",
            "Step : 1451, train Loss : 0.0042\n",
            "Step : 1461, train Loss : 0.0068\n",
            "Step : 1471, train Loss : 0.0050\n",
            "Step : 1481, train Loss : 0.0066\n",
            "Step : 1491, train Loss : 0.0049\n",
            "Step : 1501, train Loss : 0.0058\n",
            "Step : 1511, train Loss : 0.0055\n",
            "Step : 1521, train Loss : 0.0061\n",
            "Step : 1531, train Loss : 0.0046\n",
            "Step : 1541, train Loss : 0.0062\n",
            "Step : 1551, train Loss : 0.0078\n",
            "Step : 1561, train Loss : 0.0044\n",
            "Step : 1571, train Loss : 0.0069\n",
            "Step : 1581, train Loss : 0.0060\n",
            "Step : 1591, train Loss : 0.0062\n",
            "Step : 1601, train Loss : 0.0057\n",
            "Step : 1611, train Loss : 0.0068\n",
            "Step : 1621, train Loss : 0.0083\n",
            "Step : 1631, train Loss : 0.0045\n",
            "Step : 1641, train Loss : 0.0050\n",
            "Step : 1651, train Loss : 0.0061\n",
            "Step : 1661, train Loss : 0.0060\n",
            "Step : 1671, train Loss : 0.0087\n",
            "Step : 1681, train Loss : 0.0057\n",
            "Step : 1691, train Loss : 0.0049\n",
            "Step : 1701, train Loss : 0.0044\n",
            "Step : 1711, train Loss : 0.0051\n",
            "Step : 1721, train Loss : 0.0049\n",
            "Step : 1731, train Loss : 0.0060\n",
            "Step : 1741, train Loss : 0.0044\n",
            "Step : 1751, train Loss : 0.0072\n",
            "Step : 1761, train Loss : 0.0053\n",
            "Step : 1771, train Loss : 0.0043\n",
            "Step : 1781, train Loss : 0.0063\n",
            "Step : 1791, train Loss : 0.0081\n",
            "Step : 1801, train Loss : 0.0043\n",
            "Step : 1811, train Loss : 0.0061\n",
            "Step : 1821, train Loss : 0.0049\n",
            "Step : 1831, train Loss : 0.0046\n",
            "Step : 1841, train Loss : 0.0042\n",
            "Step : 1851, train Loss : 0.0076\n",
            "Step : 1861, train Loss : 0.0071\n",
            "Step : 1871, train Loss : 0.0070\n",
            "Step : 1881, train Loss : 0.0068\n",
            "Step : 1891, train Loss : 0.0071\n",
            "Step : 1901, train Loss : 0.0065\n",
            "Step : 1911, train Loss : 0.0067\n",
            "Step : 1921, train Loss : 0.0056\n",
            "Step : 1931, train Loss : 0.0061\n",
            "Step : 1941, train Loss : 0.0074\n",
            "Step : 1951, train Loss : 0.0077\n",
            "Epoch 2 Total Mean Loss : 0.0062\n",
            "*****Epoch 2 Train Finish*****\n",
            "\n",
            "SAVING EPOCH 2 ...\n",
            "*****Epoch 2 Valid Start*****\n",
            "Step: 0,  Pearson: tensor([0.8976])\n",
            "Step: 1,  Pearson: tensor([0.9334])\n",
            "Step: 2,  Pearson: tensor([0.9601])\n",
            "Step: 3,  Pearson: tensor([0.8994])\n",
            "Step: 4,  Pearson: tensor([0.8818])\n",
            "Step: 5,  Pearson: tensor([0.9817])\n",
            "Step: 6,  Pearson: tensor([0.9030])\n",
            "Step: 7,  Pearson: tensor([0.8895])\n",
            "Step: 8,  Pearson: tensor([0.9363])\n",
            "Step: 9,  Pearson: tensor([0.9324])\n",
            "Step: 10,  Pearson: tensor([0.7903])\n",
            "Step: 11,  Pearson: tensor([0.8924])\n",
            "Step: 12,  Pearson: tensor([0.8598])\n",
            "Step: 13,  Pearson: tensor([0.9150])\n",
            "Step: 14,  Pearson: tensor([0.9644])\n",
            "Step: 15,  Pearson: tensor([0.9435])\n",
            "Step: 16,  Pearson: tensor([0.9651])\n",
            "Step: 17,  Pearson: tensor([0.9628])\n",
            "Step: 18,  Pearson: tensor([0.7763])\n",
            "Step: 19,  Pearson: tensor([0.9453])\n",
            "Step: 20,  Pearson: tensor([0.9534])\n",
            "Step: 21,  Pearson: tensor([0.9602])\n",
            "Step: 22,  Pearson: tensor([0.9551])\n",
            "Step: 23,  Pearson: tensor([0.7373])\n",
            "Step: 24,  Pearson: tensor([0.8833])\n",
            "Step: 25,  Pearson: tensor([0.9494])\n",
            "Step: 26,  Pearson: tensor([0.9199])\n",
            "Step: 27,  Pearson: tensor([0.9422])\n",
            "Step: 28,  Pearson: tensor([0.9382])\n",
            "Step: 29,  Pearson: tensor([0.8263])\n",
            "Step: 30,  Pearson: tensor([0.9262])\n",
            "Step: 31,  Pearson: tensor([0.9735])\n",
            "Step: 32,  Pearson: tensor([0.8869])\n",
            "Step: 33,  Pearson: tensor([0.8490])\n",
            "Step: 34,  Pearson: tensor([0.9731])\n",
            "Step: 35,  Pearson: tensor([0.9674])\n",
            "Step: 36,  Pearson: tensor([0.8355])\n",
            "Step: 37,  Pearson: tensor([0.8809])\n",
            "Step: 38,  Pearson: tensor([0.8746])\n",
            "Step: 39,  Pearson: tensor([0.9300])\n",
            "Step: 40,  Pearson: tensor([0.9525])\n",
            "Step: 41,  Pearson: tensor([0.9599])\n",
            "Step: 42,  Pearson: tensor([0.8524])\n",
            "Step: 43,  Pearson: tensor([0.8557])\n",
            "Step: 44,  Pearson: tensor([0.9746])\n",
            "Step: 45,  Pearson: tensor([0.9113])\n",
            "Step: 46,  Pearson: tensor([0.9299])\n",
            "Step: 47,  Pearson: tensor([0.9722])\n",
            "Step: 48,  Pearson: tensor([0.8410])\n",
            "Step: 49,  Pearson: tensor([0.9247])\n",
            "Step: 50,  Pearson: tensor([0.9426])\n",
            "Step: 51,  Pearson: tensor([0.9566])\n",
            "Step: 52,  Pearson: tensor([0.9185])\n",
            "Step: 53,  Pearson: tensor([0.9021])\n",
            "Step: 54,  Pearson: tensor([0.9695])\n",
            "Step: 55,  Pearson: tensor([0.9349])\n",
            "Step: 56,  Pearson: tensor([0.8930])\n",
            "Step: 57,  Pearson: tensor([0.9544])\n",
            "Step: 58,  Pearson: tensor([0.9608])\n",
            "Step: 59,  Pearson: tensor([0.9527])\n",
            "Step: 60,  Pearson: tensor([0.9729])\n",
            "Step: 61,  Pearson: tensor([0.9492])\n",
            "Step: 62,  Pearson: tensor([0.8124])\n",
            "Step: 63,  Pearson: tensor([0.9608])\n",
            "Step: 64,  Pearson: tensor([0.9377])\n",
            "Step: 65,  Pearson: tensor([0.9466])\n",
            "Step: 66,  Pearson: tensor([0.9097])\n",
            "Step: 67,  Pearson: tensor([0.9731])\n",
            "Step: 68,  Pearson: tensor([0.9485])\n",
            "Step: 69,  Pearson: tensor([0.9318])\n",
            "Step: 70,  Pearson: tensor([0.9088])\n",
            "Step: 71,  Pearson: tensor([0.8847])\n",
            "Step: 72,  Pearson: tensor([0.9416])\n",
            "Step: 73,  Pearson: tensor([0.8842])\n",
            "Step: 74,  Pearson: tensor([0.8845])\n",
            "Step: 75,  Pearson: tensor([0.9776])\n",
            "Step: 76,  Pearson: tensor([0.9403])\n",
            "Step: 77,  Pearson: tensor([0.9217])\n",
            "Step: 78,  Pearson: tensor([0.7433])\n",
            "Step: 79,  Pearson: tensor([0.9260])\n",
            "Step: 80,  Pearson: tensor([0.9179])\n",
            "Step: 81,  Pearson: tensor([0.9492])\n",
            "Step: 82,  Pearson: tensor([0.9207])\n",
            "Step: 83,  Pearson: tensor([0.8929])\n",
            "Step: 84,  Pearson: tensor([0.9705])\n",
            "Step: 85,  Pearson: tensor([0.9016])\n",
            "Step: 86,  Pearson: tensor([0.8614])\n",
            "Step: 87,  Pearson: tensor([0.9714])\n",
            "Step: 88,  Pearson: tensor([0.9004])\n",
            "Step: 89,  Pearson: tensor([0.8094])\n",
            "Step: 90,  Pearson: tensor([0.9362])\n",
            "Step: 91,  Pearson: tensor([0.9401])\n",
            "Step: 92,  Pearson: tensor([0.9336])\n",
            "Step: 93,  Pearson: tensor([0.9643])\n",
            "Step: 94,  Pearson: tensor([0.9400])\n",
            "Step: 95,  Pearson: tensor([0.8821])\n",
            "Step: 96,  Pearson: tensor([0.9319])\n",
            "Step: 97,  Pearson: tensor([0.9496])\n",
            "Step: 98,  Pearson: tensor([0.8825])\n",
            "Step: 99,  Pearson: tensor([0.9485])\n",
            "Step: 100,  Pearson: tensor([0.9475])\n",
            "Step: 101,  Pearson: tensor([0.9206])\n",
            "Step: 102,  Pearson: tensor([0.9057])\n",
            "Step: 103,  Pearson: tensor([0.9208])\n",
            "Step: 104,  Pearson: tensor([0.8952])\n",
            "Step: 105,  Pearson: tensor([0.9170])\n",
            "Step: 106,  Pearson: tensor([0.9263])\n",
            "Step: 107,  Pearson: tensor([0.9324])\n",
            "Step: 108,  Pearson: tensor([0.9756])\n",
            "total_valid_loss :  0.5546926270931138 total_f1_score :  0.9189789123196448 total_pearsonr : tensor([0.9190])\n",
            "Epoch 2 Valid Loss : 0.5546926270931138 Valid Pearsonr : tensor([0.9190]) ValidF1 : 0.9189789123196448\n",
            "*****Epoch 2 Valid Finish*****\n",
            "\n",
            "****** STARTING TO TRAIN EPOCH #3 ******\n",
            "Step : 11, train Loss : 0.0046\n",
            "Step : 21, train Loss : 0.0044\n",
            "Step : 31, train Loss : 0.0036\n",
            "Step : 41, train Loss : 0.0039\n",
            "Step : 51, train Loss : 0.0051\n",
            "Step : 61, train Loss : 0.0042\n",
            "Step : 71, train Loss : 0.0037\n",
            "Step : 81, train Loss : 0.0048\n",
            "Step : 91, train Loss : 0.0040\n",
            "Step : 101, train Loss : 0.0047\n",
            "Step : 111, train Loss : 0.0048\n",
            "Step : 121, train Loss : 0.0048\n",
            "Step : 131, train Loss : 0.0038\n",
            "Step : 141, train Loss : 0.0034\n",
            "Step : 151, train Loss : 0.0049\n",
            "Step : 161, train Loss : 0.0044\n",
            "Step : 171, train Loss : 0.0040\n",
            "Step : 181, train Loss : 0.0045\n",
            "Step : 191, train Loss : 0.0040\n",
            "Step : 201, train Loss : 0.0037\n",
            "Step : 211, train Loss : 0.0053\n",
            "Step : 221, train Loss : 0.0039\n",
            "Step : 231, train Loss : 0.0046\n",
            "Step : 241, train Loss : 0.0033\n",
            "Step : 251, train Loss : 0.0034\n",
            "Step : 261, train Loss : 0.0044\n",
            "Step : 271, train Loss : 0.0034\n",
            "Step : 281, train Loss : 0.0047\n",
            "Step : 291, train Loss : 0.0038\n",
            "Step : 301, train Loss : 0.0038\n",
            "Step : 311, train Loss : 0.0043\n",
            "Step : 321, train Loss : 0.0040\n",
            "Step : 331, train Loss : 0.0029\n",
            "Step : 341, train Loss : 0.0051\n",
            "Step : 351, train Loss : 0.0039\n",
            "Step : 361, train Loss : 0.0033\n",
            "Step : 371, train Loss : 0.0034\n",
            "Step : 381, train Loss : 0.0050\n",
            "Step : 391, train Loss : 0.0032\n",
            "Step : 401, train Loss : 0.0041\n",
            "Step : 411, train Loss : 0.0041\n",
            "Step : 421, train Loss : 0.0039\n",
            "Step : 431, train Loss : 0.0058\n",
            "Step : 441, train Loss : 0.0040\n",
            "Step : 451, train Loss : 0.0025\n",
            "Step : 461, train Loss : 0.0043\n",
            "Step : 471, train Loss : 0.0032\n",
            "Step : 481, train Loss : 0.0042\n",
            "Step : 491, train Loss : 0.0044\n",
            "Step : 501, train Loss : 0.0041\n",
            "Step : 511, train Loss : 0.0034\n",
            "Step : 521, train Loss : 0.0033\n",
            "Step : 531, train Loss : 0.0037\n",
            "Step : 541, train Loss : 0.0031\n",
            "Step : 551, train Loss : 0.0036\n",
            "Step : 561, train Loss : 0.0037\n",
            "Step : 571, train Loss : 0.0026\n",
            "Step : 581, train Loss : 0.0033\n",
            "Step : 591, train Loss : 0.0029\n",
            "Step : 601, train Loss : 0.0038\n",
            "Step : 611, train Loss : 0.0043\n",
            "Step : 621, train Loss : 0.0044\n",
            "Step : 631, train Loss : 0.0040\n",
            "Step : 641, train Loss : 0.0024\n",
            "Step : 651, train Loss : 0.0040\n",
            "Step : 661, train Loss : 0.0036\n",
            "Step : 671, train Loss : 0.0038\n",
            "Step : 681, train Loss : 0.0034\n",
            "Step : 691, train Loss : 0.0031\n",
            "Step : 701, train Loss : 0.0038\n",
            "Step : 711, train Loss : 0.0034\n",
            "Step : 721, train Loss : 0.0040\n",
            "Step : 731, train Loss : 0.0049\n",
            "Step : 741, train Loss : 0.0043\n",
            "Step : 751, train Loss : 0.0056\n",
            "Step : 761, train Loss : 0.0038\n",
            "Step : 771, train Loss : 0.0040\n",
            "Step : 781, train Loss : 0.0042\n",
            "Step : 791, train Loss : 0.0028\n",
            "Step : 801, train Loss : 0.0057\n",
            "Step : 811, train Loss : 0.0041\n",
            "Step : 821, train Loss : 0.0042\n",
            "Step : 831, train Loss : 0.0049\n",
            "Step : 841, train Loss : 0.0039\n",
            "Step : 851, train Loss : 0.0033\n",
            "Step : 861, train Loss : 0.0035\n",
            "Step : 871, train Loss : 0.0041\n",
            "Step : 881, train Loss : 0.0040\n",
            "Step : 891, train Loss : 0.0043\n",
            "Step : 901, train Loss : 0.0044\n",
            "Step : 911, train Loss : 0.0037\n",
            "Step : 921, train Loss : 0.0038\n",
            "Step : 931, train Loss : 0.0052\n",
            "Step : 941, train Loss : 0.0036\n",
            "Step : 951, train Loss : 0.0050\n",
            "Step : 961, train Loss : 0.0033\n",
            "Step : 971, train Loss : 0.0030\n",
            "Step : 981, train Loss : 0.0052\n",
            "Step : 991, train Loss : 0.0042\n",
            "Step : 1001, train Loss : 0.0041\n",
            "Step : 1011, train Loss : 0.0037\n",
            "Step : 1021, train Loss : 0.0039\n",
            "Step : 1031, train Loss : 0.0044\n",
            "Step : 1041, train Loss : 0.0033\n",
            "Step : 1051, train Loss : 0.0039\n",
            "Step : 1061, train Loss : 0.0036\n",
            "Step : 1071, train Loss : 0.0033\n",
            "Step : 1081, train Loss : 0.0037\n",
            "Step : 1091, train Loss : 0.0040\n",
            "Step : 1101, train Loss : 0.0033\n",
            "Step : 1111, train Loss : 0.0031\n",
            "Step : 1121, train Loss : 0.0037\n",
            "Step : 1131, train Loss : 0.0043\n",
            "Step : 1141, train Loss : 0.0036\n",
            "Step : 1151, train Loss : 0.0036\n",
            "Step : 1161, train Loss : 0.0027\n",
            "Step : 1171, train Loss : 0.0047\n",
            "Step : 1181, train Loss : 0.0035\n",
            "Step : 1191, train Loss : 0.0032\n",
            "Step : 1201, train Loss : 0.0037\n",
            "Step : 1211, train Loss : 0.0027\n",
            "Step : 1221, train Loss : 0.0029\n",
            "Step : 1231, train Loss : 0.0033\n",
            "Step : 1241, train Loss : 0.0029\n",
            "Step : 1251, train Loss : 0.0032\n",
            "Step : 1261, train Loss : 0.0043\n",
            "Step : 1271, train Loss : 0.0046\n",
            "Step : 1281, train Loss : 0.0047\n",
            "Step : 1291, train Loss : 0.0035\n",
            "Step : 1301, train Loss : 0.0054\n",
            "Step : 1311, train Loss : 0.0035\n",
            "Step : 1321, train Loss : 0.0035\n",
            "Step : 1331, train Loss : 0.0035\n",
            "Step : 1341, train Loss : 0.0035\n",
            "Step : 1351, train Loss : 0.0041\n",
            "Step : 1361, train Loss : 0.0046\n",
            "Step : 1371, train Loss : 0.0041\n",
            "Step : 1381, train Loss : 0.0038\n",
            "Step : 1391, train Loss : 0.0034\n",
            "Step : 1401, train Loss : 0.0043\n",
            "Step : 1411, train Loss : 0.0033\n",
            "Step : 1421, train Loss : 0.0024\n",
            "Step : 1431, train Loss : 0.0037\n",
            "Step : 1441, train Loss : 0.0030\n",
            "Step : 1451, train Loss : 0.0025\n",
            "Step : 1461, train Loss : 0.0039\n",
            "Step : 1471, train Loss : 0.0042\n",
            "Step : 1481, train Loss : 0.0040\n",
            "Step : 1491, train Loss : 0.0034\n",
            "Step : 1501, train Loss : 0.0025\n",
            "Step : 1511, train Loss : 0.0042\n",
            "Step : 1521, train Loss : 0.0026\n",
            "Step : 1531, train Loss : 0.0044\n",
            "Step : 1541, train Loss : 0.0037\n",
            "Step : 1551, train Loss : 0.0026\n",
            "Step : 1561, train Loss : 0.0044\n",
            "Step : 1571, train Loss : 0.0034\n",
            "Step : 1581, train Loss : 0.0041\n",
            "Step : 1591, train Loss : 0.0032\n",
            "Step : 1601, train Loss : 0.0036\n",
            "Step : 1611, train Loss : 0.0029\n",
            "Step : 1621, train Loss : 0.0030\n",
            "Step : 1631, train Loss : 0.0029\n",
            "Step : 1641, train Loss : 0.0033\n",
            "Step : 1651, train Loss : 0.0030\n",
            "Step : 1661, train Loss : 0.0030\n",
            "Step : 1671, train Loss : 0.0036\n",
            "Step : 1681, train Loss : 0.0046\n",
            "Step : 1691, train Loss : 0.0038\n",
            "Step : 1701, train Loss : 0.0032\n",
            "Step : 1711, train Loss : 0.0038\n",
            "Step : 1721, train Loss : 0.0041\n",
            "Step : 1731, train Loss : 0.0035\n",
            "Step : 1741, train Loss : 0.0034\n",
            "Step : 1751, train Loss : 0.0030\n",
            "Step : 1761, train Loss : 0.0032\n",
            "Step : 1771, train Loss : 0.0038\n",
            "Step : 1781, train Loss : 0.0032\n",
            "Step : 1791, train Loss : 0.0048\n",
            "Step : 1801, train Loss : 0.0034\n",
            "Step : 1811, train Loss : 0.0029\n",
            "Step : 1821, train Loss : 0.0027\n",
            "Step : 1831, train Loss : 0.0031\n",
            "Step : 1841, train Loss : 0.0029\n",
            "Step : 1851, train Loss : 0.0030\n",
            "Step : 1861, train Loss : 0.0037\n",
            "Step : 1871, train Loss : 0.0028\n",
            "Step : 1881, train Loss : 0.0048\n",
            "Step : 1891, train Loss : 0.0060\n",
            "Step : 1901, train Loss : 0.0038\n",
            "Step : 1911, train Loss : 0.0033\n",
            "Step : 1921, train Loss : 0.0046\n",
            "Step : 1931, train Loss : 0.0048\n",
            "Step : 1941, train Loss : 0.0046\n",
            "Step : 1951, train Loss : 0.0036\n",
            "Epoch 3 Total Mean Loss : 0.0038\n",
            "*****Epoch 3 Train Finish*****\n",
            "\n",
            "SAVING EPOCH 3 ...\n",
            "*****Epoch 3 Valid Start*****\n",
            "Step: 0,  Pearson: tensor([0.8729])\n",
            "Step: 1,  Pearson: tensor([0.9435])\n",
            "Step: 2,  Pearson: tensor([0.9619])\n",
            "Step: 3,  Pearson: tensor([0.8995])\n",
            "Step: 4,  Pearson: tensor([0.8719])\n",
            "Step: 5,  Pearson: tensor([0.9800])\n",
            "Step: 6,  Pearson: tensor([0.8876])\n",
            "Step: 7,  Pearson: tensor([0.9107])\n",
            "Step: 8,  Pearson: tensor([0.9355])\n",
            "Step: 9,  Pearson: tensor([0.9411])\n",
            "Step: 10,  Pearson: tensor([0.8143])\n",
            "Step: 11,  Pearson: tensor([0.9079])\n",
            "Step: 12,  Pearson: tensor([0.8689])\n",
            "Step: 13,  Pearson: tensor([0.9228])\n",
            "Step: 14,  Pearson: tensor([0.9653])\n",
            "Step: 15,  Pearson: tensor([0.9543])\n",
            "Step: 16,  Pearson: tensor([0.9637])\n",
            "Step: 17,  Pearson: tensor([0.9559])\n",
            "Step: 18,  Pearson: tensor([0.7880])\n",
            "Step: 19,  Pearson: tensor([0.9383])\n",
            "Step: 20,  Pearson: tensor([0.9396])\n",
            "Step: 21,  Pearson: tensor([0.9487])\n",
            "Step: 22,  Pearson: tensor([0.9385])\n",
            "Step: 23,  Pearson: tensor([0.7390])\n",
            "Step: 24,  Pearson: tensor([0.8905])\n",
            "Step: 25,  Pearson: tensor([0.9479])\n",
            "Step: 26,  Pearson: tensor([0.9266])\n",
            "Step: 27,  Pearson: tensor([0.9414])\n",
            "Step: 28,  Pearson: tensor([0.9533])\n",
            "Step: 29,  Pearson: tensor([0.8572])\n",
            "Step: 30,  Pearson: tensor([0.9133])\n",
            "Step: 31,  Pearson: tensor([0.9627])\n",
            "Step: 32,  Pearson: tensor([0.9035])\n",
            "Step: 33,  Pearson: tensor([0.8653])\n",
            "Step: 34,  Pearson: tensor([0.9672])\n",
            "Step: 35,  Pearson: tensor([0.9598])\n",
            "Step: 36,  Pearson: tensor([0.8745])\n",
            "Step: 37,  Pearson: tensor([0.8745])\n",
            "Step: 38,  Pearson: tensor([0.8759])\n",
            "Step: 39,  Pearson: tensor([0.9327])\n",
            "Step: 40,  Pearson: tensor([0.9520])\n",
            "Step: 41,  Pearson: tensor([0.9587])\n",
            "Step: 42,  Pearson: tensor([0.8892])\n",
            "Step: 43,  Pearson: tensor([0.8767])\n",
            "Step: 44,  Pearson: tensor([0.9751])\n",
            "Step: 45,  Pearson: tensor([0.8971])\n",
            "Step: 46,  Pearson: tensor([0.9213])\n",
            "Step: 47,  Pearson: tensor([0.9673])\n",
            "Step: 48,  Pearson: tensor([0.8425])\n",
            "Step: 49,  Pearson: tensor([0.9090])\n",
            "Step: 50,  Pearson: tensor([0.9436])\n",
            "Step: 51,  Pearson: tensor([0.9594])\n",
            "Step: 52,  Pearson: tensor([0.9375])\n",
            "Step: 53,  Pearson: tensor([0.9128])\n",
            "Step: 54,  Pearson: tensor([0.9633])\n",
            "Step: 55,  Pearson: tensor([0.9361])\n",
            "Step: 56,  Pearson: tensor([0.8874])\n",
            "Step: 57,  Pearson: tensor([0.9578])\n",
            "Step: 58,  Pearson: tensor([0.9609])\n",
            "Step: 59,  Pearson: tensor([0.9528])\n",
            "Step: 60,  Pearson: tensor([0.9773])\n",
            "Step: 61,  Pearson: tensor([0.9523])\n",
            "Step: 62,  Pearson: tensor([0.8216])\n",
            "Step: 63,  Pearson: tensor([0.9636])\n",
            "Step: 64,  Pearson: tensor([0.9497])\n",
            "Step: 65,  Pearson: tensor([0.9577])\n",
            "Step: 66,  Pearson: tensor([0.9201])\n",
            "Step: 67,  Pearson: tensor([0.9771])\n",
            "Step: 68,  Pearson: tensor([0.9529])\n",
            "Step: 69,  Pearson: tensor([0.9232])\n",
            "Step: 70,  Pearson: tensor([0.9282])\n",
            "Step: 71,  Pearson: tensor([0.8920])\n",
            "Step: 72,  Pearson: tensor([0.9580])\n",
            "Step: 73,  Pearson: tensor([0.8999])\n",
            "Step: 74,  Pearson: tensor([0.8902])\n",
            "Step: 75,  Pearson: tensor([0.9788])\n",
            "Step: 76,  Pearson: tensor([0.9525])\n",
            "Step: 77,  Pearson: tensor([0.9373])\n",
            "Step: 78,  Pearson: tensor([0.7598])\n",
            "Step: 79,  Pearson: tensor([0.9328])\n",
            "Step: 80,  Pearson: tensor([0.9320])\n",
            "Step: 81,  Pearson: tensor([0.9500])\n",
            "Step: 82,  Pearson: tensor([0.9229])\n",
            "Step: 83,  Pearson: tensor([0.9015])\n",
            "Step: 84,  Pearson: tensor([0.9757])\n",
            "Step: 85,  Pearson: tensor([0.9060])\n",
            "Step: 86,  Pearson: tensor([0.8790])\n",
            "Step: 87,  Pearson: tensor([0.9704])\n",
            "Step: 88,  Pearson: tensor([0.9134])\n",
            "Step: 89,  Pearson: tensor([0.8337])\n",
            "Step: 90,  Pearson: tensor([0.9289])\n",
            "Step: 91,  Pearson: tensor([0.9446])\n",
            "Step: 92,  Pearson: tensor([0.9250])\n",
            "Step: 93,  Pearson: tensor([0.9747])\n",
            "Step: 94,  Pearson: tensor([0.9436])\n",
            "Step: 95,  Pearson: tensor([0.8880])\n",
            "Step: 96,  Pearson: tensor([0.9254])\n",
            "Step: 97,  Pearson: tensor([0.9400])\n",
            "Step: 98,  Pearson: tensor([0.8916])\n",
            "Step: 99,  Pearson: tensor([0.9498])\n",
            "Step: 100,  Pearson: tensor([0.9494])\n",
            "Step: 101,  Pearson: tensor([0.9358])\n",
            "Step: 102,  Pearson: tensor([0.8982])\n",
            "Step: 103,  Pearson: tensor([0.9342])\n",
            "Step: 104,  Pearson: tensor([0.8848])\n",
            "Step: 105,  Pearson: tensor([0.9035])\n",
            "Step: 106,  Pearson: tensor([0.9303])\n",
            "Step: 107,  Pearson: tensor([0.9339])\n",
            "Step: 108,  Pearson: tensor([0.9823])\n",
            "total_valid_loss :  0.5121074392708069 total_f1_score :  0.9131403118040089 total_pearsonr : tensor([0.9228])\n",
            "Epoch 3 Valid Loss : 0.5121074392708069 Valid Pearsonr : tensor([0.9228]) ValidF1 : 0.9131403118040089\n",
            "*****Epoch 3 Valid Finish*****\n",
            "\n",
            "** Train Completed! **\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6aab5e212f2f45cbb713fcc319b97c55"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▆█</td></tr><tr><td>total_f1_score </td><td>▁▆█▆</td></tr><tr><td>total_pearsonr</td><td>▁▅▆█</td></tr><tr><td>total_train_loss</td><td>█▃▂▁</td></tr><tr><td>total_train_lr</td><td>█▆▃▁</td></tr><tr><td>total_valid_loss</td><td>█▇▄▁</td></tr><tr><td>train_loss</td><td>█▃▃▂▃▂▂▃▃▄▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_lr</td><td>▂▃▅▇███▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>4</td></tr><tr><td>total_f1_score </td><td>0.91314</td></tr><tr><td>total_pearsonr</td><td>0.92283</td></tr><tr><td>total_train_loss</td><td>0.00381</td></tr><tr><td>total_train_lr</td><td>0.0</td></tr><tr><td>total_valid_loss</td><td>0.51211</td></tr><tr><td>train_loss</td><td>0.00364</td></tr><tr><td>train_lr</td><td>0.0</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">vocal-sweep-5</strong>: <a href=\"https://wandb.ai/kdb/sts_v2/runs/f2xjgp2n\" target=\"_blank\">https://wandb.ai/kdb/sts_v2/runs/f2xjgp2n</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20220529_200355-f2xjgp2n/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, test_dataloader):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    all_prediction = []\n",
        "    all_reallabel = []\n",
        "\n",
        "    for step, batch in enumerate(test_dataloader):\n",
        "\n",
        "        batch = tuple(items.to(device) for items in batch)\n",
        "\n",
        "        (x_batch_one, x_batch_two, batch_y) = batch\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logit = model(x_batch_one, x_batch_two)\n",
        "        logit = logit*5\n",
        "        logit = logit.cpu()\n",
        "        batch_y = batch_y.cpu()\n",
        "\n",
        "        all_prediction = all_prediction + logit.tolist()\n",
        "        all_reallabel = all_reallabel + batch_y.tolist()\n",
        "\n",
        "    pred = torch.Tensor(all_prediction) # x\n",
        "    real = torch.Tensor(all_reallabel) # y\n",
        "    \n",
        "    pearson = pearsonr(pred, real) #stats.spearmanr(pred, real)\n",
        "\n",
        "    #f1\n",
        "    fone = f1_process(pred, real)\n",
        "\n",
        "    return pearson, fone"
      ],
      "metadata": {
        "id": "bLQ2LpPV9xP_"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ckpt1 = '/content/drive/MyDrive/data/checkpoints/sts_hyper.ckpt.0'\n",
        "ckpt2 = '/content/drive/MyDrive/data/checkpoints/sts_hyper.ckpt.1'\n",
        "ckpt3 = '/content/drive/MyDrive/data/checkpoints/sts_hyper.ckpt.2'\n",
        "ckpt4 = '/content/drive/MyDrive/data/checkpoints/sts_hyper.ckpt.3'"
      ],
      "metadata": {
        "id": "TvyZRe_lpDXx"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_checkpoints = [ckpt1, ckpt2, ckpt3, ckpt4]\n",
        "\n",
        "for checkpoint in all_checkpoints:\n",
        "    loaded_ckpt = torch.load(checkpoint)\n",
        "    model, optimizer, scheduler = initializer(train_dataloader, 1)\n",
        "    model.load_state_dict(loaded_ckpt['model_state_dict'])\n",
        "    #model.load_state_dict(torch.load(checkpoint, map_location=device))\n",
        "    pearson_score, fonescore = predict(model, test_dataloader)\n",
        "    print(f'{checkpoint[44:]} pearsonr: {pearson_score}, f1_score: {fonescore}')"
      ],
      "metadata": {
        "id": "tPaPBJqu6nIF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "3baf2937c8eb4c85bf18aeb15b4442e1",
            "d8f2c79c3d5749faa9c25c5ce1a8f903",
            "2776671f8f6b435098dc823dd5a14490",
            "dd32f42a3f074420aa1a828d011e874c",
            "d2a8f82cf1bd42ac810adb9a0788fcd9",
            "0f8b3dd50ce344608b36a45d643a87ac",
            "618d899b54794b698b6267119a697056",
            "6dded0365b8f47458d791a1400a0fd97",
            "a678beed18534bac8c325cbfe8cd8cac",
            "b5d0384596494f9482ae7e76f64f9b35",
            "8398f6c1800e41f287190b2aaf3f2497",
            "9518ad898ac842519d9b6f38d9104208",
            "0d2894ad3a1f4a23b73a42c37a10a66d",
            "2501e9d7023144c0b1c50da5b1ceefb6",
            "816ea30dca6841cdb2404786deae348a",
            "529e0ca10a234727aabd3b5b2fdf403f",
            "98f6367f4e9140fd84235e300f26b465",
            "64aff2f1cc48486ca38af292b479598f",
            "9170cbdf2902471d9454b69ee12e80ae",
            "f719057c9d1f4b2d9286cb3585434b11",
            "ff87f0733405423693fe14feb989b3b5",
            "70fb4e7cf24947e9a1d3868e8d8b230c",
            "d28c956d5f9b4fbcb4738ca53ae83142",
            "8790902061ee4339a2a6e77d2bce063d",
            "8d4aea82b7a14f18bb45d615c4538b6d",
            "cdfef1eaf3754e2594dacd6e8f437b3d",
            "b53285a5a6a54c14857a4a71c8becb3a",
            "60b7eaf4a16c41fc94bcabded61ab8c7",
            "1adb9b7052684924a2973aaefe938be8",
            "94ccc1e1ad4b497887de3cdfd0169f4e",
            "0c5a5c49409e4741a70f0a229f3a6369",
            "a60a0f17836a4523927d95831d14f769"
          ]
        },
        "outputId": "2e3a406a-d5cf-4552-ffab-149dd4c94447"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href=\"https://wandb.me/wandb-init\" target=\"_blank\">the W&B docs</a>."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:f2xjgp2n) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3baf2937c8eb4c85bf18aeb15b4442e1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">vocal-sweep-5</strong>: <a href=\"https://wandb.ai/kdb/sts_v2/runs/f2xjgp2n\" target=\"_blank\">https://wandb.ai/kdb/sts_v2/runs/f2xjgp2n</a><br/>Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20220529_224509-f2xjgp2n/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:f2xjgp2n). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.17"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/NLP/wandb/run-20220529_224634-f2xjgp2n</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/kdb/sts_v2/runs/f2xjgp2n\" target=\"_blank\">vocal-sweep-5</a></strong> to <a href=\"https://wandb.ai/kdb/sts_v2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/kdb/sts_v2/sweeps/sy73kc4x\" target=\"_blank\">https://wandb.ai/kdb/sts_v2/sweeps/sy73kc4x</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at klue/roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total step: 1953\n",
            "hyper.ckpt.0 pearsonr: tensor([0.8652]), f1_score: 0.8259109311740891\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href=\"https://wandb.me/wandb-init\" target=\"_blank\">the W&B docs</a>."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:f2xjgp2n) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a678beed18534bac8c325cbfe8cd8cac"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">vocal-sweep-5</strong>: <a href=\"https://wandb.ai/kdb/sts_v2/runs/f2xjgp2n\" target=\"_blank\">https://wandb.ai/kdb/sts_v2/runs/f2xjgp2n</a><br/>Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20220529_224634-f2xjgp2n/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:f2xjgp2n). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.17"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/NLP/wandb/run-20220529_224654-f2xjgp2n</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/kdb/sts_v2/runs/f2xjgp2n\" target=\"_blank\">vocal-sweep-5</a></strong> to <a href=\"https://wandb.ai/kdb/sts_v2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/kdb/sts_v2/sweeps/sy73kc4x\" target=\"_blank\">https://wandb.ai/kdb/sts_v2/sweeps/sy73kc4x</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at klue/roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total step: 1953\n",
            "hyper.ckpt.1 pearsonr: tensor([0.8701]), f1_score: 0.841046277665996\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href=\"https://wandb.me/wandb-init\" target=\"_blank\">the W&B docs</a>."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:f2xjgp2n) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "98f6367f4e9140fd84235e300f26b465"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">vocal-sweep-5</strong>: <a href=\"https://wandb.ai/kdb/sts_v2/runs/f2xjgp2n\" target=\"_blank\">https://wandb.ai/kdb/sts_v2/runs/f2xjgp2n</a><br/>Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20220529_224654-f2xjgp2n/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:f2xjgp2n). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.17"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/NLP/wandb/run-20220529_224714-f2xjgp2n</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/kdb/sts_v2/runs/f2xjgp2n\" target=\"_blank\">vocal-sweep-5</a></strong> to <a href=\"https://wandb.ai/kdb/sts_v2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/kdb/sts_v2/sweeps/sy73kc4x\" target=\"_blank\">https://wandb.ai/kdb/sts_v2/sweeps/sy73kc4x</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at klue/roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total step: 1953\n",
            "hyper.ckpt.2 pearsonr: tensor([0.8768]), f1_score: 0.8336673346693387\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href=\"https://wandb.me/wandb-init\" target=\"_blank\">the W&B docs</a>."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:f2xjgp2n) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8d4aea82b7a14f18bb45d615c4538b6d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">vocal-sweep-5</strong>: <a href=\"https://wandb.ai/kdb/sts_v2/runs/f2xjgp2n\" target=\"_blank\">https://wandb.ai/kdb/sts_v2/runs/f2xjgp2n</a><br/>Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20220529_224714-f2xjgp2n/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:f2xjgp2n). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.17"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/NLP/wandb/run-20220529_224734-f2xjgp2n</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/kdb/sts_v2/runs/f2xjgp2n\" target=\"_blank\">vocal-sweep-5</a></strong> to <a href=\"https://wandb.ai/kdb/sts_v2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/kdb/sts_v2/sweeps/sy73kc4x\" target=\"_blank\">https://wandb.ai/kdb/sts_v2/sweeps/sy73kc4x</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at klue/roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total step: 1953\n",
            "hyper.ckpt.3 pearsonr: tensor([0.8804]), f1_score: 0.8384458077709611\n"
          ]
        }
      ]
    }
  ]
}